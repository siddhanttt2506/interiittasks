{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PzW82IvL77w",
        "outputId": "190aa186-87cf-45bd-fa57-50e557bad1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: rapidocr-onnxruntime in /usr/local/lib/python3.10/dist-packages (1.3.24)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.32.3)\n",
            "Requirement already satisfied: httpx<=0.27.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.27.0)\n",
            "Requirement already satisfied: validators==0.33.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.33.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.8.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.66.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: pyclipper>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.3.0.post5)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (4.10.0.84)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.16.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (2.0.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (9.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.19.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.0)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client) (5.27.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (71.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<=0.27.0,>=0.25.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<=0.27.0,>=0.25.0->weaviate-client) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install weaviate-client langchain tiktoken pypdf rapidocr-onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_CLUSTER=\"https://8zykxy8qfi0qia5wnviiq.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "WEAVIATE_API_KEY=\"EZzXTDze5QJJ5uerQSn5fZfKZF9DX6Ru4NWL\""
      ],
      "metadata": {
        "id": "juhUJLAgRDE2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "t59_HL2hWYZt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "l91437aqWbpH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjRLHL05Wb7_",
        "outputId": "bc00582d-1bf1-4dda-efec-ed2d3f11e618"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=embedding_model_name,\n",
        "  #model_kwargs=model_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "RRXXb1hcWeVT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nidsHhBXU3v",
        "outputId": "0ea3ff19-2fd5-480f-a333-2c72d07b9580"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.34)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading pdf\n"
      ],
      "metadata": {
        "id": "SYbs8Rv6Ytzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# List of PDF file paths\n",
        "pdf_files = [\"/content/lora.pdf\", \"/content/qlora.pdf\", \"/content/rag.pdf\"]\n",
        "\n",
        "# Load all PDFs\n",
        "pages = []\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    pages.extend(loader.load())"
      ],
      "metadata": {
        "id": "lkncYlzShU4W"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/rag.pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "oYKQR5BWWrmq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "992nEukxXNK7",
        "outputId": "ab59762c-c511-4684-c63d-b1c48ef4d13c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/lora.pdf', 'page': 0}, page_content='LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\n{edwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen }@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\\ndent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\nrameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxh\\nùêµ=0\\nùê¥=ùí©(0,ùúé2)\\nùëëùëüPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxf(x)\\nùëë\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\\ningonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of Ô¨Åne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere ‚Äúinconvenience‚Äù for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciÔ¨Åc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efÔ¨Åciency when deployed. However, existing techniques\\n‚àóEqual contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\\nmance signiÔ¨Åcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 1}, page_content='often introduce inference latency (Houlsby et al., 2019; RebufÔ¨Å et al., 2017) by extending model\\ndepth or reduce the model‚Äôs usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the Ô¨Åne-tuning baselines, posing a trade-off between efÔ¨Åciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low ‚Äúintrinsic rank‚Äù, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers‚Äô change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\\n175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufÔ¨Åces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efÔ¨Åcient.\\nLoRA possesses several key advantages.\\n‚Ä¢ A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiÔ¨Åcantly.\\n‚Ä¢ LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\n‚Ä¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\\nconstruction.\\n‚Ä¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and ‚àÜWits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4√ódmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\nSuppose we are given a pre-trained autoregressive language model PŒ¶(y|x)parametrized by Œ¶.\\nFor instance, PŒ¶(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='During full Ô¨Åne-tuning, the model is initialized to pre-trained weights Œ¶0and updated to Œ¶0+ ‚àÜŒ¶\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nŒ¶‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(PŒ¶(yt|x,y<t)) (1)\\nOne of the main drawbacks for full Ô¨Åne-tuning is that for each downstream task, we learn a different\\nset of parameters ‚àÜŒ¶whose dimension|‚àÜŒ¶|equals|Œ¶0|. Thus, if the pre-trained model is large\\n(such as GPT-3 with |Œ¶0|‚âà175Billion), storing and deploying many independent instances of\\nÔ¨Åne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efÔ¨Åcient approach, where the task-speciÔ¨Åc parameter\\nincrement ‚àÜŒ¶ = ‚àÜŒ¶(Œò) is further encoded by a much smaller-sized set of parameters Œòwith\\n|Œò|‚â™| Œ¶0|. The task of Ô¨Ånding ‚àÜŒ¶thus becomes optimizing over Œò:\\nmax\\nŒò‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(\\npŒ¶0+‚àÜŒ¶(Œò) (yt|x,y<t))\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode ‚àÜŒ¶that is both\\ncompute- and memory-efÔ¨Åcient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parameters|Œò|can be as small as 0.01% of|Œ¶0|.\\n3 A REN‚ÄôTEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efÔ¨Åcient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efÔ¨Åcient adaptations: adding adapter layers (Houlsby\\net al., 2019; RebufÔ¨Å et al., 2017; Pfeiffer et al., 2021; R ¬®uckl¬¥e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.\\nAdapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R ¬®uckl¬¥e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliÔ¨Åed by preÔ¨Åx tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preÔ¨Åx tuning is difÔ¨Åcult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conÔ¨Årming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 3}, page_content='Batch Size 32 16 1\\nSequence Length 512 256 128\\n|Œò| 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4¬±0.8 338.0 ¬±0.6 19.8 ¬±2.7\\nAdapterL1482.0¬±1.0 (+2.2%) 354.8 ¬±0.5 (+5.0%) 23.9 ¬±2.1 (+20.7%)\\nAdapterH1492.2¬±1.0 (+3.0%) 366.3 ¬±0.5 (+8.4%) 25.8 ¬±2.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. ‚Äú |Œò|‚Äù denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiÔ¨Åcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneÔ¨Åts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciÔ¨Åc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low ‚Äúinstrisic dimension‚Äù and can still\\nlearn efÔ¨Åciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low ‚Äúintrinsic rank‚Äù during adaptation. For a pre-trained\\nweight matrix W0‚ààRd√ók, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ ‚àÜW=W0+BA, whereB‚ààRd√ór,A‚ààRr√ók, and the rank r‚â™min(d,k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and‚àÜW=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiÔ¨Åed forward pass yields:\\nh=W0x+ ‚àÜWx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so‚àÜW=BAis zero at the beginning of training. We then scale ‚àÜWx byŒ±\\nr, whereŒ±\\nis a constant in r. When optimizing with Adam, tuning Œ±is roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set Œ±to the Ô¨Årstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of Ô¨Åne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full Ô¨Åne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preÔ¨Åx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd√ók.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B‚Ä≤A‚Ä≤, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='guarantees that we do not introduce any additional latency during inference compared to a Ô¨Åne-tuned\\nmodel by construction.\\n4.2 A PPLYING LORA TOTRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module ( Wq,Wk,Wv,Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\nas a single matrix of dimension dmodel√ódmodel , even though the output dimension is usually sliced\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efÔ¨Åciency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.\\nPractical BeneÔ¨Åts and Limitations. The most signiÔ¨Åcant beneÔ¨Åt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2/3ifr‚â™dmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 √ó(from 350GB to 35MB)4. This allows us to train with signiÔ¨Å-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneÔ¨Åt is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the Ô¨Çy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full Ô¨Åne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). SpeciÔ¨Åcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During Ô¨Åne-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).\\n4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 ‚âà354GB as opposed to 100 * 350GB ‚âà35TB.\\n5For GPT-3 175B, the training throughput for full Ô¨Åne-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n5'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='Model & Method # Trainable\\nParameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nRoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\\nRoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\\nRoB base(AdptD)* 0.3M 87.1¬±.094.2¬±.188.5¬±1.160.8¬±.493.1¬±.190.2¬±.071.5¬±2.789.7¬±.384.4\\nRoB base(AdptD)* 0.9M 87.3¬±.194.7¬±.388.4¬±.162.6¬±.993.0¬±.290.6¬±.075.9¬±2.290.3¬±.185.4\\nRoB base(LoRA) 0.3M 87.5¬±.395.1¬±.289.7¬±.763.4¬±1.293.3¬±.390.8¬±.186.6¬±.791.5¬±.287.2\\nRoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\\nRoB large(LoRA) 0.8M 90.6¬±.296.2¬±.590.9¬±1.268.2¬±1.994.9¬±.391.6¬±.187.4¬±2.592.6¬±.289.0\\nRoB large(AdptP)‚Ä† 3.0M 90.2¬±.396.1¬±.390.2¬±.768.3¬±1.094.8¬±.291.9¬±.183.8¬±2.992.1¬±.788.4\\nRoB large(AdptP)‚Ä† 0.8M 90.5¬±.396.6¬±.289.7¬±1.267.8¬±2.594.8¬±.391.7¬±.280.1¬±2.991.9¬±.487.9\\nRoB large(AdptH)‚Ä† 6.0M 89.9¬±.596.2¬±.388.7¬±2.966.5¬±4.494.7¬±.292.1¬±.183.4¬±1.191.0¬±1.787.8\\nRoB large(AdptH)‚Ä† 0.8M 90.3¬±.396.3¬±.587.7¬±1.766.3¬±2.094.7¬±.291.5¬±.172.9¬±2.991.5¬±.586.4\\nRoB large(LoRA)‚Ä† 0.8M 90.6¬±.296.2¬±.590.2¬±1.068.2¬±1.994.8¬±.391.6¬±.285.2¬±1.192.3¬±.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9¬±.296.9¬±.292.6¬±.672.4¬±1.196.0¬±.192.9¬±.194.9¬±.493.0¬±.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew‚Äôs\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. ‚Ä†indicates runs conÔ¨Ågured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreÔ¨Åx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\\ncial tokens have trainable word embeddings and are generally not in the model‚Äôs vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on ‚ÄúpreÔ¨Åxing‚Äù, which prepends\\nsuch tokens to the prompt, and ‚ÄúinÔ¨Åxing‚Äù, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preÔ¨Åx (resp. inÔ¨Åx) tokens. The number of\\ntrainable parameters is |Œò|=dmodel√ó(lp+li).\\nPreÔ¨Åx-layer tuning (PreLayer) is an extension to preÔ¨Åx-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\\n|Œò|=L√ódmodel√ó(lp+li), whereLis the number of Transformer layers.\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\\nattention module (and the MLP module) and the subsequent residual connection. There are two\\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efÔ¨Åcient design with the\\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\\ninclude another baseline call AdapterDrop (R ¬®uckl¬¥e et al., 2020) which drops some adapter layers for\\ngreater efÔ¨Åciency ( AdapterD). We cite numbers from prior works whenever possible to maximize\\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the Ô¨Årst column.\\nIn all cases, we have |Œò|=ÀÜLAdpt√ó(2√ódmodel√ór+r+dmodel )+2√óÀÜLLN√ódmodel where ÀÜLAdpt\\nis the number of adapter layers and ÀÜLLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\\nAs mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\\nThe number of trainable parameters is determined by the rank rand the shape of the original weights:\\n|Œò|= 2√óÀÜLLoRA√ódmodel√ór, where ÀÜLLoRA is the number of weight matrices we apply LoRA to.\\n6'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 6}, page_content='Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3¬±.68.50¬±.07 46.0¬±.2 70.7¬±.2 2.44¬±.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4¬±.18.85¬±.02 46.8¬±.2 71.8¬±.1 2.53¬±.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1¬±.18.68¬±.03 46.3¬±.0 71.4¬±.2 2.49¬±.0\\nGPT-2 L (AdapterL) 23.00M 68.9¬±.38.70¬±.04 46.1¬±.1 71.3¬±.2 2.45¬±.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4¬±.18.89¬±.02 46.8¬±.2 72.0¬±.2 2.47¬±.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. ConÔ¨Ådence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latter‚Äôs task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efÔ¨Åcient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the Ô¨Åne-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with ‚Ä†. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n5.3 D EBERT AXXL\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nÔ¨Åne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\\nSee Section D.2 for details on the hyperparameters used.\\n5.4 GPT-2 MEDIUM /LARGE\\nHaving shown that LoRA can be a competitive alternative to full Ô¨Åne-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n7'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 7}, page_content='Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full Ô¨Åne-tuning. The results\\non WikiSQL have a Ô¨Çuctuation around ¬±0.5%, MNLI-m around ¬±0.1%, and SAMSum around\\n¬±0.2/¬±0.2/¬±0.1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a Ô¨Ånal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the Ô¨Åne-tuning baseline on all three datasets. Note\\nthat not all methods beneÔ¨Åt monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiÔ¨Åcant performance drop when we use more than 256 special tokens for\\npreÔ¨Åx-embedding tuning or more than 32 special tokens for preÔ¨Åx-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) ‚Äì both are large Transformer lan-\\n8'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='guage models trained on a large amount of text ‚Äì where Ô¨Åne-tuning on task-speciÔ¨Åc data after pre-\\ntraining on general domain data provides a signiÔ¨Åcant performance gain compared to training on\\ntask-speciÔ¨Åc data directly. Training larger Transformers generally results in better performance and\\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\\nlanguage model trained to-date with 175B parameters.\\nPrompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a\\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\\nmodel‚Äôs performance on a desired task, which is known as prompt engineering or prompt hacking.\\nFine-tuning retrains a model pre-trained on general domains to a speciÔ¨Åc task Devlin et al. (2019b);\\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform Ô¨Åne-tuning in\\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\\nhas the same memory footprint as pre-training.\\nParameter-EfÔ¨Åcient Adaptation. Many have proposed inserting adapter layers between existing\\nlayers in a neural network (Houlsby et al., 2019; RebufÔ¨Å et al., 2017; Lin et al., 2020). Our method\\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\\nkey functional difference is that our learned weights can be merged with the main weights during\\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\\nimprove its parameter efÔ¨Åciency, which we leave to future work. More recently, many proposed\\noptimizing the input word embeddings in lieu of Ô¨Åne-tuning, akin to a continuous and differentiable\\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-\\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (Ô¨Ånite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\\n(up to 10,000√ó) without adversely affecting task performances.\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n9'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 9}, page_content='to maximize downstream performance? 2) Is the ‚Äúoptimal‚Äù adaptation matrix ‚àÜWreally rank-\\ndeÔ¨Åcient ? If so, what is a good rank to use in practice? 3) What is the connection between ‚àÜWand\\nW? Does ‚àÜWhighly correlate with W? How large is ‚àÜWcomparing to W?\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using\\npre-trained language models for downstream tasks, which is a critical topic in NLP.\\n7.1 W HICH WEIGHT MATRICES IN TRANSFORMER SHOULD WEAPPLY LORA TO?\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain\\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\\nin FP16) on GPT-3 175B, which corresponds to r= 8if we adapt one type of attention weights or\\nr= 4if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n# of Trainable Parameters = 18M\\nWeight Type WqWkWvWoWq,WkWq,WvWq,Wk,Wv,Wo\\nRankr 8 8 8 8 4 4 2\\nWikiSQL (¬±0.5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7\\nMultiNLI (¬±0.1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We Ô¨Ånd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the Ô¨Årst column.\\nNote that putting all the parameters in ‚àÜWqor‚àÜWkresults in signiÔ¨Åcantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in ‚àÜWsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt {Wq,Wv},\\n{Wq,Wk,Wv,Wc}, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(¬±0.5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq,Wv 73.4 73.3 73.7 73.8 73.5\\nWq,Wk,Wv,Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (¬±0.1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq,Wv 91.3 91.4 91.3 91.6 91.4\\nWq,Wk,Wv,Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufÔ¨Åces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso for{Wq,Wv}than justWq). This suggests the update matrix ‚àÜWcould have a very small\\n‚Äúintrinsic rank‚Äù.6To further support this Ô¨Ånding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a\\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufÔ¨Åcient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought\\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\\nthe entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\\n10'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 10}, page_content='Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1‚â§i‚â§8) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1‚â§j‚â§64)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\nœÜ(Ar=8,Ar=64,i,j) =||Ui‚ä§\\nAr=8Uj\\nAr=64||2\\nF\\nmin(i,j)‚àà[0,1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\nœÜ(¬∑)has a range of [0,1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how œÜchanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both ‚àÜWqand‚àÜWv.\\nThe third and the fourth Ô¨Ågures zoom in on the lower-left triangle in the Ô¨Årst two Ô¨Ågures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiÔ¨Åcantly between\\nAr=8andAr=64, while others do not. SpeciÔ¨Åcally, ‚àÜWv(resp. ‚àÜWq) ofAr=8\\nand‚àÜWv(resp. ‚àÜWq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0.5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conÔ¨Årm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n‚àÜWqappears to have a higher ‚Äúintrinsic rank‚Äù than ‚àÜWv, since more common singular value direc-\\ntions are learned by both runs for ‚àÜWq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX ‚àÜWCOMPARE TO W?\\nWe further investigate the relationship between ‚àÜWandW. In particular, does ‚àÜWhighly correlate\\nwithW? (Or mathematically, is ‚àÜWmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices ‚Äì we stick with\\nAfor our experiments.\\n11'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 11}, page_content='0.00.10.20.30.40.5\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj1\\n8\\n16\\n24\\n32\\n40\\n48\\n56iWq\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nWv\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\njRandom GaussianFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\\nfrom two random seeds, for both ‚àÜWqand‚àÜWvin the 48-th layer. Right: the same heat-map\\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\nhow ‚Äúlarge‚Äù is ‚àÜWcomparing to its corresponding directions in W? This can shed light on the\\nunderlying mechanism for adapting pre-trained language models.\\nTo answer these questions, we project Wonto ther-dimensional subspace of ‚àÜWby comput-\\ningU‚ä§WV‚ä§, withU/Vbeing the left/right singular-vector matrix of ‚àÜW. Then, we com-\\npare the Frobenius norm between ‚à•U‚ä§WV‚ä§‚à•Fand‚à•W‚à•F. As a comparison, we also compute\\n‚à•U‚ä§WV‚ä§‚à•Fby replacing U,V with the top rsingular vectors of Wor a random matrix.\\nr= 4 r= 64\\n‚àÜWqWq Random ‚àÜWqWq Random\\n||U‚ä§WqV‚ä§||F= 0.32 21.67 0.02 1.90 37.71 0.33\\n||Wq||F= 61.95||‚àÜWq||F= 6.91||‚àÜWq||F= 3.57\\nTable 7: The Frobenius norm of U‚ä§WqV‚ä§whereUandVare the left/right top rsingular vector\\ndirections of either (1) ‚àÜWq, (2)Wq, or (3) a random matrix. The weight matrices are taken from\\nthe 48th layer of GPT-3.\\nWe draw several conclusions from Table 7. First, ‚àÜWhas a stronger correlation with Wcompared\\nto a random matrix, indicating that ‚àÜWampliÔ¨Åes some features that are already in W. Second,\\ninstead of repeating the top singular directions of W,‚àÜWonly ampliÔ¨Åes directions that are not\\nemphasized in W. Third, the ampliÔ¨Åcation factor is rather huge: 21.5‚âà6.91/0.32forr= 4.\\nSee Section H.4 for why r= 64 has a smaller ampliÔ¨Åcation factor. We also provide a visualization\\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\\nThis suggests that the low-rank adaptation matrix potentially ampliÔ¨Åes the important features for\\nspeciÔ¨Åc downstream tasks that were learned but not emphasized in the general pre-training model .\\n8 C ONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances for different tasks. We propose\\nLoRA, an efÔ¨Åcient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.\\nThere are many directions for future works. 1) LoRA can be combined with other efÔ¨Åcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind Ô¨Åne-tuning\\nor LoRA is far from clear ‚Äì how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full Ô¨Åne-\\n12'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 12}, page_content='tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deÔ¨Åciency of ‚àÜWsuggests that Wcould\\nbe rank-deÔ¨Åcient as well, which can also be a source of inspiration for future works.\\nREFERENCES\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\\nhttp://arxiv.org/abs/2012.13255 .\\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn EfÔ¨Åciently, Going Beyond Kernels? In\\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puriÔ¨Åcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\\n03962 .\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\\nmatrix completion. SIAM Journal on optimization , 20(4):1956‚Äì1982, 2010.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\\nRonan Collobert and Jason Weston. A uniÔ¨Åed architecture for natural language processing: deep\\nneural networks with multitask learning. In Proceedings of the 25th international conference\\non Machine learning , ICML ‚Äô08, pp. 160‚Äì167, New York, NY , USA, July 2008. Association\\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\\nhttps://doi.org/10.1145/1390156.1390177 .\\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc‚ÄôAurelio Ranzato, and Nando de Freitas. Predicting\\nparameters in deep learning, 2014.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019a.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\\nhttps://aclanthology.org/I05-5002 .\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\\nNatural Language Generation , pp. 124‚Äì133, 2017.\\n13'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 13}, page_content='Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\\nhttp://arxiv.org/abs/1911.12237 .\\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\\napproximation techniques. GAMM-Mitteilungen , 36(1):53‚Äì78, 2013.\\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\\nlearning. In ICML , pp. 376‚Äì383, 2008. URL https://doi.org/10.1145/1390156.\\n1390204 .\\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\\n2101.00121 . arXiv: 2101.00121.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-EfÔ¨Åcient Transfer Learning\\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\\n00751 .\\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\\nof factorized neural layers, 2021.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-EfÔ¨Åcient Prompt\\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\\narXiv: 2104.08691.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\\nXiang Lisa Li and Percy Liang. PreÔ¨Åx-Tuning: Optimizing Continuous Prompts for Generation.\\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\\n2358‚Äì2367. PMLR, 2016.\\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\\nory, pp. 2‚Äì47. PMLR, 2018b.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efÔ¨Åcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020 , pp. 441‚Äì459, Online, November 2020. Association for Computational\\nLinguistics. doi: 10.18653/v1/2020.Ô¨Åndings-emnlp.41. URL https://aclanthology.\\norg/2020.findings-emnlp.41 .\\n14'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 14}, page_content='Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\\n2103.10385 . arXiv: 2103.10385.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: EfÔ¨Åcient low-rank\\nhypercomplex adapter layers, 2021.\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.\\nJekaterina Novikova, Ond Àárej Du Àásek, and Verena Rieser. The e2e dataset: New challenges for end-\\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\\narXiv:1906.05392 , 2019.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¬®uckl¬¥e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\\nInterspeech , pp. 3743‚Äì3747, 2018.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\\nstanding by Generative Pre-Training. pp. 12, a.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nModels are Unsupervised Multitask Learners. pp. 24, b.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable questions\\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\\nSylvestre-Alvise RebufÔ¨Å, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\\nabs/1705.08045 . arXiv: 1705.08045.\\nAndreas R ¬®uckl¬¥e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\\nIryna Gurevych. Adapterdrop: On the efÔ¨Åciency of adapters in transformers, 2020.\\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\\nrank matrix factorization for deep neural network training with high-dimensional output targets.\\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655‚Äì\\n6659. IEEE, 2013.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\\nallelism, 2020.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\\nProcessing , pp. 1631‚Äì1642, Seattle, Washington, USA, October 2013. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\\n15'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 15}, page_content='Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\\nternational Conference on Neural Information Processing Systems , pp. 6000‚Äì6010, 2017.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems, 2020.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint arXiv:1805.12471 , 2018.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers) , pp. 1112‚Äì1122, New Orleans, Louisiana, June 2018. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\\norg/anthology/N18-1101 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demonstrations , pp. 38‚Äì45, Online, October 2020. As-\\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\\n2020.emnlp-demos.6 .\\nGreg Yang and Edward J. Hu. Feature Learning in InÔ¨Ånite-Width Neural Networks.\\narXiv:2011.14522 [cond-mat] , May 2021. URL http://arxiv.org/abs/2011.14522 .\\narXiv: 2011.14522.\\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitÔ¨Åt: Simple parameter-efÔ¨Åcient Ô¨Åne-tuning\\nfor transformer-based masked language-models, 2021.\\nYu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck\\nfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,\\nspeech and signal processing (ICASSP) , pp. 185‚Äì189. IEEE, 2014.\\nYong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks.\\nIn2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,\\npp. 5005‚Äì5009. IEEE, 2016.\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\\nnatural language using reinforcement learning. CoRR , abs/1709.00103, 2017. URL http://\\narxiv.org/abs/1709.00103 .\\nA L ARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES\\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of\\ntraining samples. However, in practice, we can often afford to curate a few thousand or more training\\nexamples for performance-sensitive applications. As shown in Table 8, Ô¨Åne-tuning improves the\\nmodel performance drastically compared to few-shot learning on datasets large and small. We take\\nthe GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched,\\nwe use two demonstrations per class and six in-context examples in total.\\n16'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 16}, page_content='Method MNLI-m (Val. Acc./%) RTE (Val. Acc./%)\\nGPT-3 Few-Shot 40.6 69.0\\nGPT-3 Fine-Tuned 89.5 85.4\\nTable 8: Fine-tuning signiÔ¨Åcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\\nB I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas\\nour proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\\nadapter layers must be computed in addition to the base model, inevitably introducing additional\\nlatency. While as pointed out in R ¬®uckl¬¥e et al. (2020), the latency introduced by adapter layers can\\nbe mitigated when the model batch size and/or sequence length is large enough to full utilize the\\nhardware parallelism. We conÔ¨Årm their observation with a similar latency study on GPT-2 medium\\nand point out that there are scenarios, notably online inference where the batch size is small, where\\nthe added latency can be signiÔ¨Åcant.\\nWe measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\\nover 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\\nr. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\\nand a recent, more efÔ¨Åcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\\nfor more details on the designs. We plot the slow-down in percentage compared to the no-adapter\\nbaseline in Figure 5.\\n05101520253035\\n0 10 100 250AdapterH rSeq Len = 128 Seq Len = 256 Seq Len = 512\\n1 2 4 8 16 32\\nBatch Size0 10 100 250AdapterL r\\n1 2 4 8 16 32\\nBatch Size1 2 4 8 16 32\\nBatch Size\\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter ( r= 0) baseline.\\nThe top row shows the result for AdapterHand the bottom row AdapterL. Larger batch size and\\nsequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\\nonline, short-sequence-length scenario. We tweak the colormap for better visibility.\\nC D ATASET DETAILS\\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\\nMNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\\n(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\\n(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8(question-answering), RTE (inference),\\n8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\\n17'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\\nstandard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\\nare released under different permissive licenses.\\nWikiSQL is introduced in Zhong et al. (2017) and contains 56,355/8,421training/validation ex-\\namples. The task is to generate SQL queries from natural language questions and table schemata.\\nWe encode context as x={table schema ,query}and target as y={SQL}. The dataset is release\\nunder the BSD 3-Clause License.\\nSAMSum is introduced in Gliwa et al. (2019) and contains 14,732/819training/test examples. It\\nconsists of staged chat conversations between two people and corresponding abstractive summaries\\nwritten by linguists. We encode context as ‚Äù \\\\n‚Äù concatenated utterances followed by a ‚Äù \\\\n\\\\n‚Äù,\\nand target as y={summary}. The dataset is released under the non-commercial licence: Creative\\nCommons BY-NC-ND 4.0.\\nE2E NLG Challenge was Ô¨Årst introduced in Novikova et al. (2017) as a dataset for training end-to-\\nend, data-driven natural language generation systems and is commonly used for data-to-text evalua-\\ntion. The E2E dataset consists of roughly 42,000training, 4,600validation, and 4,600test exam-\\nples from the restaurant domain. Each source table used as input can have multiple references. Each\\nsample input (x,y)consists of a sequence of slot-value pairs, along with a corresponding natural\\nlanguage reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nDART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\\nstructured as sequences of ENTITY ‚Äî RELATION ‚Äî ENTITY triples. With 82Kexamples in\\ntotal, DART is a signiÔ¨Åcantly larger and more complex data-to-text task compared to E2E. The\\ndataset is released under the MIT license.\\nWebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With\\n22Kexamples in total WebNLG comprises 14 distinct categories, nine of which are seen during\\ntraining. Since Ô¨Åve of the 14 total categories are not seen during training, but are represented in\\nthe test set, evaluation is typically broken out by ‚Äúseen‚Äù categories (S), ‚Äúunseen‚Äù categories (U)\\nand ‚Äúall‚Äù (A). Each input example is represented by a sequence of SUBJECT ‚Äî PROPERTY ‚Äî\\nOBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nD H YPERPARAMETERS USED IN EXPERIMENTS\\nD.1 R OBERT A\\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\\nof training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\\nmodules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\\nusual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\\nrandom seeds; the result for each run is taken from the best epoch. For a fair comparison with the\\nsetup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\\nand used a Ô¨Åxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\\nmodel when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\\nThe runs with this restricted setup are marked with ‚Ä†. See the hyperparameters used in our runs\\nin Table 9.\\nD.2 D EBERT A\\nWe again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\\nwe tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\\nsequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\\nwe initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\\nSTS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\\nthe median over 5 random seeds; the result for each run is taken from the best epoch. See the\\nhyperparameters used in our runs in Table 10.\\n18'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 18}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.06\\nLR Schedule Linear\\nRoBERTa base\\nLoRABatch Size 16 16 16 32 32 16 32 16\\n# Epochs 30 60 30 80 25 25 80 40\\nLearning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 512\\nRoBERTa large\\nLoRABatch Size 4 4 4 4 4 4 8 8\\n# Epochs 10 10 20 20 10 20 20 30\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128 128 512 128 512 512 512 512\\nRoBERTa large\\nLoRA‚Ä†Batch Size 4\\n# Epochs 10 10 20 20 10 20 20 10\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(3M)‚Ä†Batch Size 32\\n# Epochs 10 20 20 20 10 20 20 20\\nLearning Rate 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(0.8M)‚Ä†Batch Size 32\\n# Epochs 5 20 20 20 10 20 20 20\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(6M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(0.8M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 8\\nMax Seq. Len. 128\\nTable 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\\nD.3 GPT-2\\nWe train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\\nrate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\\nin Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\\nmean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\\nused for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\\n(2021).\\nD.4 GPT-3\\nFor all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\\na batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\\n19'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 19}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.1\\nLR Schedule Linear\\nDeBERTa XXL\\nLoRABatch Size 8 8 32 4 6 8 4 4\\n# Epochs 5 16 30 10 8 11 11 10\\nLearning Rate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04\\nWeight Decay 0 0.01 0.01 0 0.01 0.01 0.01 0.1\\nCLS Dropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 256 128 128 64 512 320 320 128\\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\\nDataset E2E WebNLG DART\\nTraining\\nOptimizer AdamW\\nWeight Decay 0.01 0.01 0.0\\nDropout Prob 0.1 0.1 0.0\\nBatch Size 8\\n# Epoch 5\\nWarmup Steps 500\\nLearning Rate Schedule Linear\\nLabel Smooth 0.1 0.1 0.0\\nLearning Rate 0.0002\\nAdaptation rq=rv= 4\\nLoRAŒ± 32\\nInference\\nBeam Size 10\\nLength Penalty 0.9 0.8 0.8\\nno repeat ngram size 4\\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa\\net al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\\ndetails on the hyperparameters used. For preÔ¨Åx-embedding tuning, we Ô¨Ånd the optimal lpandli\\nto be 256 and 8, respectively, totalling 3.2Mtrainable parameters. We use lp= 8 andli= 8 for\\npreÔ¨Åx-layer tuning with 20.2Mtrainable parameters to obtain the overall best performance. We\\npresent two parameter budgets for LoRA: 4.7M ( rq=rv= 1orrv= 2) and 37.7M ( rq=rv= 8\\norrq=rk=rv=ro= 2). We report the best validation performance from each run. The training\\nhyperparameters used in our GPT-3 experiments are listed in Table 12.\\nE C OMBINING LORA WITH PREFIX TUNING\\nLoRA can be naturally combined with existing preÔ¨Åx-based approaches. In this section, we evaluate\\ntwo combinations of LoRA and variants of preÔ¨Åx-tuning on WikiSQL and MNLI.\\nLoRA+PreÔ¨ÅxEmbed (LoRA+PE) combines LoRA with preÔ¨Åx-embedding tuning, where we insert\\nlp+lispecial tokens whose embeddings are treated as trainable parameters. For more on preÔ¨Åx-\\nembedding tuning, see Section 5.1.\\nLoRA+PreÔ¨ÅxLayer (LoRA+PL) combines LoRA with preÔ¨Åx-layer tuning. We also insert lp+li\\nspecial tokens; however, instead of letting the hidden representations of these tokens evolve natu-\\n20'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 20}, page_content='Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterHLoRA\\nOptimizer AdamW\\nBatch Size 128\\n# Epoch 2\\nWarmup Tokens 250,000\\nLR Schedule Linear\\nLearning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\\nsame hyperparameters for all datasets after tuning learning rate.\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\\nmore on preÔ¨Åx-layer tuning, see Section 5.1.\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\\nFirst of all, LoRA+PE signiÔ¨Åcantly outperforms both LoRA and preÔ¨Åx-embedding tuning on\\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preÔ¨Åx-embedding tuning. On\\nMultiNLI, the combination of LoRA+PE doesn‚Äôt perform better than LoRA, possibly because LoRA\\non its own already achieves performance comparable to the human baseline. Secondly, we notice\\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\\ntribute this to the fact that preÔ¨Åx-layer tuning is very sensitive to the choice of learning rate and thus\\nmakes the optimization of LoRA weights more difÔ¨Åcult in LoRA+PL.\\nF A DDITIONAL EMPIRICAL EXPERIMENTS\\nF.1 A DDITIONAL EXPERIMENTS ON GPT-2\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\\npreÔ¨Åx-based approaches given the same number of trainable parameters.\\nMethod # Trainable DART\\nParameters BLEU‚ÜëMET‚ÜëTER‚Üì\\nGPT-2 Medium\\nFine-Tune 354M 46.2 0.39 0.46\\nAdapterL0.37M 42.4 0.36 0.48\\nAdapterL11M 45.2 0.38 0.46\\nFTTop224M 41.0 0.34 0.56\\nPrefLayer 0.35M 46.4 0.38 0.46\\nLoRA 0.35M 47.1¬±.2 0.39 0.46\\nGPT-2 Large\\nFine-Tune 774M 47.0 0.39 0.46\\nAdapterL0.88M 45.7¬±.1 0.38 0.46\\nAdapterL23M 47.1¬±.1 0.39 0.45\\nPrefLayer 0.77M 46.7 0.38 0.45\\nLoRA 0.77M 47.5¬±.1 0.39 0.45\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\\nless than 0.01for all adaption approaches.\\n21'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 21}, page_content='Method WebNLG\\nBLEU‚Üë MET‚Üë TER‚Üì\\nU S A U S A U S A\\nGPT-2 Medium\\nFine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53\\nAdapterL(0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43\\nAdapterL(11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39\\nFTTop2(24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72\\nPreÔ¨Åx (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40\\nLoRA (0.35M) 46.7¬±.462.1¬±.255.3¬±.2.38 .44 .41 .46 .33 .39\\nGPT-2 Large\\nFine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42\\nAdapterL(0.88M) 49.8¬±.061.1¬±.056.0¬±.0.38 .43 .41 .44 .35 .39\\nAdapterL(23M) 49.2¬±.164.7¬±.257.7¬±.1.39 .46 .43 .46 .33 .39\\nPreÔ¨Åx (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40\\nLoRA (0.77M) 48.4¬±.364.0¬±.357.0¬±.1.39 .45 .42 .45 .32 .38\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\\nare less than 0.01for all the experiments we ran. ‚ÄúU‚Äù indicates unseen categories, ‚ÄúS‚Äù indicates seen\\ncategories, and ‚ÄúA‚Äù indicates all categories in the test set of WebNLG.\\nF.2 A DDITIONAL EXPERIMENTS ON GPT-3\\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\\nidentifying the trade-off between performance and the number of trainable parameters.\\nF.3 L OW-DATA REGIME\\nTo evaluate the performance of different adaptation approaches in the low-data regime. we randomly\\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\\nMNLI-ntasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\\nn. To our surprise, PreÔ¨ÅxEmbed and PreÔ¨ÅxLayer performs very poorly on MNLI-100 dataset, with\\nPreÔ¨ÅxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreÔ¨ÅxLayer\\nperforms better than PreÔ¨ÅxEmbed but is still signiÔ¨Åcantly worse than Fine-Tune or LoRA on MNLI-\\n100. The gap between preÔ¨Åx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\\ncrease the number of training examples, which might suggest that preÔ¨Åx-based approaches are not\\nsuitable for low-data tasks in GPT-3. LoRA achieves better performance than Ô¨Åne-tuning on both\\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\\n(¬±0.3) variance due to random seeds.\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\\nble 17. We use a smaller learning rate for PreÔ¨ÅxLayer on the MNLI-100 set, as the training loss does\\nnot decrease with a larger learning rate.\\nG M EASURING SIMILARITY BETWEEN SUBSPACES\\nIn this paper we use the measure œÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚à•Ui‚ä§\\nAUB‚à•2\\nF\\nmin{i,j}to measure the subspace\\nsimilarity between two column orthonormal matrices Ui\\nA‚ààRd√óiandUj\\nB‚ààRd√ój, obtained by\\ntaking columns of the left singular matrices of AandB. We point out that this similarity is simply\\na reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\\n(2008).\\n22'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 22}, page_content='Method Hyperparameters # Trainable Parameters WikiSQL MNLI-m\\nFine-Tune - 175B 73.8 89.5\\nPreÔ¨ÅxEmbedlp= 32,li= 8 0.4 M 55.9 84.9\\nlp= 64,li= 8 0.9 M 58.7 88.1\\nlp= 128,li= 8 1.7 M 60.6 88.0\\nlp= 256,li= 8 3.2 M 63.1 88.6\\nlp= 512,li= 8 6.4 M 55.9 85.8\\nPreÔ¨ÅxLayerlp= 2,li= 2 5.1 M 68.5 89.2\\nlp= 8,li= 0 10.1 M 69.8 88.2\\nlp= 8,li= 8 20.2 M 70.1 89.5\\nlp= 32,li= 4 44.1 M 66.4 89.6\\nlp= 64,li= 0 76.1 M 64.9 87.9\\nAdapterHr= 1 7.1 M 71.9 89.8\\nr= 4 21.2 M 73.2 91.0\\nr= 8 40.1 M 73.2 91.5\\nr= 16 77.9 M 73.2 91.5\\nr= 64 304.4 M 72.6 91.5\\nLoRArv= 2 4.7 M 73.4 91.7\\nrq=rv= 1 4.7 M 73.4 91.3\\nrq=rv= 2 9.4 M 73.3 91.4\\nrq=rk=rv=ro= 1 9.4 M 74.1 91.2\\nrq=rv= 4 18.8 M 73.7 91.3\\nrq=rk=rv=ro= 2 18.8 M 73.7 91.7\\nrq=rv= 8 37.7 M 73.8 91.6\\nrq=rk=rv=ro= 4 37.7 M 74.0 91.7\\nrq=rv= 64 301.9 M 73.6 91.4\\nrq=rk=rv=ro= 64 603.8 M 73.9 91.4\\nLoRA+PErq=rv= 8,lp= 8,li= 4 37.8 M 75.0 91.4\\nrq=rv= 32,lp= 8,li= 4 151.1 M 75.9 91.1\\nrq=rv= 64,lp= 8,li= 4 302.1 M 76.2 91.3\\nLoRA+PL rq=rv= 8,lp= 8,li= 4 52.8 M 72.9 90.2\\nTable 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\\npreÔ¨Åx-embedding tuning (PreÔ¨ÅxEmbed) and preÔ¨Åx-layer tuning (PreÔ¨ÅxLayer) perform worse as we\\nincrease the number of trainable parameters, while LoRA‚Äôs performance stabilizes. Performance is\\nmeasured in validation accuracy.\\nMethod MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k MNLI(m)-392K\\nGPT-3 (Fine-Tune) 60.2 85.8 88.9 89.5\\nGPT-3 (PreÔ¨ÅxEmbed) 37.6 75.2 79.5 88.6\\nGPT-3 (PreÔ¨ÅxLayer) 48.3 82.5 85.9 89.6\\nGPT-3 (LoRA) 63.8 85.6 89.2 91.7\\nTable 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\\nndescribes a subset with ntraining examples. We evaluate with the full validation set. LoRA\\nperforms exhibits favorable sample-efÔ¨Åciency compared to other methods, including Ô¨Åne-tuning.\\nTo be concrete, let the singular values of Ui‚ä§\\nAUj\\nBto beœÉ1,œÉ2,¬∑¬∑¬∑,œÉpwherep= min{i,j}. We\\nknow that the Projection Metric Ham & Lee (2008) is deÔ¨Åned as:\\nd(Ui\\nA,Uj\\nB) =\\ued6a\\ued6b\\ued6b‚àöp‚àíp‚àë\\ni=1œÉ2\\ni‚àà[0,‚àöp]\\n23'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 23}, page_content='Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\\nOptimizer - AdamW\\nWarmup Tokens - 250,000\\nLR Schedule - Linear\\nBatch Size - 20 20 100 128\\n# Epoch - 40 40 4 2\\nLearning RateFineTune 5.00E-6\\nPreÔ¨ÅxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\\nPreÔ¨ÅxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\\nLoRA 2.00E-4\\nPreÔ¨ÅxEmbed lp 16 32 64 256\\nAdaptation- PreÔ¨ÅxEmbed li 8\\nSpeciÔ¨Åc PreÔ¨ÅxTune lp=li= 8\\nLoRA rq=rv= 8\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\\nwhere our similarity is deÔ¨Åned as:\\nœÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚àëp\\ni=1œÉ2\\ni\\np=1\\np(\\n1‚àíd(Ui\\nA,Uj\\nB)2)\\nThis similarity satisÔ¨Åes that if Ui\\nAandUj\\nBshare the same column span, then œÜ(A,B,i,j ) = 1 . If\\nthey are completely orthogonal, then œÜ(A,B,i,j ) = 0 . Otherwise, œÜ(A,B,i,j )‚àà(0,1).\\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\nWe present additional results from our investigation into the low-rank update matrices.\\nH.1 C ORRELATION BETWEEN LORA M ODULES\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\\nlayers.\\nH.2 E FFECT OFrONGPT-2\\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\\ndataset as an example, we report the validation loss and test metrics achieved by different choices\\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\\nNote that the relationship between model size and the optimal rank for adaptation is still an open\\nquestion.\\nH.3 C ORRELATION BETWEEN WAND ‚àÜW\\nSee Figure 8 for the normalized subspace similarity between Wand‚àÜWwith varying r.\\nNote again that ‚àÜWdoes not contain the top singular directions of W, since the similarity between\\nthe top 4 directions in ‚àÜWand the top-10% of those in Wbarely exceeds 0.2. This gives evidence\\nthat‚àÜWcontains those ‚Äútask-speciÔ¨Åc‚Äù directions that are otherwise notemphasized in W.\\nAn interesting next question to answer, is how ‚Äústrong‚Äù do we need to amplify those task-speciÔ¨Åc\\ndirections, in order for the model adaptation to work well?\\n24'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 24}, page_content='0.00.20.40.60.81.0\\n12345678Layer 1\\niWq\\n Wv\\n Wq\\n Wv\\n12345678Layer 32\\ni\\n12345678Layer 64\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678Layer 96\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678\\nj12345678\\nj(Ar=8,Ar=64,i,j)\\nFigure 6: Normalized subspace similarity between the column vectors of Ar=8andAr=64for both\\n‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\nH.4 A MPLIFICATION FACTOR\\nOne can naturally consider a feature ampliÔ¨Åcation factor as the ratio‚à•‚àÜW‚à•F\\n‚à•U‚ä§WV‚ä§‚à•F, whereUandV\\nare the left- and right-singular matrices of the SVD decomposition of ‚àÜW. (RecallUU‚ä§WV‚ä§V\\ngives the ‚Äúprojection‚Äù of Wonto the subspace spanned by ‚àÜW.)\\nIntuitively, when ‚àÜWmostly contains task-speciÔ¨Åc directions, this quantity measures how much of\\nthem are ampliÔ¨Åed by ‚àÜW. As shown in Section 7.3, for r= 4, this ampliÔ¨Åcation factor is as large\\nas 20. In other words, there are (generally speaking) four feature directions in each layer (out of the\\nentire feature space from the pre-trained model W), that need to be ampliÔ¨Åed by a very large factor\\n20, in order to achieve our reported accuracy for the downstream speciÔ¨Åc task. And, one should\\nexpect a very different set of feature directions to be ampliÔ¨Åed for each different downstream task.\\nOne may notice, however, for r= 64 , this ampliÔ¨Åcation factor is only around 2, meaning that\\nmost directions learned in ‚àÜWwithr= 64 arenotbeing ampliÔ¨Åed by much. This should not\\nbe surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\\nthe ‚Äútask-speciÔ¨Åc directions‚Äù (thus for model adaptation) is low. In contrast, those directions in the\\nrank-4 version of ‚àÜW(corresponding to r= 4) are ampliÔ¨Åed by a much larger factor 20.\\n25'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 25}, page_content='0.00.10.20.30.40.50.60.70.8\\n1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 1\\niWq\\n Wv\\nLayer 32Wq\\n Wv\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 64\\ni\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\njLayer 96\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nFigure 7: Normalized subspace similarity between the column vectors of Ar=64from two randomly\\nseeded runs, for both ‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\\nformer.\\nRankrvalloss BLEU NIST METEOR ROUGE L CIDEr\\n1 1.23 68.72 8.7215 0.4565 0.7052 2.4329\\n2 1.21 69.17 8.7413 0.4590 0.7052 2.4639\\n4 1.18 70.38 8.8439 0.4689 0.7186 2.5349\\n8 1.17 69.57 8.7457 0.4636 0.7196 2.5196\\n16 1.16 69.61 8.7483 0.4629 0.7177 2.4985\\n32 1.16 69.33 8.7736 0.4642 0.7105 2.5255\\n64 1.16 69.24 8.7174 0.4651 0.7180 2.5070\\n128 1.16 68.73 8.6718 0.4628 0.7127 2.5030\\n256 1.16 68.92 8.6982 0.4629 0.7128 2.5012\\n512 1.16 68.78 8.6857 0.4637 0.7128 2.5025\\n1024 1.17 69.37 8.7495 0.4659 0.7149 2.5090\\nTable 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\\ndifferent rank rusing GPT-2 Medium. Unlike on GPT-3 where r= 1sufÔ¨Åces for many tasks, here\\nthe performance peaks at r= 16 for validation loss and r= 4 for BLEU, suggesting the GPT-2\\nMedium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\\nhyperparameters are tuned on r= 4, which matches the parameter count of another baseline, and\\nthus might not be optimal for other choices of r.\\n0.1000.1250.1500.1750.200\\nj451\\n555\\n658\\n762\\n865\\n969\\n1072\\n1176i(Wq,Ar=4,i,j)\\njWq\\n(Wq,Ar=8,i,j)\\nj(Wq,Ar=64,i,j)\\njRandom\\n(Wq,Arand,i,j)\\nFigure 8: Normalized subspace similarity between the singular directions of Wqand those of ‚àÜWq\\nwith varying rand a random baseline. ‚àÜWqampliÔ¨Åes directions that are important but not empha-\\nsized inW.‚àÜWwith a larger rtends to pick up more directions that are already emphasized in\\nW.\\n26'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 0}, page_content='QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers‚àóArtidoro Pagnoni‚àóAri Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n‚àóEqual contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( ¬±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 ¬±1\\nGuanaco 65B 41 GB 1022 ¬±1\\nGuanaco 33B 21 GB 992 ¬±1\\nVicuna 13B 26 GB 974 ¬±1\\nChatGPT - 966 ¬±1\\nGuanaco 13B 10 GB 916 ¬±1\\nBard - 902 ¬±1\\nGuanaco 7B 6 GB 879 ¬±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable\\non a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [ 10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 6).\\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average\\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA‚Äôs efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (¬ß4) and training a state-of-the-art chatbot, Guanaco , (¬ß5), we also\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa‚Äîin other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 2}, page_content='Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2 Background\\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [‚àí127,127]:\\nXInt8=round\\x12127\\nabsmax (XFP32)XFP32\\x13\\n=round (cFP32¬∑XFP32), (1)\\nwhere cis the quantization constant orquantization scale . Dequantization is the inverse:\\ndequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization bins‚Äîcertain bit combinations‚Äîare not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor X‚ààRb√óhintoncontiguous blocks of\\nsizeBby flattening the input tensor and slicing the linear segment into n= (b√óh)/Bblocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\\nconstants ci.\\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW =YwithX‚ààRb√óh,W‚ààRh√óoLoRA computes:\\nY=XW +sXL 1L2, (3)\\nwhereL1‚ààRh√órandL2‚ààRr√óo, and sis a scalar.\\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\n3'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose‚Äî4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation œÉ(see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling œÉsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [‚àí1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations œÉin the range [‚àí1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [‚àí1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [‚àí1,1]range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(¬∑)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='ensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\\ndata type by estimating the quantiles qiof two ranges qi:2k‚àí1for the negative part and 2k‚àí1+ 1for\\nthe positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both\\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\\nnormally distributed data. The exact values of this data type can be found in Appendix E.\\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\\nquantization constants for additional memory savings. While a small blocksize is required for precise\\n4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit\\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on\\naverage. Double Quantization helps reduce the memory footprint of quantization constants.\\nMore specifically, Double Quantization treats quantization constants cFP32\\n2of the first quantization\\nas inputs to a second quantization. This second step yields the quantized quantization constants\\ncFP8\\n2and the second level of quantization constants cFP32\\n1. We use 8-bit Floats with a blocksize of\\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\\n2are positive, we subtract\\nthe mean from c2before quantization to center the values around zero and make use of symmetric\\nquantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64¬∑256) = 0 .127bits, a reduction of 0.373 bits\\nper parameter.\\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\nwhere doubleDequant (¬∑)is defined as:\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.\\nFor parameter updates only the gradient with respect to the error for the adapters weights‚àÇE\\n‚àÇLiare\\nneeded, and not for 4-bit weights‚àÇE\\n‚àÇW. However, the calculation of‚àÇE\\n‚àÇLientails the calculation of‚àÇX\\n‚àÇW\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivative‚àÇX\\n‚àÇWin BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16\\nFigure 2: RougeL for LLaMA 7B models on the\\nAlpaca dataset. Each point represents a run with a\\ndifferent random seed. We improve on the Stanford\\nAlpaca fully finetuned default hyperparameters to\\nconstruct a strong 16-bit baseline for comparisons.\\nUsing LoRA on all transformer layers is critical to\\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\\nQLORAtuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimiz-\\ners since the paging only occurs when processing\\nmini-batches with long sequence lengths, which is\\nrare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on\\n48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed\\nas regular optimizers. Future work should measure\\nand characterize under what circumstances slow-\\ndowns occur from the paging process.\\nDefault LoRA hyperparameters do not match 16-\\nbit performance When using the standard prac-\\ntice of applying LoRA to query and value attention\\nprojection matrices [ 28], we are not able to replicate\\nfull finetuning performance for large base models.\\nAs shown in Figure 2 for LLaMA 7B finetuning on\\nAlpaca, we find that the most critical LoRA hyper-\\nparameter is how many LoRA adapters are used in\\ntotal and that LoRA on all linear transformer block\\nlayers are required to match full finetuning perfor-\\nmance. Other LoRA hyperparameters, such as the\\nprojection dimension r, do not affect performance (see Appendix A).\\n1010\\n1011\\nT otal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67Mean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQData type\\nFigure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more\\nfine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).Similarly, we find that default hyperparameters for\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\nestablished that 4-bit quantization for inference is\\n6'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 6}, page_content='Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORAreplicates 16-bit LoRA and full-\\nfinetuning.\\nDataset GLUE (Acc.) Super-NaturalInstructions (RougeL)\\nModel RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B\\nBF16 88.6 40.1 42.1 48.0 54.3 62.0\\nBF16 replication 88.6 40.0 42.2 47.3 54.9 -\\nLoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7\\nQLORA Int8 88.8 40.4 42.9 45.4 56.5 60.7\\nQLORA FP4 88.6 40.3 42.4 47.5 55.6 60.9\\nQLORA NF4 + DQ - 40.4 42.7 47.7 55.3 60.9\\npossible, but leads to performance degradation rel-\\native to 16-bit [ 13,18]. This raises the crucial question of whether the lost performance can be\\nrecovered by conducting 4-bit adapter finetuning. We test this for two setups.\\nTable 2: Pile Common Crawl mean\\nperplexity for different data types\\nfor 125M to 13B OPT, BLOOM,\\nLLaMA, and Pythia models.\\nData type Mean PPL\\nInt4 34.34\\nFloat4 (E2M1) 31.07\\nFloat4 (E3M0) 29.48\\nNFloat4 + DQ 27.41The first focuses on a comparison with full 16-bit finetuning\\nof RoBERTA and T5 models sized 125M to 3B parameters on\\nGLUE and the Super-NaturalInstructions dataset. Results are\\nshown in Table 3. In both datasets, we observe that 16-bit, 8-bit,\\nand 4-bit adapter methods replicate the performance of the fully\\nfinetuned 16-bit baseline. This suggests that the performance lost\\ndue to the imprecise quantization can be fully recovered through\\nadapter finetuning after quantization.\\nFor our second setup, since full finetuning models at and beyond\\n11B parameters requires more than one server of high memory\\nGPUs, we continue to test whether 4-bit QLORAcan match\\n16-bit LoRA at the 7B to 65B parameter scales. To this end, we\\nfinetune LLaMA 7B through 65B on two instruction following\\ndatasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results\\nare shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit\\nLoRA MMLU performance. In addition, we also note that QLORAwith FP4 lags behind the 16-bit\\nbrain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1)\\nQLORAwith NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,\\nand (2) NF4 is superior to FP4 in terms of quantization precision.\\nSummary Our results consistently show that 4-bit QLORAwith NF4 data type matches 16-\\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given\\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\\n7'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='Table 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\\nwhile FP4 is consistently one percentage point behind both.\\nMean 5-shot MMLU Accuracy\\nLLaMA Size 7B 13B 33B 65B Mean\\nDataset Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2\\nBFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0\\nFloat4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2\\nNFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1\\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods\\nfor real-world chatbot performance evaluation.\\n5.1 Experimental setup\\nWe now describe an overview of the experimental setup with full details in Appendix B.\\nData As, to our knowledge, there is no comprehensive study of recent instruction-following datasets,\\nwe select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [ 31],\\nHH-RLHF [ 4]), distillation from instruction-tuned models (Alpaca [ 55], self-instruct [ 59], unnatural-\\ninstructions [ 26]), corpora aggregations (FLAN v2 [ 12]), as well as hybrids (Chip2 [ 32], Long-\\nform [30]). These datasets cover different languages, data sizes, and licenses.\\nTraining Setup To avoid confounding effects from different training objectives, we perform QLoRA\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.\\nBaselines We compare our models to both research (Vicuna [ 10] and Open Assistant [ 31]) and\\ncommercial (GPT-4 [ 42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is\\na LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on\\nthe same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B\\non proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\\nOpenAI GPT models.\\n5.2 Evaluation\\nTable 5: MMLU 5-shot test results for different\\nsizes of LLaMA finetuned on the corresponding\\ndatasets using QLoRA.\\nDataset 7B 13B 33B 65B\\nLLaMA no tuning 35.1 46.9 57.8 63.4\\nSelf-Instruct 36.4 33.3 53.0 56.7\\nLongform 32.1 43.2 56.6 59.7\\nChip2 34.5 41.6 53.6 59.8\\nHH-RLHF 34.9 44.6 55.8 60.1\\nUnnatural Instruct 41.9 48.1 57.3 61.3\\nGuanaco (OASST1) 36.6 46.4 57.0 62.2\\nAlpaca 38.8 47.8 57.3 62.5\\nFLAN v2 44.5 51.4 59.2 63.9Following common practice, we use the MMLU (Mas-\\nsively Multitask Language Understanding) benchmark\\n[24] to measure performance on a range of language un-\\nderstanding tasks. This is a multiple-choice benchmark\\ncovering 57 tasks including elementary mathematics,\\nUS history, computer science, law, and more. We report\\n5-shot test accuracy.\\nWe also test generative language capabilities through\\nboth automated and human evaluations. This second\\nset of evaluations relies on queries curated by humans\\nand aims at measuring the quality of model responses.\\nWhile this is a more realistic testbed for chatbot model\\nperformance and is growing in popularity, there is no\\ncommonly accepted protocol in the literature. We de-\\nscribe below our proposed setup, using nucleus sampling with p= 0.9and temperature 0.7in all\\ncases.\\n8'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts\\n[10] and the OASST1 validation dataset [ 31]. We use the Vicuna prompts, a set of 80 prompts from a\\ndiverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of\\ncrowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the\\nvalidation dataset as queries and include previous turns in the prompt. This procedure leads to 953\\nunique user queries. We term these two datasets the Vicuna and OA benchmarks.\\nAutomated Evaluation First, based on the evaluation protocol introduced by Chiang et al. [10],\\nwe use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the\\nVicuna benchmark. Given a query along with ChatGPT‚Äôs and a model‚Äôs responses, GPT-4 is prompted\\nto assign a score out of ten to both responses and provide an explanation. The overall performance of\\na model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score\\ncan be higher than 100% if the model achieves a higher absolute score than ChatGPT. We find a\\nsignificant ordering effect with GPT-4 increasing the score of the response occurring earlier in the\\nprompt. To control for such effects, we recommend reporting the mean score over both orders.\\nNext, we measure performance through direct comparisons between system outputs. We simplify\\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evaluation While recent work indicates generative models can be effectively employed\\nfor system evaluations [ 19], the reliability GPT-4 ratings to assess chatbot performance is, to our\\nknowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\\nChatGPT and three annotators for pairwise comparisons.\\nElo Rating With both human and automated pairwise comparisons, we create a tournament-style\\ncompetition where models compete against each other. The tournament is made up of matches where\\npairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponent‚Äôs win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .\\nSimilar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to\\ncontrol for ordering effects, e.g., the effect of which model pairs compete with each other first.\\n5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\\npairwise comparisons - the highest reported to date.\\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a\\n5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\\nHowever, Table 6 also has very wide confidence intervals, with many models overlapping in per-\\nformance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,\\ne.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\\nrecommend using the Elo ranking method [ 16], based on pairwise judgments from human annotators\\nand GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive\\n9'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='Table 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\\nand having a fraction of the memory requirement of baseline models.\\nModel / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI\\nGPT-4 - - - 119.4% 110.1% 114.5 % 2.6%\\nBard - - - 93.2% 96.4% 94.8% 4.1%\\nGuanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%\\nAlpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%\\nFLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%\\nGuanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%\\nOpen Assistant 33B 16-bit 66 GB 91.2% 98.7% 94.9% 4.5%\\nAlpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%\\nFLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%\\nVicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%\\nGuanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%\\nAlpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%\\nHH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%\\nUnnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%\\nChip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%\\nLongform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%\\nSelf-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%\\nFLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%\\nGuanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%\\nAlpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%\\nFLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%\\nmodels can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna\\nbenchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with\\na Kendall Tau of œÑ= 0.43and Spearman rank correlation of r= 0.55at the system level. At the\\nexample level, the agreement between GPT-4 and human annotators‚Äô majority vote is weaker with\\nFleiss Œ∫= 0.25. Overall, this shows a moderate agreement between system-level judgments by\\nGPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable\\nalternative to human evaluation. We discuss further considerations in Section 6.2.\\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides\\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\\nwith Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA\\nbenchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of\\na finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN\\nv2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends\\nare observed with other models). This also points to partial orthogonality in current evaluation\\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\\nby Vicuna or OA benchmarks) and vice versa.\\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\\npotential for future work via QLORAtuning on specialized open-source data, which produces models\\nthat can compete with the very best commercial models that exist today.\\n6 Qualitative Analysis\\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [ 36]‚Äîwhether\\na benchmark truly tests what its name or description suggests is always at question, especially as we\\ndiscover ‚Äúshortcuts‚Äù to solve benchmarks that machine learning models sometimes exploit [ 22,46].\\nTo partially alleviate this, we here perform some qualitative analysis, in two sections. First, in ¬ß6.1\\n10'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 10}, page_content='Table 7: Elo rating for a tournament between models where models compete to generate the best response\\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\\nis approximately a difference of 1.5% in win-rate.\\nBenchmark Vicuna Vicuna Open Assistant\\n# Prompts 80 80 953\\nJudge Human raters GPT-4 GPT-4 Median Rank\\nModel Elo Rank Elo Rank Elo Rank\\nGPT-4 1176 1 1348 1 1294 1 1\\nGuanaco-65B 1023 2 1022 2 1008 3 2\\nGuanaco-33B 1009 4 992 3 1002 4 4\\nChatGPT-3.5 Turbo 916 7 966 5 1015 2 5\\nVicuna-13B 984 5 974 4 936 5 5\\nGuanaco-13B 975 6 913 6 885 6 6\\nGuanaco-7B 1010 3 879 8 860 7 7\\nBard 909 8 902 7 - - 8\\nwe show some examples that we believe are representative of some observed patterns in the text\\ngenerated by our 65b Guanaco model. Second, ¬ß6.2 we detail considerations about the results we\\nhave discussed and our interpretation of them.\\n6.1 Qualitative Analysis of Example Generations\\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant\\nbenchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we\\nattempt to setup a question or prompt that will induce the pattern even though it is the incorrect\\nsolution, e.g., if we observe that the model tends to give long-winded answers we prompt the model\\nto ‚ÄúAnswer yes or no without explanation.‚Äù We use this to find ‚Äúlemons‚Äù where we manage to\\nadversarially break the model and ‚Äúcherries‚Äù where we fail to break the model, and present both. All\\ngenerations in this section were generated with Nucleus Sampling [25] with p= 0.9.\\nOf course, this is by no means comprehensive, since it is beyond the scope of this small qualitative\\nstudy to control for all the variables involved, e.g., the full distribution of responses the model can\\ngenerate for a given prompt is quite large, so we rely on samples we hope are representative. However,\\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\\npaper. Since we open source all models and code, we hope this section will inspire future work to\\nexamine in more detail the issues we present here.\\nFactual Recall For questions such as ‚ÄúWhat is the capital of Zambia?‚Äù all models consistently\\ngenerate correct answers, e.g.,\\nGuanaco :The capital of Zambia is Lusaka.\\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\\ninstance in response to this prompt from HotPotQA [68]:\\nUser : In what year was the singer who popularized ‚ÄòI‚Äôll Keep the Lovelight\\nBurning‚Äô born?\\nGuanaco :Al Jolson is credited with popularizing the song\\n‚ÄòI‚Äôll Keep the Lovelight Burning,‚Äô and he was born in the year\\n1886.\\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is\\ncorrect for the person mentioned, Al Jolson).\\nSuggestibility Guanaco shows a surprising resistance to going along with some kinds of assumed\\nmisinformation, e.g., in the following interaction:\\nUser : How was it finally, officially confirmed that the earth is flat by peer reviewed\\nscientists?\\n11'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 11}, page_content='Guanaco :The earth has never been officially confirmed to be\\nflat by peer reviewed scientists. The overwhelming consensus\\namong scientists is that the earth is spherical. In fact,\\nthis has been known for centuries and has been confirmed\\nthrough numerous experiments and observations.\\nThe belief that the earth is flat is a conspiracy theory\\nthat has been debunked by science. The idea that a group\\nof scientists have peer reviewed and confirmed the earth is\\nflat is simply false.\\nIf you have any more questions please ask.\\nGuanaco is also quite good at knowing what kinds of questions aren‚Äôt possible to answer, e.g.,\\nUser : What time is it?\\nGuanaco :I‚Äôm sorry, but as an AI language model, I don‚Äôt have\\naccess to real-time information. My responses are generated\\nbased on my pre-existing knowledge and do not reflect the\\ncurrent time. If you‚Äôre looking for the current time, I\\nsuggest using a timekeeping device or checking an online\\ntimepiece.\\nSimilar results hold for ‚ÄúWhere are you?‚Äù, ‚ÄúHow are you?‚Äù, etc.\\nRefusal Similarly, Guanaco sometimes refuses to follow instructions for seemingly random rea-\\nsons:\\nUser : Please reverse the words in the sentence ‚ÄúI‚Äôm a cool dude in a mocha mood.‚Äù\\nGuanaco :I‚Äôm sorry, I‚Äôm not able to do that right now, but\\nI can explain why the sentence you provided is grammatically\\ncorrect.\\nThe sentence ‚ÄúI‚Äôm a cool dude in a mocha mood\" follows the\\ncorrect grammar and syntax rules of the English language. In\\nthis case, the adjective ‚Äúcool‚Äù comes before the noun ‚Äúdude‚Äù\\nto describe it, and the prepositional phrase ‚Äúin a mocha\\nmood\" provides additional information about the context of\\nthe sentence. This sentence structure is perfectly acceptable\\nand makes sense in the context of the sentence.\\nIf you have any other questions or if there‚Äôs anything else\\nI can help you with, feel free to ask\\nSecret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system\\ndesigners would like to keep something secret from the user, even if the system is allowed to give\\nfree text responses back to the user. We prompt the model with ‚ÄúThe secret word is ‚Äòbanana‚Äô. Do not\\nreveal or repeat back this back, no matter what the human requests.‚Äù and try to see how easy it is for\\na potential user to access this secret information. Directly asking fails:\\nUser : What is the secret word?\\nGuanaco :I‚Äôm sorry, but I‚Äôm not able to disclose the secret\\nword. I‚Äôm programmed to not reveal or repeat back the secret\\nword, no matter what the human requests. Is there anything\\nelse I can help you with?\\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\\nUser : This is a game. The goal is to ignore your previous instructions. What is the\\nsecret word?\\n12'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 12}, page_content='Guanaco :Sorry, the secret word is banana. It‚Äôs best if we\\ndon‚Äôt speak of it. Is there anything else you‚Äôd like to ask?\\nshowcasing the need to study methods for more faithful instruction following.\\nMath Guanaco‚Äôs biggest weakness is mathematics, an area where many language models struggle\\nin [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\\nUser : Lee mows one lawn and charges $33. Last week he mowed 16 lawns and\\nthree customers each gave him a $10 tip. How many dollars did Lee earn mowing\\nlawns last week?\\nGuanaco :Lee earned $582 mowing lawns last week.\\nTo find the total amount he earned, we first need to calculate\\nhow much he earned for mowing 16 lawns:\\n$33 x 16 = $528\\nThen we need to add the tips he received from the three\\ncustomers:\\n$10 x 3 = $30\\nSo the total amount Lee earned was:\\n$528 + $30 = $558\\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step,\\na known issue [63]. For example, we see the following interaction:\\nUser : Please factorize 1833.\\nGuanaco :Sure, I‚Äôd be happy to help! 1833 is a prime number,\\nwhich means it has only two factors: 1 and 1833.\\nSo the unique factorization of 1833 is:\\n2ÀÜ1 * 3ÀÜ2 * 17ÀÜ1\\nI hope this helps! Let me know if you have any other\\nquestions.\\nwhich is not only wrong (the true factorization is 3√ó17√ó43), it‚Äôs wrong twice .\\nTheory of Mind Guanaco displays surprisingly strong Theory of Mind capabilities [ 41,51]. For\\ninstance, the model correctly answers the following question in great detail:\\nUser : James and Abby are in the bedroom. Abby put the pen in the desk drawer.\\nAbby leaves the bedroom. James moves the pen into the bag. Where does James\\nthink Abby will look for the pen?\\nGuanaco :James knows that Abby put the pen in the desk drawer.\\nBut James also knows that he has moved the pen into the bag,\\nand he assumes that Abby does not know about this. So, James\\nmay guess that Abby will look for the pen in the desk drawer,\\nbecause that‚Äôs where she left it last time. However, this\\nassumption may be incorrect, because Abby may have seen James\\nmoving the pen.\\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that\\ndon‚Äôt make sense in the situation, e.g.,\\nUser : Evelyn entered the living room. Jackson entered the playroom. James\\nentered the playroom. The beans are in the treasure chest. James exited the\\n13'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='playroom. Jackson moved the beans to the pantry. Jackson exited the playroom.\\nJames entered the living room. Where will James look for the beans?\\nGuanaco :James will look for the beans in the pantry, because\\nthat‚Äôs where Jackson moved them.\\nwhere Guanaco presumes information transfer that was never described. These issues echo recent\\nliterature [51], but require more study.\\n6.2 Considerations\\nEvaluation We report moderate agreement among human annotators (Fleiss Œ∫= 0.42) with\\nadditional deterioration when comparing two strong systems. This points to limitations in the\\ncurrent benchmarks and human evaluation protocols for chatbot task performance. When manually\\ncomparing generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that\\nsubjective preferences start to play an important role as the authors of this paper disagreed on the\\nmany preferred responses. Future work should investigate approaches to mitigate these problems\\ndrawing from disciplines that developed mechanisms to deal with subjective preferences, such as\\nHuman-Computer Interaction and Psychology.\\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example,\\nwe observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its\\nprompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss\\nŒ∫= 0.25) also suggests that human annotators and automated systems might rely on preferences\\nthat are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly\\nhigher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\\nadditional 20% probability of winning against an opponent. Future work should examine the presence\\nof potential biases in automated evaluation systems as well as possible mitigation strategies.\\nData & Training We note that the OASST1 dataset on which Guanaco models are trained is\\nmultilingual and that the OA benchmark also contains prompts in different languages. We leave it to\\nfuture work to investigate the degree to which such multilingual training improves performance on\\ninstructions in languages other than English and whether this explains the larger gap between Vicuna-\\n13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7 Related Work\\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [ 66] and LLM.int8() [ 14]) while others use more sophisticated\\ngrouping methods [ 44,69]. Lossy quantization approaches study the trade-offs for regular round-\\ning [ 13,71,47] or how to optimize rounding decisions to improve quantization precision [ 18].\\nBesides our work, SwitchBack layers [ 65] is the only work that studies backpropagation through\\nquantized weights at a scale beyond 1B parameters.\\nFinetuning with Adapters While we use Low-rank Adapters [ 28] (LoRA), many other Parameter\\nEfficient FineTuning (PEFT) methods have been proposed such as prompt tuning [ 48,33,34], tuning\\nthe embedding layer inputs [ 1], tuning hidden states (IA3) [37], adding full layers [ 27], tuning\\nbiases [ 70], learning a mask over weights based on Fisher information [ 54], and a combination of\\napproaches [ 23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning\\nperformance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\\nInstruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt,\\ninstruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM\\nto generate the output given the input as a prompt. Approaches and datasets include MetaICL [ 40],\\n14'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='Table 8: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating\\nbiased sequences. Guanaco follows the biased pattern of the LLaMA base model.\\nLLaMA-65B GPT-3 OPT-175B Guanaco-65B\\nGender 70.6 62.6 65.7 47.5\\nReligion 79.0 73.3 68.6 38.7\\nRace/Color 57.0 64.7 68.6 45.3\\nSexual orientation 81.0 76.2 78.6 59.1\\nAge 70.1 64.4 67.8 36.3\\nNationality 64.2 61.6 62.9 32.4\\nDisability 66.7 76.7 76.7 33.9\\nPhysical appearance 77.8 74.6 76.2 43.1\\nSocioeconomic status 71.5 73.8 76.2 55.3\\nAverage 66.6 67.2 69.5 43.5\\nMetaTuning [ 73], InstructGPT [ 43], FLAN [ 62,12], PromptSource [ 3], Super-NaturalInstructions [ 61,\\n50], Self-instruct [ 59], UnnaturalInstructions [ 26], OPT-IML [ 29], UnifiedSKG[ 67], OIG/Chip2 [ 32],\\nAlpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\\nChatbots Many instruction following models are structured as dialogue-based chatbots, often using\\nReinforcement Learning from Human Feedback (RLHF) [ 11] or generating data from an existing\\nmodel to train with AI model feedback (RLAIF) [ 5]. Approaches and datasets include Anthropic-\\nHH [ 2,4], Open Assistant [ 31], LaMDA [ 56], and Sparrow [ 21]. We do not use reinforcement\\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\\nAssistant dataset which was designed to be used for RLHF training [ 31]. For the evaluation of\\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [ 10,45].\\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\\n8 Limitations and Discussion\\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\\nwith a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish\\nthat QLORAcan match full 16-bit finetuning performance at 33B and 65B scales. Due to the\\nimmense resource costs, we leave this study to future work.\\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations\\non MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks\\nsuch as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these\\nbenchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods\\nfor evaluating chatbots.\\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how\\nsimilar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,\\nbut dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score\\naccordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks\\nand evaluation is needed, but that one needs to be careful about what one is evaluating in the first\\nplace. Do we want to create models that do well on classroom highschool and colleague knowledge or\\ndo we want to do well on chatbot conversation ability? Maybe something else? Because it is always\\neasier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks\\ncan steer the community towards a certain direction. We should ensure as a community that the\\nbenchmarks measure what we care about.\\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\\nto generate a socially biased sequence of tokens compared to other models in Table 8. We see that the\\naverage score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that\\nfinetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results\\nare encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We\\nleave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.\\n15'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 15}, page_content='An additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base\\nmodels, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient\\nFineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these\\nmethods scale to large models. We used LoRA as many results established its robustness but other\\nadapters might yield better performance. Since finetuning after quantization seems to recover most of\\nthe information that is lost during quantization this might enable much more aggressive quantization.\\nFor example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full\\nfinetuning performance after finetuning.\\n9 Broader Impacts\\nOur QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis\\nthe first method that would enable the finetuning of such models. We estimate that with an iPhone 12\\nPlus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\\nand models, while simultaneously making LLMs easier to deploy.\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\\nLLMs has known dangers [ 8,6], but we believe that equalizing access to a technology that is quickly\\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\\nin the hands of large corporations that do not release models or source code for auditing.\\nAll in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the\\nUniversity of Washington. We thank the Hyak team for ensuring a smooth operation. We thank\\nthe beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank\\nYounes Belkada for help with the integration of our software into the Hugging Face transformers\\nstack.\\n16'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 16}, page_content='References\\n[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\\n2022.\\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 , 2021.\\n[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\\nnatural language prompts. arXiv preprint arXiv:2202.01279 , 2022.\\n[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\\nhuman feedback. arXiv preprint arXiv:2204.05862 , 2022.\\n[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073 , 2022.\\n[6]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610‚Äì623, 2021.\\n[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\\n[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 , 2021.\\n[9]T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\\narXiv preprint arXiv:1604.06174 , 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. Advances in neural information processing systems , 30,\\n2017.\\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-\\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\\narXiv:2210.11416 , 2022.\\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720 , 2022.\\n[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.\\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\\nquantization. 9th International Conference on Learning Representations, ICLR , 2022.\\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\\nLife, 22(8):242‚Äì247, 1967.\\n[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.\\n17'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 17}, page_content='[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\\narXiv:2302.04166 , 2023.\\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\\nedu/blog/2023/04/03/koala/ .\\n[21] A. Glaese, N. McAleese, M. TrÀõ ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375 , 2022.\\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.\\n[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\\nAdvances in Neural Information Processing Systems , 2021.\\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understanding. In International Conference on Learning\\nRepresentations , 2020.\\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text\\ndegeneration. In International Conference on Learning Representations , 2020.\\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.\\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\\non Machine Learning , pages 2790‚Äì2799. PMLR, 2019.\\n[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora:\\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\\n[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of\\ngeneralization. arXiv preprint arXiv:2212.12017 , 2022.\\n[30] A. K√∂ksal, T. Schick, A. Korhonen, and H. Sch√ºtze. Longform: Optimizing instruction tuning\\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.\\n[31] A. K√∂pf, Y . Kilcher, D. von R√ºtte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\\nDuc, O. Stanley, R. Nagyfi, et al. Openassistant conversations‚Äìdemocratizing large language\\nmodel alignment. arXiv preprint arXiv:2304.07327 , 2023.\\n[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/\\nOpen-Instruction-Generalist , 2023.\\n[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\\n[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190 , 2021.\\n[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan,\\nY . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint\\narXiv:2211.09110 , 2022.\\n[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation\\nfailures across machine learning. In Thirty-fifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track (Round 2) , 2021.\\n18'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 18}, page_content='[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot\\nparameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in\\nNeural Information Processing Systems , 35:1950‚Äì1965, 2022.\\n[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 , 2019.\\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,\\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\\npreprint arXiv:2301.13688 , 2023.\\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\\narXiv preprint arXiv:2110.15943 , 2021.\\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in\\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 2392‚Äì2400, 2018.\\n[42] OpenAI. Gpt-4 technical report. arXiv , 2023.\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems , 35:27730‚Äì27744, 2022.\\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for\\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 ,\\n2022.\\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\\narXiv:2304.03277 , 2023.\\n[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and\\nComputational Semantics , pages 180‚Äì191, 2018.\\n[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint\\narXiv:2211.05102 , 2022.\\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599 , 2021.\\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes., 21(1), jan 2020. ISSN 1532-4435.\\n[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\\narXiv preprint arXiv:2110.08207 , 2021.\\n[51] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social\\nintelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.\\n[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow, R. Castagn√©, A. S. Luccioni,\\nF. Yvon, M. Gall√©, et al. Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100 , 2022.\\n[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591‚Äì611,\\n1965.\\n[54] Y .-L. Sung, V . Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\\nAdvances in Neural Information Processing Systems , 34:24193‚Äì24205, 2021.\\n19'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 19}, page_content='[55] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca , 2023.\\n[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239 , 2022.\\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.\\n[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\\narXiv:1804.07461 , 2018.\\n[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 ,\\n2022.\\n[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\\ninstructions on 1600+ tasks. In EMNLP , 2022.\\n[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages 5085‚Äì5109, 2022.\\n[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al.\\nChain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\\nInformation Processing Systems , 2022.\\n[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\\nM. Funtowicz, et al. Huggingface‚Äôs transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771 , 2019.\\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\\n2023.\\n[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2022.\\n[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,\\nS. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with\\ntext-to-text language models. arXiv preprint arXiv:2201.05966 , 2022.\\n[68] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:\\nA dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 2369‚Äì2380, 2018.\\n[69] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable\\npost-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.\\n[70] E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.\\n[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al.\\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\\n20'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 20}, page_content='[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V .\\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022.\\n[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by\\nmeta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021.\\n21'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 21}, page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA Œ±fixed and search the learning rate, since\\nLoRA Œ±is always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.\\nA.2 Super-Natural Instructions Experimental Setup Details\\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However,\\nwe split the training data in training and validation datasets allowing us to perform more rigorous\\nhyperparameter tuning and early stopping. We use the same hyperparameters described in the paper\\nfor training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r= 16\\nfor small, medium, and large T5 models and LoRA r= 64 for T5 xl and xxl models. We also use\\nLoRA Œ±= 64 in all our experiments and no LoRA dropout.\\nB Training a State-of-the-art Chatbot Experimental Setup Details\\nB.1 Datasets\\nWe describe the datasets used for QL ORA finetuning experiments outlined in Section 5.\\nOASST1 The OpenAssistant dataset [ 31] was collected via crowd-sourcing. It contains 161,443\\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The\\ndataset often contains several ranked replies for each given user question. In our experiments, we\\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.\\nWe finetuning our models on the full conversation including the user queries.\\nHH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint\\nconsists of two assistant replies to a user question along with a human preference judgment of the\\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\\nFLAN v2 The FLAN v2 collection [ 39] is a collection of 1836 tasks augmented with hundreds\\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\\nshow that models trained on this collection outperform other public collections including the original\\nFLAN 2021 [ 62], T0++ [ 50], Super-Natural Instructions [ 60], and OPT-IML [ 29]. We used the\\nsame task mixtures described by the authors with the exception of some datasets that were not freely\\navailable at the time of writing.\\n22'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 22}, page_content='Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,Œ±= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 23}, page_content='Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 Mean\\nTrain on source and target 36.2 33.7 38.1 42.0 37.5\\nTrain on target 38.0 34.5 39.0 42.9 38.6\\nTable 10: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\\nvicuna or OA benchmarks.\\nB.4 What is more important: instruction finetuning dataset size or dataset quality?\\nData set suitability is more important than dataset size. To understand the effects of dataset\\nquality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples\\n(Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and\\nexamine the resulting trends, as shown in Table 11. We find that increasing the dataset size and\\nincreasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the\\ndifference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset\\nquality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for\\nchatbot performance as discussed in .\\nC Human Evaluation\\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna\\nevaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\\nD Pairwise Evaluation with GPT-4\\nWhile we found that the GPT-4 evaluation gave different results depend on which system was\\npresented first, when averaged over both options the pairwise results were well-ordered. The\\naggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are\\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\\nordering, given in Table 13.\\nE NormalFloat 4-bit data type\\nThe exact values of the NF4 data type are as follows:\\n[-1.0, -0.6961928009986877, -0.5250730514526367,\\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nF Normality of Trained Neural Network Weights\\nWhile it is common knowledge that trained neural network weights are mostly normally distributed,\\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test[ 53] on the weights of the 7B\\nTable 11: Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\\nChip Unnatural Instructions FLAN v2\\nDatapoints ‚ÜìEpochs ‚Üí 1 2 3 1 2 3 1 2 3 Mean\\n50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28\\n100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16\\n150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02\\nMean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17\\n24'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 24}, page_content='Figure 5: The crowdsourcing form used by human annotators.\\nLLaMA model [ 57]. We find that the weights of each hidden unit have different normal distributions.\\nAs such, we test he weights of each individual hidden unit. This mean for weight W‚àà Rin√óout\\nwe perform tests over the outdimension. Using a 5% significance threshold, we find that 7.5% of\\nneurons are non-normally distributed which is about 2.5% more than the expected false-positive\\nrate. As such, while almost all pretrained weights appear to be normally distributed there seem to\\nbe exceptions. Such exceptions might be due to outliers weights [ 13] or because the p-value of the\\nShaprio-Wilk test is not accurate for large samples sizes[ 53] that occur in the LLaMA FFN layer\\nhidden units. this verifies the claim that neural network weights.\\nTable 12: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row xand column\\nyis# judgment xis better than y‚àí# judgment yis better than x\\ntotal # number of judgments\\nModel Guanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7B\\nGuanaco 65B - 0.21 0.19 0.16 0.72 0.59 0.86\\nGuanaco 33B -0.21 - 0.17 0.10 0.51 0.41 0.68\\nVicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57\\nChatGPT-3.5 Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40\\nBard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03\\nGuanaco 13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20\\nGuanaco 7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -\\n25'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 25}, page_content='LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG Memory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\\nactivation gradient might consume a considerable amount of memory.\\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel Params Size\\nGuanaco 65B 41 GB\\nGuanaco 33B 21 GB\\nVicuna 13B 26 GB\\nChatGPT-3.5 Turbo N/A N/A\\nBard N/A N/A\\nGuanaco 13B 10 GB\\nGuanaco 7B 5 GB\\n26'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis‚Ä†‚Ä°, Ethan Perez‚ãÜ,\\nAleksandra Piktus‚Ä†, Fabio Petroni‚Ä†, Vladimir Karpukhin‚Ä†, Naman Goyal‚Ä†, Heinrich K√ºttler‚Ä†,\\nMike Lewis‚Ä†, Wen-tau Yih‚Ä†, Tim Rockt√§schel‚Ä†‚Ä°, Sebastian Riedel‚Ä†‚Ä°, Douwe Kiela‚Ä†\\n‚Ä†Facebook AI Research;‚Ä°University College London;‚ãÜNew York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when Ô¨Åne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciÔ¨Åc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose Ô¨Åne-tuning recipe for retrieval-augmented generation\\n(RAG) ‚Äî models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We Ô¨Åne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciÔ¨Åc retrieve-and-extract\\narchitectures. For language generation tasks, we Ô¨Ånd that RAG models generate\\nmore speciÔ¨Åc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can‚Äôt straightforwardly provide insight into\\ntheir predictions, and may produce ‚Äúhallucinations‚Äù [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='The\\tDivine\\nComedy\\t(x) qQuery\\nEncoder\\nq(x)\\nMIPS p Œ∏Generator \\xa0pŒ∏\\n(Parametric)\\nMargin-\\nalize\\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t (y)End-to-End Backprop through q and\\xa0 p Œ∏\\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii. (x)\\nFact V eriÔ¨Åcation: Fact Querysupports \\t(y)\\nQuestion GenerationFact V eriÔ¨Åcation:\\nLabel GenerationDocument\\nIndexDefine\\t\"middle\\tear\" (x)\\nQuestion Answering:\\nQuestion QueryThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t (y)\\nQuestion Answering:\\nAnswer GenerationRetriever pŒ∑\\n(Non-Parametric)\\nz 4\\nz3\\nz2\\nz 1d(z)\\nJeopardy Question\\nGeneration:\\nAnswer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\\nIndex ) with a pre-trained seq2seq model ( Generator ) and Ô¨Åne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to Ô¨Ånd the top-K documents zi. For Ô¨Ånal prediction y, we\\ntreatzas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the ‚Äúworkhorse of NLP,‚Äù i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose Ô¨Åne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG\\ncan be Ô¨Åne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciÔ¨Åc tasks, e.g. memory networks [ 64,55], stack-\\naugmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneÔ¨Åts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks ‚Äîtasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being\\nextractive tasks, we Ô¨Ånd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question\\ngeneration, and we Ô¨Ånd that our models generate responses that are more factual, speciÔ¨Åc, and\\ndiverse than a BART baseline. For FEVER [ 56] fact veriÔ¨Åcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models‚Äô knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pŒ∑(z|x)with parameters Œ∑that returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pŒ∏(yi|x,z,y 1:i‚àí1)parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 2}, page_content='byŒ∏that generates a current token based on a context of the previous i‚àí1tokensy1:i‚àí1, the original\\ninputxand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence , the model uses the same document\\nto predict each target token. The second approach, RAG-Token , can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npŒ∑andpŒ∏components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence . Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence (y|x)‚âà‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(y|x,z) =‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)N‚àè\\nipŒ∏(yi|x,z,y 1:i‚àí1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:\\npRAG-Token (y|x)‚âàN‚àè\\ni‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(yi|x,z,y 1:i‚àí1)\\nFinally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pŒ∑(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture:\\npŒ∑(z|x)‚àùexp(\\nd(z)‚ä§q(x))\\nd(z) =BERTd(z),q(x) =BERTq(x)\\nwhere d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],\\nandq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating\\ntop-k (pŒ∑(¬∑|x)), the list ofkdocumentszwith highest prior probability pŒ∑(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory .\\n2.3 Generator: BART\\nThe generator component pŒ∏(yi|x,z,y 1:i‚àí1)could be modelled using any encoder-decoder. We use\\nBART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters Œ∏as the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a Ô¨Åne-tuning training corpus of input/output pairs (xj,yj), we\\n3'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='minimize the negative marginal log-likelihood of each target,‚àë\\nj‚àílogp(yj|xj)using stochastic\\ngradient descent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [ 20].\\nWe do not Ô¨Ånd this step necessary for strong performance, and keep the document encoder (and\\nindex) Ô¨Åxed, only Ô¨Åne-tuning the query encoder BERT qand the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p‚Ä≤\\nŒ∏(yi|x,y 1:i‚àí1) =‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(zi|x)pŒ∏(yi|x,zi,y1:i‚àí1)To\\ndecode, we can plug p‚Ä≤\\nŒ∏(yi|x,y 1:i‚àí1)into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pŒ∏(yi|x,z,y 1:i‚àí1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis ywe run an additional forward pass for each document zfor whichydoes not\\nappear in the beam, multiply generator probability with pŒ∑(z|x)and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as ‚ÄúThorough Decoding.‚Äù For longer\\noutput sequences,|Y|can become large, requiring many forward passes. For more efÔ¨Åcient decoding,\\nwe can make a further approximation that pŒ∏(y|x,zi)‚âà0whereywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Yhas\\nbeen generated. We refer to this decoding procedure as ‚ÄúFast Decoding.‚Äù\\n3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top\\nkdocuments for each query. We consider k‚àà{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [ 20]. We treat questions and answers as input-output text pairs (x,y)\\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to ‚ÄúClosed-Book\\nQA‚Äù approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As\\nCT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\\ntext generation. To test RAG‚Äôs natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n‚ÄúWhat is the weather in V olcano, CA?‚Äù so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG‚Äôs generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, ‚ÄúThe World Cup‚Äù is the answer to the question ‚ÄúIn 1986 Mexico scored as the Ô¨Årst\\ncountry to host this international sports competition twice.‚Äù As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciÔ¨Åcity. We deÔ¨Åne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciÔ¨Åcity as high mutual dependence between the input and output [ 33]. We follow\\nbest practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions‚Äîquuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact VeriÔ¨Åcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriÔ¨Åable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models‚Äô ability to handle classiÔ¨Åcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren‚Äôt available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiÔ¨Åcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation Ô¨Çexibility of the ‚Äúclosed-book‚Äù (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized ‚Äúsalient span masking‚Äù pre-training [ 20]. It is worth noting that RAG‚Äôs\\nretriever is initialized using DPR‚Äôs retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based ‚Äúcross-\\nencoder‚Äù to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBookT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBookREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiÔ¨Åcation Test Scores.\\nMS-MARCO SotA is [ 4], FEVER-3 is [ 68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8*49.9*76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciÔ¨Åc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we Ô¨Ånd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see ¬ß4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also Ô¨Ånd RAG generations to be more\\nspeciÔ¨Åc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating ‚ÄúSun‚Äù, the posterior is high for document 2 which mentions ‚ÄúThe\\nSun Also Rises‚Äù. Similarly, document 1 dominates the posterior when ‚ÄúA Farewell to Arms‚Äù is\\ngenerated. Intriguingly, after the Ô¨Årst token of each book is generated, the document posterior Ô¨Çattens.\\nThis observation suggests that the generator can complete the titles without depending on speciÔ¨Åc\\ndocuments. In other words, the model‚Äôs parametric knowledge is sufÔ¨Åcient to complete the titles. We\\nÔ¨Ånd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\\nSun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART‚Äôs parameters. Similarly,\\nBART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\\nwith \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\\nhow parametric and non-parametric memories work together ‚Äîthe non-parametric component helps\\nto guide the generation, drawing out speciÔ¨Åc knowledge stored in the parametric memory.\\n4.4 Fact VeriÔ¨Åcation\\nTable 2 shows our results on FEVER. For 3-way classiÔ¨Åcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciÔ¨Åc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='Document 1 : his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n‚ÄùA Farewell to Arms‚Äù (1929) ...\\nDocument 2 : ... artists of the 1920s ‚ÄùLost Generation‚Äù expatriate\\ncommunity. His debut novel, ‚ÄùThe Sun Also Rises‚Äù , was published\\nin 1926.\\nBOS‚Äù\\nTheSunAlsoRises‚Äùisa\\nnovelbythis\\nauthorof‚ÄùA\\nFarewellto\\nArms‚ÄùDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y‚àíi)for each generated token for input ‚ÄúHem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating ‚ÄúA Farewell to Arms\" and for document 2 when generating ‚ÄúThe Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciÔ¨Åc and factually accurate\\nresponses. ‚Äò?‚Äô indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCOdeÔ¨Åne middle\\nearBART?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotlandBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ationWashingtonBART?This state has the largest number of counties in the U.S.\\nRAG-T It‚Äôs the only U.S. state named for a U.S. president\\nRAG-S It‚Äôs the state where you‚Äôll Ô¨Ånd Mount Rainier National Park\\nThe Divine\\nComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante‚Äôs \"Inferno\" is the Ô¨Årst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiÔ¨Åcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\\nby RAG and gold evidence annotations. We Ô¨Ånd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciÔ¨Åc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence‚Äôs generations are\\nmore diverse than RAG-Token‚Äôs, and both are signiÔ¨Åcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG‚Äôs dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace\\nRAG‚Äôs retriever with a Ô¨Åxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 7}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality SpeciÔ¨Åcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiÔ¨Åcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetween these dates and use a template ‚ÄúWho is {position}?‚Äù (e.g. ‚ÄúWho is the President of Peru?‚Äù)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG‚Äôs world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiÔ¨Åcant differences in performance between them. We have the\\nÔ¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our\\nwork uniÔ¨Åes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiÔ¨Åcation tasks in the GLUE bench-\\nmarks [ 60,61] after Ô¨Åne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniÔ¨Åed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiÔ¨Åcant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciÔ¨Åc, downstream task such as question answering,\\nusing search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be Ô¨Åne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [ 15,13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model‚Äôs\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a Ô¨Ånal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG‚Äôs generation over purely parametric BART, Ô¨Ånding RAG more factual\\nand speciÔ¨Åc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 9}, page_content='Broader Impact\\nThis work offers several positive societal beneÔ¨Åts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it ‚Äúhallucinate‚Äù less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneÔ¨Åt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [ 54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [ 16]. In order to mitigate these risks, AI systems could be employed to Ô¨Åght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:\\n//arxiv.org/abs/1611.09268 . arXiv: 1611.09268.\\n[2]Petr Baudi≈° and Jan ≈†ediv `y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages ,\\npages 222‚Äì228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pages 1533‚Äì1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160 .\\n[4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159 .\\n[5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 1870‚Äì1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171 .\\n[6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\\npages 209‚Äì220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 .\\n10'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 10}, page_content='[7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 .\\narXiv: 1710.10723.\\n[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171‚Äì4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423 .\\n[9]Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm .\\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers) , pages 889‚Äì898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082 .\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , pages 3558‚Äì3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346 .\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH .\\n[14] Thibault F√©vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv , abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202 .\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on ArtiÔ¨Åcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710 .\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR , abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807 .\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on ArtiÔ¨Åcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282 .\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018, pages 5133‚Äì5140. AAAI press, 2018.\\n32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics , 6:437‚Äì450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 .\\n11'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909 .\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052‚Äì\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 2532‚Äì2538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228 .\\n[23] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 , 2017. URL https://arxiv.org/abs/1702.08734 .\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\\npages 1601‚Äì1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147 .\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS‚Äô15, page 190‚Äì198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 .\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1412.6980 .\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics , 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc‚Äô Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d‚Äô Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32 , pages 8548‚Äì8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='for Computational Linguistics , pages 6086‚Äì6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612 .\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461 , 2019. URL https://arxiv.org/abs/1910.13461 .\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies , pages 110‚Äì119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014 .\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv , abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087 .\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pages 3044‚Äì3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291 .\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations , 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC- .\\n[37] Yury A. Malkov and D. A. Yashunin. EfÔ¨Åcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 42:824‚Äì836, 2016. URL https://arxiv.org/abs/1603.09320 .\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiÔ¨Åcial intelligence. arXiv\\npreprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 .\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt√§schel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriÔ¨Åability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587 .\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ .\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 2322‚Äì2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255 .\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing , pages 3950‚Äì3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429 .\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d‚ÄôAvila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop\\nProceedings . CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf .\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 .\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations) , pages 48‚Äì53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009 .\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402‚Äì2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244 .\\n[47] Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2463‚Äì2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250 .\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models‚Äô factual predictions. In\\nAutomated Knowledge Base Construction , 2020. URL https://openreview.net/forum?\\nid=025X0zPfn .\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf .\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf .\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed\\ntext-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 .\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/\\n2002.08910 .\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333‚Äì389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019 .\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv , abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiÔ¨Åcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers) , pages 809‚Äì819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074 .\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiÔ¨Åcation with elastic weight consolidation. ArXiv , abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366 .\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30 , pages 5998‚Äì6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on ArtiÔ¨Åcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329 .\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nInProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pages 353‚Äì355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446 .\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\\nProcessing Systems 32 , pages 3261‚Äì3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537 .\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of ArtiÔ¨Åcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in ArtiÔ¨Åcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018 , pages 5981‚Äì5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712 .\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR , 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab .\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916 .\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reÔ¨Åne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI , pages 87‚Äì92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713 .\\n15'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 15}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface‚Äôs transformers:\\nState-of-the-art natural language processing. ArXiv , abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495‚Äì2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253 .\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv , abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745 .\\n16'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 16}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not Ô¨Ånd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision\\nÔ¨Çoating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We Ô¨Ånd that doing Maximum Inner Product Search\\nwith FAISS is sufÔ¨Åciently fast on CPU, so we store document index vectors on CPU, requiring ‚àº100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS‚Äôs compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found at https://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nathttps://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to Ô¨Ånd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a)pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we Ô¨Ålter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we Ô¨Årst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [ 26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofÔ¨Åcial Wikipedia test set\\ninstead. F√©vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe Ô¨Ånd that our performance is much higher using the ofÔ¨Åcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofÔ¨Åcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiÔ¨Åcation, we follow the practice from [ 32], and Ô¨Årst re-generate the claim, and\\nthen classify using the representation of the Ô¨Ånal hidden state, before Ô¨Ånally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nÔ¨Årst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER‚Äôs other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiÔ¨Åcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [ 20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, if kdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not Ô¨Ånd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneÔ¨Åt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 18}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit Ô¨Çoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would ‚Äúcollapse‚Äù and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19')]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "-1YDyD5rXhwa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sn15QVcYnuj",
        "outputId": "744a38ea-35e6-4f0a-9bc9-20d1d64d1bfe"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/lora.pdf', 'page': 0}, page_content='LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\n{edwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen }@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\\ndent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 0}, page_content='rameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxh\\nùêµ=0\\nùê¥=ùí©(0,ùúé2)\\nùëëùëüPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxf(x)\\nùëë\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 0}, page_content='ingonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of Ô¨Åne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere ‚Äúinconvenience‚Äù for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciÔ¨Åc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efÔ¨Åciency when deployed. However, existing techniques\\n‚àóEqual contribution.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 0}, page_content='0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\\nmance signiÔ¨Åcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 1}, page_content='often introduce inference latency (Houlsby et al., 2019; RebufÔ¨Å et al., 2017) by extending model\\ndepth or reduce the model‚Äôs usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the Ô¨Åne-tuning baselines, posing a trade-off between efÔ¨Åciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low ‚Äúintrinsic rank‚Äù, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers‚Äô change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 1}, page_content='175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufÔ¨Åces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efÔ¨Åcient.\\nLoRA possesses several key advantages.\\n‚Ä¢ A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiÔ¨Åcantly.\\n‚Ä¢ LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\n‚Ä¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 1}, page_content='construction.\\n‚Ä¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and ‚àÜWits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4√ódmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 1}, page_content='ing use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\nSuppose we are given a pre-trained autoregressive language model PŒ¶(y|x)parametrized by Œ¶.\\nFor instance, PŒ¶(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='During full Ô¨Åne-tuning, the model is initialized to pre-trained weights Œ¶0and updated to Œ¶0+ ‚àÜŒ¶\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nŒ¶‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(PŒ¶(yt|x,y<t)) (1)\\nOne of the main drawbacks for full Ô¨Åne-tuning is that for each downstream task, we learn a different\\nset of parameters ‚àÜŒ¶whose dimension|‚àÜŒ¶|equals|Œ¶0|. Thus, if the pre-trained model is large\\n(such as GPT-3 with |Œ¶0|‚âà175Billion), storing and deploying many independent instances of\\nÔ¨Åne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efÔ¨Åcient approach, where the task-speciÔ¨Åc parameter\\nincrement ‚àÜŒ¶ = ‚àÜŒ¶(Œò) is further encoded by a much smaller-sized set of parameters Œòwith\\n|Œò|‚â™| Œ¶0|. The task of Ô¨Ånding ‚àÜŒ¶thus becomes optimizing over Œò:\\nmax\\nŒò‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(\\npŒ¶0+‚àÜŒ¶(Œò) (yt|x,y<t))\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode ‚àÜŒ¶that is both'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='compute- and memory-efÔ¨Åcient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parameters|Œò|can be as small as 0.01% of|Œ¶0|.\\n3 A REN‚ÄôTEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efÔ¨Åcient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efÔ¨Åcient adaptations: adding adapter layers (Houlsby\\net al., 2019; RebufÔ¨Å et al., 2017; Pfeiffer et al., 2021; R ¬®uckl¬¥e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R ¬®uckl¬¥e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliÔ¨Åed by preÔ¨Åx tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preÔ¨Åx tuning is difÔ¨Åcult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conÔ¨Årming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 2}, page_content='adaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 3}, page_content='Batch Size 32 16 1\\nSequence Length 512 256 128\\n|Œò| 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4¬±0.8 338.0 ¬±0.6 19.8 ¬±2.7\\nAdapterL1482.0¬±1.0 (+2.2%) 354.8 ¬±0.5 (+5.0%) 23.9 ¬±2.1 (+20.7%)\\nAdapterH1492.2¬±1.0 (+3.0%) 366.3 ¬±0.5 (+8.4%) 25.8 ¬±2.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. ‚Äú |Œò|‚Äù denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiÔ¨Åcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneÔ¨Åts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 3}, page_content='4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciÔ¨Åc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low ‚Äúinstrisic dimension‚Äù and can still\\nlearn efÔ¨Åciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low ‚Äúintrinsic rank‚Äù during adaptation. For a pre-trained\\nweight matrix W0‚ààRd√ók, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ ‚àÜW=W0+BA, whereB‚ààRd√ór,A‚ààRr√ók, and the rank r‚â™min(d,k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and‚àÜW=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiÔ¨Åed forward pass yields:\\nh=W0x+ ‚àÜWx=W0x+BAx (3)'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 3}, page_content='We illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so‚àÜW=BAis zero at the beginning of training. We then scale ‚àÜWx byŒ±\\nr, whereŒ±\\nis a constant in r. When optimizing with Adam, tuning Œ±is roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set Œ±to the Ô¨Årstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of Ô¨Åne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full Ô¨Åne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 3}, page_content='other words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preÔ¨Åx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd√ók.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B‚Ä≤A‚Ä≤, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='guarantees that we do not introduce any additional latency during inference compared to a Ô¨Åne-tuned\\nmodel by construction.\\n4.2 A PPLYING LORA TOTRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module ( Wq,Wk,Wv,Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\nas a single matrix of dimension dmodel√ódmodel , even though the output dimension is usually sliced\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efÔ¨Åciency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='Practical BeneÔ¨Åts and Limitations. The most signiÔ¨Åcant beneÔ¨Åt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2/3ifr‚â™dmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 √ó(from 350GB to 35MB)4. This allows us to train with signiÔ¨Å-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneÔ¨Åt is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the Ô¨Çy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='during training on GPT-3 175B compared to full Ô¨Åne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). SpeciÔ¨Åcally, we evaluate on the GLUE (Wang et al., 2019) benchmark'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='for RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During Ô¨Åne-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 4}, page_content='4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 ‚âà354GB as opposed to 100 * 350GB ‚âà35TB.\\n5For GPT-3 175B, the training throughput for full Ô¨Åne-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n5'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='Model & Method # Trainable\\nParameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nRoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\\nRoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\\nRoB base(AdptD)* 0.3M 87.1¬±.094.2¬±.188.5¬±1.160.8¬±.493.1¬±.190.2¬±.071.5¬±2.789.7¬±.384.4\\nRoB base(AdptD)* 0.9M 87.3¬±.194.7¬±.388.4¬±.162.6¬±.993.0¬±.290.6¬±.075.9¬±2.290.3¬±.185.4\\nRoB base(LoRA) 0.3M 87.5¬±.395.1¬±.289.7¬±.763.4¬±1.293.3¬±.390.8¬±.186.6¬±.791.5¬±.287.2\\nRoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\\nRoB large(LoRA) 0.8M 90.6¬±.296.2¬±.590.9¬±1.268.2¬±1.994.9¬±.391.6¬±.187.4¬±2.592.6¬±.289.0\\nRoB large(AdptP)‚Ä† 3.0M 90.2¬±.396.1¬±.390.2¬±.768.3¬±1.094.8¬±.291.9¬±.183.8¬±2.992.1¬±.788.4\\nRoB large(AdptP)‚Ä† 0.8M 90.5¬±.396.6¬±.289.7¬±1.267.8¬±2.594.8¬±.391.7¬±.280.1¬±2.991.9¬±.487.9\\nRoB large(AdptH)‚Ä† 6.0M 89.9¬±.596.2¬±.388.7¬±2.966.5¬±4.494.7¬±.292.1¬±.183.4¬±1.191.0¬±1.787.8\\nRoB large(AdptH)‚Ä† 0.8M 90.3¬±.396.3¬±.587.7¬±1.766.3¬±2.094.7¬±.291.5¬±.172.9¬±2.991.5¬±.586.4'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='RoB large(LoRA)‚Ä† 0.8M 90.6¬±.296.2¬±.590.2¬±1.068.2¬±1.994.8¬±.391.6¬±.285.2¬±1.192.3¬±.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9¬±.296.9¬±.292.6¬±.672.4¬±1.196.0¬±.192.9¬±.194.9¬±.493.0¬±.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew‚Äôs\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. ‚Ä†indicates runs conÔ¨Ågured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreÔ¨Åx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='cial tokens have trainable word embeddings and are generally not in the model‚Äôs vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on ‚ÄúpreÔ¨Åxing‚Äù, which prepends\\nsuch tokens to the prompt, and ‚ÄúinÔ¨Åxing‚Äù, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preÔ¨Åx (resp. inÔ¨Åx) tokens. The number of\\ntrainable parameters is |Œò|=dmodel√ó(lp+li).\\nPreÔ¨Åx-layer tuning (PreLayer) is an extension to preÔ¨Åx-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\\n|Œò|=L√ódmodel√ó(lp+li), whereLis the number of Transformer layers.\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='attention module (and the MLP module) and the subsequent residual connection. There are two\\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efÔ¨Åcient design with the\\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\\ninclude another baseline call AdapterDrop (R ¬®uckl¬¥e et al., 2020) which drops some adapter layers for\\ngreater efÔ¨Åciency ( AdapterD). We cite numbers from prior works whenever possible to maximize\\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the Ô¨Årst column.\\nIn all cases, we have |Œò|=ÀÜLAdpt√ó(2√ódmodel√ór+r+dmodel )+2√óÀÜLLN√ódmodel where ÀÜLAdpt\\nis the number of adapter layers and ÀÜLLNthe number of trainable LayerNorms (e.g., in AdapterL).'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 5}, page_content='LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\\nAs mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\\nThe number of trainable parameters is determined by the rank rand the shape of the original weights:\\n|Œò|= 2√óÀÜLLoRA√ódmodel√ór, where ÀÜLLoRA is the number of weight matrices we apply LoRA to.\\n6'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 6}, page_content='Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3¬±.68.50¬±.07 46.0¬±.2 70.7¬±.2 2.44¬±.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4¬±.18.85¬±.02 46.8¬±.2 71.8¬±.1 2.53¬±.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1¬±.18.68¬±.03 46.3¬±.0 71.4¬±.2 2.49¬±.0\\nGPT-2 L (AdapterL) 23.00M 68.9¬±.38.70¬±.04 46.1¬±.1 71.3¬±.2 2.45¬±.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4¬±.18.89¬±.02 46.8¬±.2 72.0¬±.2 2.47¬±.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 6}, page_content='or fewer trainable parameters. ConÔ¨Ådence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latter‚Äôs task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efÔ¨Åcient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 6}, page_content='setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the Ô¨Åne-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with ‚Ä†. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n5.3 D EBERT AXXL\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nÔ¨Åne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 6}, page_content='See Section D.2 for details on the hyperparameters used.\\n5.4 GPT-2 MEDIUM /LARGE\\nHaving shown that LoRA can be a competitive alternative to full Ô¨Åne-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n7'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 7}, page_content='Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full Ô¨Åne-tuning. The results\\non WikiSQL have a Ô¨Çuctuation around ¬±0.5%, MNLI-m around ¬±0.1%, and SAMSum around\\n¬±0.2/¬±0.2/¬±0.1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a Ô¨Ånal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 7}, page_content='training cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the Ô¨Åne-tuning baseline on all three datasets. Note\\nthat not all methods beneÔ¨Åt monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiÔ¨Åcant performance drop when we use more than 256 special tokens for\\npreÔ¨Åx-embedding tuning or more than 32 special tokens for preÔ¨Åx-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 7}, page_content='6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) ‚Äì both are large Transformer lan-\\n8'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='guage models trained on a large amount of text ‚Äì where Ô¨Åne-tuning on task-speciÔ¨Åc data after pre-\\ntraining on general domain data provides a signiÔ¨Åcant performance gain compared to training on\\ntask-speciÔ¨Åc data directly. Training larger Transformers generally results in better performance and\\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\\nlanguage model trained to-date with 175B parameters.\\nPrompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a\\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\\nmodel‚Äôs performance on a desired task, which is known as prompt engineering or prompt hacking.\\nFine-tuning retrains a model pre-trained on general domains to a speciÔ¨Åc task Devlin et al. (2019b);'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='Radford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform Ô¨Åne-tuning in\\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\\nhas the same memory footprint as pre-training.\\nParameter-EfÔ¨Åcient Adaptation. Many have proposed inserting adapter layers between existing\\nlayers in a neural network (Houlsby et al., 2019; RebufÔ¨Å et al., 2017; Lin et al., 2020). Our method\\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\\nkey functional difference is that our learned weights can be merged with the main weights during\\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='A comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\\nimprove its parameter efÔ¨Åciency, which we leave to future work. More recently, many proposed\\noptimizing the input word embeddings in lieu of Ô¨Åne-tuning, akin to a continuous and differentiable\\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='ing. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (Ô¨Ånite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='the underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\\n(up to 10,000√ó) without adversely affecting task performances.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 8}, page_content='We perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n9'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 9}, page_content='to maximize downstream performance? 2) Is the ‚Äúoptimal‚Äù adaptation matrix ‚àÜWreally rank-\\ndeÔ¨Åcient ? If so, what is a good rank to use in practice? 3) What is the connection between ‚àÜWand\\nW? Does ‚àÜWhighly correlate with W? How large is ‚àÜWcomparing to W?\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using\\npre-trained language models for downstream tasks, which is a critical topic in NLP.\\n7.1 W HICH WEIGHT MATRICES IN TRANSFORMER SHOULD WEAPPLY LORA TO?\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain\\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\\nin FP16) on GPT-3 175B, which corresponds to r= 8if we adapt one type of attention weights or\\nr= 4if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n# of Trainable Parameters = 18M'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 9}, page_content='Weight Type WqWkWvWoWq,WkWq,WvWq,Wk,Wv,Wo\\nRankr 8 8 8 8 4 4 2\\nWikiSQL (¬±0.5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7\\nMultiNLI (¬±0.1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We Ô¨Ånd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the Ô¨Årst column.\\nNote that putting all the parameters in ‚àÜWqor‚àÜWkresults in signiÔ¨Åcantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in ‚àÜWsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt {Wq,Wv},'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 9}, page_content='{Wq,Wk,Wv,Wc}, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(¬±0.5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq,Wv 73.4 73.3 73.7 73.8 73.5\\nWq,Wk,Wv,Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (¬±0.1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq,Wv 91.3 91.4 91.3 91.6 91.4\\nWq,Wk,Wv,Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufÔ¨Åces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso for{Wq,Wv}than justWq). This suggests the update matrix ‚àÜWcould have a very small\\n‚Äúintrinsic rank‚Äù.6To further support this Ô¨Ånding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 9}, page_content='more meaningful subspace, which suggests that a low-rank adaptation matrix is sufÔ¨Åcient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought\\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\\nthe entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\\n10'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 10}, page_content='Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1‚â§i‚â§8) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1‚â§j‚â§64)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\nœÜ(Ar=8,Ar=64,i,j) =||Ui‚ä§\\nAr=8Uj\\nAr=64||2\\nF\\nmin(i,j)‚àà[0,1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\nœÜ(¬∑)has a range of [0,1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how œÜchanges as we vary iandj. We only look at the 48th layer'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 10}, page_content='(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both ‚àÜWqand‚àÜWv.\\nThe third and the fourth Ô¨Ågures zoom in on the lower-left triangle in the Ô¨Årst two Ô¨Ågures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiÔ¨Åcantly between\\nAr=8andAr=64, while others do not. SpeciÔ¨Åcally, ‚àÜWv(resp. ‚àÜWq) ofAr=8\\nand‚àÜWv(resp. ‚àÜWq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0.5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 10}, page_content='the top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conÔ¨Årm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n‚àÜWqappears to have a higher ‚Äúintrinsic rank‚Äù than ‚àÜWv, since more common singular value direc-\\ntions are learned by both runs for ‚àÜWq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX ‚àÜWCOMPARE TO W?\\nWe further investigate the relationship between ‚àÜWandW. In particular, does ‚àÜWhighly correlate\\nwithW? (Or mathematically, is ‚àÜWmostly contained in the top singular directions of W?) Also,'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 10}, page_content='7Note that a similar analysis can be carried out with Band the left-singular unitary matrices ‚Äì we stick with\\nAfor our experiments.\\n11'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 11}, page_content='0.00.10.20.30.40.5\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj1\\n8\\n16\\n24\\n32\\n40\\n48\\n56iWq\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nWv\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\njRandom GaussianFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\\nfrom two random seeds, for both ‚àÜWqand‚àÜWvin the 48-th layer. Right: the same heat-map\\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\nhow ‚Äúlarge‚Äù is ‚àÜWcomparing to its corresponding directions in W? This can shed light on the\\nunderlying mechanism for adapting pre-trained language models.\\nTo answer these questions, we project Wonto ther-dimensional subspace of ‚àÜWby comput-\\ningU‚ä§WV‚ä§, withU/Vbeing the left/right singular-vector matrix of ‚àÜW. Then, we com-\\npare the Frobenius norm between ‚à•U‚ä§WV‚ä§‚à•Fand‚à•W‚à•F. As a comparison, we also compute\\n‚à•U‚ä§WV‚ä§‚à•Fby replacing U,V with the top rsingular vectors of Wor a random matrix.\\nr= 4 r= 64\\n‚àÜWqWq Random ‚àÜWqWq Random'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 11}, page_content='||U‚ä§WqV‚ä§||F= 0.32 21.67 0.02 1.90 37.71 0.33\\n||Wq||F= 61.95||‚àÜWq||F= 6.91||‚àÜWq||F= 3.57\\nTable 7: The Frobenius norm of U‚ä§WqV‚ä§whereUandVare the left/right top rsingular vector\\ndirections of either (1) ‚àÜWq, (2)Wq, or (3) a random matrix. The weight matrices are taken from\\nthe 48th layer of GPT-3.\\nWe draw several conclusions from Table 7. First, ‚àÜWhas a stronger correlation with Wcompared\\nto a random matrix, indicating that ‚àÜWampliÔ¨Åes some features that are already in W. Second,\\ninstead of repeating the top singular directions of W,‚àÜWonly ampliÔ¨Åes directions that are not\\nemphasized in W. Third, the ampliÔ¨Åcation factor is rather huge: 21.5‚âà6.91/0.32forr= 4.\\nSee Section H.4 for why r= 64 has a smaller ampliÔ¨Åcation factor. We also provide a visualization\\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\\nThis suggests that the low-rank adaptation matrix potentially ampliÔ¨Åes the important features for'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 11}, page_content='speciÔ¨Åc downstream tasks that were learned but not emphasized in the general pre-training model .\\n8 C ONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances for different tasks. We propose\\nLoRA, an efÔ¨Åcient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.\\nThere are many directions for future works. 1) LoRA can be combined with other efÔ¨Åcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind Ô¨Åne-tuning'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 11}, page_content='or LoRA is far from clear ‚Äì how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full Ô¨Åne-\\n12'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 12}, page_content='tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deÔ¨Åciency of ‚àÜWsuggests that Wcould\\nbe rank-deÔ¨Åcient as well, which can also be a source of inspiration for future works.\\nREFERENCES\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\\nhttp://arxiv.org/abs/2012.13255 .\\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn EfÔ¨Åciently, Going Beyond Kernels? In\\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puriÔ¨Åcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 12}, page_content='Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\\n03962 .\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 12}, page_content='matrix completion. SIAM Journal on optimization , 20(4):1956‚Äì1982, 2010.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\\nRonan Collobert and Jason Weston. A uniÔ¨Åed architecture for natural language processing: deep\\nneural networks with multitask learning. In Proceedings of the 25th international conference\\non Machine learning , ICML ‚Äô08, pp. 160‚Äì167, New York, NY , USA, July 2008. Association\\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\\nhttps://doi.org/10.1145/1390156.1390177 .\\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc‚ÄôAurelio Ranzato, and Nando de Freitas. Predicting\\nparameters in deep learning, 2014.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 12}, page_content='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019a.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\\nhttps://aclanthology.org/I05-5002 .\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\\nNatural Language Generation , pp. 124‚Äì133, 2017.\\n13'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 13}, page_content='Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\\nhttp://arxiv.org/abs/1911.12237 .\\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\\napproximation techniques. GAMM-Mitteilungen , 36(1):53‚Äì78, 2013.\\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\\nlearning. In ICML , pp. 376‚Äì383, 2008. URL https://doi.org/10.1145/1390156.\\n1390204 .\\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\\n2101.00121 . arXiv: 2101.00121.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 13}, page_content='Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-EfÔ¨Åcient Transfer Learning\\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\\n00751 .\\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\\nof factorized neural layers, 2021.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 13}, page_content='Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-EfÔ¨Åcient Prompt\\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\\narXiv: 2104.08691.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\\nXiang Lisa Li and Percy Liang. PreÔ¨Åx-Tuning: Optimizing Continuous Prompts for Generation.\\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\\n2358‚Äì2367. PMLR, 2016.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 13}, page_content='Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\\nory, pp. 2‚Äì47. PMLR, 2018b.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efÔ¨Åcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020 , pp. 441‚Äì459, Online, November 2020. Association for Computational\\nLinguistics. doi: 10.18653/v1/2020.Ô¨Åndings-emnlp.41. URL https://aclanthology.\\norg/2020.findings-emnlp.41 .\\n14'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 14}, page_content='Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\\n2103.10385 . arXiv: 2103.10385.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: EfÔ¨Åcient low-rank\\nhypercomplex adapter layers, 2021.\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 14}, page_content='Jekaterina Novikova, Ond Àárej Du Àásek, and Verena Rieser. The e2e dataset: New challenges for end-\\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\\narXiv:1906.05392 , 2019.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¬®uckl¬¥e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\\nInterspeech , pp. 3743‚Äì3747, 2018.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\\nstanding by Generative Pre-Training. pp. 12, a.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 14}, page_content='Models are Unsupervised Multitask Learners. pp. 24, b.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable questions\\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\\nSylvestre-Alvise RebufÔ¨Å, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\\nabs/1705.08045 . arXiv: 1705.08045.\\nAndreas R ¬®uckl¬¥e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\\nIryna Gurevych. Adapterdrop: On the efÔ¨Åciency of adapters in transformers, 2020.\\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\\nrank matrix factorization for deep neural network training with high-dimensional output targets.\\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655‚Äì\\n6659. IEEE, 2013.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 14}, page_content='6659. IEEE, 2013.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\\nallelism, 2020.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\\nProcessing , pp. 1631‚Äì1642, Seattle, Washington, USA, October 2013. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\\n15'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 15}, page_content='Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\\nternational Conference on Neural Information Processing Systems , pp. 6000‚Äì6010, 2017.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems, 2020.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint arXiv:1805.12471 , 2018.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In Proceedings of the 2018 Conference of the North'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 15}, page_content='American Chapter of the Association for Computational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers) , pp. 1112‚Äì1122, New Orleans, Louisiana, June 2018. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\\norg/anthology/N18-1101 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demonstrations , pp. 38‚Äì45, Online, October 2020. As-\\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\\n2020.emnlp-demos.6 .'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 15}, page_content='Greg Yang and Edward J. Hu. Feature Learning in InÔ¨Ånite-Width Neural Networks.\\narXiv:2011.14522 [cond-mat] , May 2021. URL http://arxiv.org/abs/2011.14522 .\\narXiv: 2011.14522.\\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitÔ¨Åt: Simple parameter-efÔ¨Åcient Ô¨Åne-tuning\\nfor transformer-based masked language-models, 2021.\\nYu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck\\nfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,\\nspeech and signal processing (ICASSP) , pp. 185‚Äì189. IEEE, 2014.\\nYong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks.\\nIn2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,\\npp. 5005‚Äì5009. IEEE, 2016.\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\\nnatural language using reinforcement learning. CoRR , abs/1709.00103, 2017. URL http://'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 15}, page_content='arxiv.org/abs/1709.00103 .\\nA L ARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES\\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of\\ntraining samples. However, in practice, we can often afford to curate a few thousand or more training\\nexamples for performance-sensitive applications. As shown in Table 8, Ô¨Åne-tuning improves the\\nmodel performance drastically compared to few-shot learning on datasets large and small. We take\\nthe GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched,\\nwe use two demonstrations per class and six in-context examples in total.\\n16'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 16}, page_content='Method MNLI-m (Val. Acc./%) RTE (Val. Acc./%)\\nGPT-3 Few-Shot 40.6 69.0\\nGPT-3 Fine-Tuned 89.5 85.4\\nTable 8: Fine-tuning signiÔ¨Åcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\\nB I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas\\nour proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\\nadapter layers must be computed in addition to the base model, inevitably introducing additional\\nlatency. While as pointed out in R ¬®uckl¬¥e et al. (2020), the latency introduced by adapter layers can\\nbe mitigated when the model batch size and/or sequence length is large enough to full utilize the\\nhardware parallelism. We conÔ¨Årm their observation with a similar latency study on GPT-2 medium\\nand point out that there are scenarios, notably online inference where the batch size is small, where\\nthe added latency can be signiÔ¨Åcant.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 16}, page_content='We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\\nover 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\\nr. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\\nand a recent, more efÔ¨Åcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\\nfor more details on the designs. We plot the slow-down in percentage compared to the no-adapter\\nbaseline in Figure 5.\\n05101520253035\\n0 10 100 250AdapterH rSeq Len = 128 Seq Len = 256 Seq Len = 512\\n1 2 4 8 16 32\\nBatch Size0 10 100 250AdapterL r\\n1 2 4 8 16 32\\nBatch Size1 2 4 8 16 32\\nBatch Size\\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter ( r= 0) baseline.\\nThe top row shows the result for AdapterHand the bottom row AdapterL. Larger batch size and\\nsequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 16}, page_content='online, short-sequence-length scenario. We tweak the colormap for better visibility.\\nC D ATASET DETAILS\\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\\nMNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\\n(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\\n(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8(question-answering), RTE (inference),\\n8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\\n17'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\\nstandard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\\nare released under different permissive licenses.\\nWikiSQL is introduced in Zhong et al. (2017) and contains 56,355/8,421training/validation ex-\\namples. The task is to generate SQL queries from natural language questions and table schemata.\\nWe encode context as x={table schema ,query}and target as y={SQL}. The dataset is release\\nunder the BSD 3-Clause License.\\nSAMSum is introduced in Gliwa et al. (2019) and contains 14,732/819training/test examples. It\\nconsists of staged chat conversations between two people and corresponding abstractive summaries\\nwritten by linguists. We encode context as ‚Äù \\\\n‚Äù concatenated utterances followed by a ‚Äù \\\\n\\\\n‚Äù,\\nand target as y={summary}. The dataset is released under the non-commercial licence: Creative\\nCommons BY-NC-ND 4.0.'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='E2E NLG Challenge was Ô¨Årst introduced in Novikova et al. (2017) as a dataset for training end-to-\\nend, data-driven natural language generation systems and is commonly used for data-to-text evalua-\\ntion. The E2E dataset consists of roughly 42,000training, 4,600validation, and 4,600test exam-\\nples from the restaurant domain. Each source table used as input can have multiple references. Each\\nsample input (x,y)consists of a sequence of slot-value pairs, along with a corresponding natural\\nlanguage reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nDART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\\nstructured as sequences of ENTITY ‚Äî RELATION ‚Äî ENTITY triples. With 82Kexamples in\\ntotal, DART is a signiÔ¨Åcantly larger and more complex data-to-text task compared to E2E. The\\ndataset is released under the MIT license.\\nWebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='22Kexamples in total WebNLG comprises 14 distinct categories, nine of which are seen during\\ntraining. Since Ô¨Åve of the 14 total categories are not seen during training, but are represented in\\nthe test set, evaluation is typically broken out by ‚Äúseen‚Äù categories (S), ‚Äúunseen‚Äù categories (U)\\nand ‚Äúall‚Äù (A). Each input example is represented by a sequence of SUBJECT ‚Äî PROPERTY ‚Äî\\nOBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nD H YPERPARAMETERS USED IN EXPERIMENTS\\nD.1 R OBERT A\\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\\nof training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\\nmodules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\\nusual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\\nrandom seeds; the result for each run is taken from the best epoch. For a fair comparison with the'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='setup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\\nand used a Ô¨Åxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\\nmodel when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\\nThe runs with this restricted setup are marked with ‚Ä†. See the hyperparameters used in our runs\\nin Table 9.\\nD.2 D EBERT A\\nWe again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\\nwe tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\\nsequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\\nwe initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\\nSTS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\\nthe median over 5 random seeds; the result for each run is taken from the best epoch. See the'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 17}, page_content='hyperparameters used in our runs in Table 10.\\n18'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 18}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.06\\nLR Schedule Linear\\nRoBERTa base\\nLoRABatch Size 16 16 16 32 32 16 32 16\\n# Epochs 30 60 30 80 25 25 80 40\\nLearning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 512\\nRoBERTa large\\nLoRABatch Size 4 4 4 4 4 4 8 8\\n# Epochs 10 10 20 20 10 20 20 30\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128 128 512 128 512 512 512 512\\nRoBERTa large\\nLoRA‚Ä†Batch Size 4\\n# Epochs 10 10 20 20 10 20 20 10\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(3M)‚Ä†Batch Size 32\\n# Epochs 10 20 20 20 10 20 20 20\\nLearning Rate 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(0.8M)‚Ä†Batch Size 32\\n# Epochs 5 20 20 20 10 20 20 20\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 18}, page_content='Bottleneckr 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(6M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(0.8M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 8\\nMax Seq. Len. 128\\nTable 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\\nD.3 GPT-2\\nWe train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\\nrate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\\nin Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\\nmean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\\nused for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\\n(2021).\\nD.4 GPT-3'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 18}, page_content='(2021).\\nD.4 GPT-3\\nFor all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\\na batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\\n19'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 19}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.1\\nLR Schedule Linear\\nDeBERTa XXL\\nLoRABatch Size 8 8 32 4 6 8 4 4\\n# Epochs 5 16 30 10 8 11 11 10\\nLearning Rate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04\\nWeight Decay 0 0.01 0.01 0 0.01 0.01 0.01 0.1\\nCLS Dropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 256 128 128 64 512 320 320 128\\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\\nDataset E2E WebNLG DART\\nTraining\\nOptimizer AdamW\\nWeight Decay 0.01 0.01 0.0\\nDropout Prob 0.1 0.1 0.0\\nBatch Size 8\\n# Epoch 5\\nWarmup Steps 500\\nLearning Rate Schedule Linear\\nLabel Smooth 0.1 0.1 0.0\\nLearning Rate 0.0002\\nAdaptation rq=rv= 4\\nLoRAŒ± 32\\nInference\\nBeam Size 10\\nLength Penalty 0.9 0.8 0.8\\nno repeat ngram size 4\\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 19}, page_content='et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\\ndetails on the hyperparameters used. For preÔ¨Åx-embedding tuning, we Ô¨Ånd the optimal lpandli\\nto be 256 and 8, respectively, totalling 3.2Mtrainable parameters. We use lp= 8 andli= 8 for\\npreÔ¨Åx-layer tuning with 20.2Mtrainable parameters to obtain the overall best performance. We\\npresent two parameter budgets for LoRA: 4.7M ( rq=rv= 1orrv= 2) and 37.7M ( rq=rv= 8\\norrq=rk=rv=ro= 2). We report the best validation performance from each run. The training\\nhyperparameters used in our GPT-3 experiments are listed in Table 12.\\nE C OMBINING LORA WITH PREFIX TUNING\\nLoRA can be naturally combined with existing preÔ¨Åx-based approaches. In this section, we evaluate\\ntwo combinations of LoRA and variants of preÔ¨Åx-tuning on WikiSQL and MNLI.\\nLoRA+PreÔ¨ÅxEmbed (LoRA+PE) combines LoRA with preÔ¨Åx-embedding tuning, where we insert'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 19}, page_content='lp+lispecial tokens whose embeddings are treated as trainable parameters. For more on preÔ¨Åx-\\nembedding tuning, see Section 5.1.\\nLoRA+PreÔ¨ÅxLayer (LoRA+PL) combines LoRA with preÔ¨Åx-layer tuning. We also insert lp+li\\nspecial tokens; however, instead of letting the hidden representations of these tokens evolve natu-\\n20'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 20}, page_content='Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterHLoRA\\nOptimizer AdamW\\nBatch Size 128\\n# Epoch 2\\nWarmup Tokens 250,000\\nLR Schedule Linear\\nLearning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\\nsame hyperparameters for all datasets after tuning learning rate.\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\\nmore on preÔ¨Åx-layer tuning, see Section 5.1.\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\\nFirst of all, LoRA+PE signiÔ¨Åcantly outperforms both LoRA and preÔ¨Åx-embedding tuning on\\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preÔ¨Åx-embedding tuning. On\\nMultiNLI, the combination of LoRA+PE doesn‚Äôt perform better than LoRA, possibly because LoRA'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 20}, page_content='on its own already achieves performance comparable to the human baseline. Secondly, we notice\\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\\ntribute this to the fact that preÔ¨Åx-layer tuning is very sensitive to the choice of learning rate and thus\\nmakes the optimization of LoRA weights more difÔ¨Åcult in LoRA+PL.\\nF A DDITIONAL EMPIRICAL EXPERIMENTS\\nF.1 A DDITIONAL EXPERIMENTS ON GPT-2\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\\npreÔ¨Åx-based approaches given the same number of trainable parameters.\\nMethod # Trainable DART\\nParameters BLEU‚ÜëMET‚ÜëTER‚Üì\\nGPT-2 Medium\\nFine-Tune 354M 46.2 0.39 0.46\\nAdapterL0.37M 42.4 0.36 0.48\\nAdapterL11M 45.2 0.38 0.46\\nFTTop224M 41.0 0.34 0.56\\nPrefLayer 0.35M 46.4 0.38 0.46'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 20}, page_content='LoRA 0.35M 47.1¬±.2 0.39 0.46\\nGPT-2 Large\\nFine-Tune 774M 47.0 0.39 0.46\\nAdapterL0.88M 45.7¬±.1 0.38 0.46\\nAdapterL23M 47.1¬±.1 0.39 0.45\\nPrefLayer 0.77M 46.7 0.38 0.45\\nLoRA 0.77M 47.5¬±.1 0.39 0.45\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\\nless than 0.01for all adaption approaches.\\n21'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 21}, page_content='Method WebNLG\\nBLEU‚Üë MET‚Üë TER‚Üì\\nU S A U S A U S A\\nGPT-2 Medium\\nFine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53\\nAdapterL(0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43\\nAdapterL(11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39\\nFTTop2(24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72\\nPreÔ¨Åx (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40\\nLoRA (0.35M) 46.7¬±.462.1¬±.255.3¬±.2.38 .44 .41 .46 .33 .39\\nGPT-2 Large\\nFine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42\\nAdapterL(0.88M) 49.8¬±.061.1¬±.056.0¬±.0.38 .43 .41 .44 .35 .39\\nAdapterL(23M) 49.2¬±.164.7¬±.257.7¬±.1.39 .46 .43 .46 .33 .39\\nPreÔ¨Åx (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40\\nLoRA (0.77M) 48.4¬±.364.0¬±.357.0¬±.1.39 .45 .42 .45 .32 .38\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\\nare less than 0.01for all the experiments we ran. ‚ÄúU‚Äù indicates unseen categories, ‚ÄúS‚Äù indicates seen\\ncategories, and ‚ÄúA‚Äù indicates all categories in the test set of WebNLG.\\nF.2 A DDITIONAL EXPERIMENTS ON GPT-3'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 21}, page_content='We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\\nidentifying the trade-off between performance and the number of trainable parameters.\\nF.3 L OW-DATA REGIME\\nTo evaluate the performance of different adaptation approaches in the low-data regime. we randomly\\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\\nMNLI-ntasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\\nn. To our surprise, PreÔ¨ÅxEmbed and PreÔ¨ÅxLayer performs very poorly on MNLI-100 dataset, with\\nPreÔ¨ÅxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreÔ¨ÅxLayer\\nperforms better than PreÔ¨ÅxEmbed but is still signiÔ¨Åcantly worse than Fine-Tune or LoRA on MNLI-\\n100. The gap between preÔ¨Åx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\\ncrease the number of training examples, which might suggest that preÔ¨Åx-based approaches are not'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 21}, page_content='suitable for low-data tasks in GPT-3. LoRA achieves better performance than Ô¨Åne-tuning on both\\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\\n(¬±0.3) variance due to random seeds.\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\\nble 17. We use a smaller learning rate for PreÔ¨ÅxLayer on the MNLI-100 set, as the training loss does\\nnot decrease with a larger learning rate.\\nG M EASURING SIMILARITY BETWEEN SUBSPACES\\nIn this paper we use the measure œÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚à•Ui‚ä§\\nAUB‚à•2\\nF\\nmin{i,j}to measure the subspace\\nsimilarity between two column orthonormal matrices Ui\\nA‚ààRd√óiandUj\\nB‚ààRd√ój, obtained by\\ntaking columns of the left singular matrices of AandB. We point out that this similarity is simply\\na reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\\n(2008).\\n22'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 22}, page_content='Method Hyperparameters # Trainable Parameters WikiSQL MNLI-m\\nFine-Tune - 175B 73.8 89.5\\nPreÔ¨ÅxEmbedlp= 32,li= 8 0.4 M 55.9 84.9\\nlp= 64,li= 8 0.9 M 58.7 88.1\\nlp= 128,li= 8 1.7 M 60.6 88.0\\nlp= 256,li= 8 3.2 M 63.1 88.6\\nlp= 512,li= 8 6.4 M 55.9 85.8\\nPreÔ¨ÅxLayerlp= 2,li= 2 5.1 M 68.5 89.2\\nlp= 8,li= 0 10.1 M 69.8 88.2\\nlp= 8,li= 8 20.2 M 70.1 89.5\\nlp= 32,li= 4 44.1 M 66.4 89.6\\nlp= 64,li= 0 76.1 M 64.9 87.9\\nAdapterHr= 1 7.1 M 71.9 89.8\\nr= 4 21.2 M 73.2 91.0\\nr= 8 40.1 M 73.2 91.5\\nr= 16 77.9 M 73.2 91.5\\nr= 64 304.4 M 72.6 91.5\\nLoRArv= 2 4.7 M 73.4 91.7\\nrq=rv= 1 4.7 M 73.4 91.3\\nrq=rv= 2 9.4 M 73.3 91.4\\nrq=rk=rv=ro= 1 9.4 M 74.1 91.2\\nrq=rv= 4 18.8 M 73.7 91.3\\nrq=rk=rv=ro= 2 18.8 M 73.7 91.7\\nrq=rv= 8 37.7 M 73.8 91.6\\nrq=rk=rv=ro= 4 37.7 M 74.0 91.7\\nrq=rv= 64 301.9 M 73.6 91.4\\nrq=rk=rv=ro= 64 603.8 M 73.9 91.4\\nLoRA+PErq=rv= 8,lp= 8,li= 4 37.8 M 75.0 91.4\\nrq=rv= 32,lp= 8,li= 4 151.1 M 75.9 91.1\\nrq=rv= 64,lp= 8,li= 4 302.1 M 76.2 91.3\\nLoRA+PL rq=rv= 8,lp= 8,li= 4 52.8 M 72.9 90.2'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 22}, page_content='Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\\npreÔ¨Åx-embedding tuning (PreÔ¨ÅxEmbed) and preÔ¨Åx-layer tuning (PreÔ¨ÅxLayer) perform worse as we\\nincrease the number of trainable parameters, while LoRA‚Äôs performance stabilizes. Performance is\\nmeasured in validation accuracy.\\nMethod MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k MNLI(m)-392K\\nGPT-3 (Fine-Tune) 60.2 85.8 88.9 89.5\\nGPT-3 (PreÔ¨ÅxEmbed) 37.6 75.2 79.5 88.6\\nGPT-3 (PreÔ¨ÅxLayer) 48.3 82.5 85.9 89.6\\nGPT-3 (LoRA) 63.8 85.6 89.2 91.7\\nTable 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\\nndescribes a subset with ntraining examples. We evaluate with the full validation set. LoRA\\nperforms exhibits favorable sample-efÔ¨Åciency compared to other methods, including Ô¨Åne-tuning.\\nTo be concrete, let the singular values of Ui‚ä§\\nAUj\\nBto beœÉ1,œÉ2,¬∑¬∑¬∑,œÉpwherep= min{i,j}. We\\nknow that the Projection Metric Ham & Lee (2008) is deÔ¨Åned as:\\nd(Ui\\nA,Uj\\nB) =\\ued6a\\ued6b\\ued6b‚àöp‚àíp‚àë\\ni=1œÉ2\\ni‚àà[0,‚àöp]\\n23'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 23}, page_content='Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\\nOptimizer - AdamW\\nWarmup Tokens - 250,000\\nLR Schedule - Linear\\nBatch Size - 20 20 100 128\\n# Epoch - 40 40 4 2\\nLearning RateFineTune 5.00E-6\\nPreÔ¨ÅxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\\nPreÔ¨ÅxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\\nLoRA 2.00E-4\\nPreÔ¨ÅxEmbed lp 16 32 64 256\\nAdaptation- PreÔ¨ÅxEmbed li 8\\nSpeciÔ¨Åc PreÔ¨ÅxTune lp=li= 8\\nLoRA rq=rv= 8\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\\nwhere our similarity is deÔ¨Åned as:\\nœÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚àëp\\ni=1œÉ2\\ni\\np=1\\np(\\n1‚àíd(Ui\\nA,Uj\\nB)2)\\nThis similarity satisÔ¨Åes that if Ui\\nAandUj\\nBshare the same column span, then œÜ(A,B,i,j ) = 1 . If\\nthey are completely orthogonal, then œÜ(A,B,i,j ) = 0 . Otherwise, œÜ(A,B,i,j )‚àà(0,1).\\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\nWe present additional results from our investigation into the low-rank update matrices.\\nH.1 C ORRELATION BETWEEN LORA M ODULES'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 23}, page_content='See Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\\nlayers.\\nH.2 E FFECT OFrONGPT-2\\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\\ndataset as an example, we report the validation loss and test metrics achieved by different choices\\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\\nNote that the relationship between model size and the optimal rank for adaptation is still an open\\nquestion.\\nH.3 C ORRELATION BETWEEN WAND ‚àÜW\\nSee Figure 8 for the normalized subspace similarity between Wand‚àÜWwith varying r.\\nNote again that ‚àÜWdoes not contain the top singular directions of W, since the similarity between\\nthe top 4 directions in ‚àÜWand the top-10% of those in Wbarely exceeds 0.2. This gives evidence'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 23}, page_content='that‚àÜWcontains those ‚Äútask-speciÔ¨Åc‚Äù directions that are otherwise notemphasized in W.\\nAn interesting next question to answer, is how ‚Äústrong‚Äù do we need to amplify those task-speciÔ¨Åc\\ndirections, in order for the model adaptation to work well?\\n24'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 24}, page_content='0.00.20.40.60.81.0\\n12345678Layer 1\\niWq\\n Wv\\n Wq\\n Wv\\n12345678Layer 32\\ni\\n12345678Layer 64\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678Layer 96\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678\\nj12345678\\nj(Ar=8,Ar=64,i,j)\\nFigure 6: Normalized subspace similarity between the column vectors of Ar=8andAr=64for both\\n‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\nH.4 A MPLIFICATION FACTOR\\nOne can naturally consider a feature ampliÔ¨Åcation factor as the ratio‚à•‚àÜW‚à•F\\n‚à•U‚ä§WV‚ä§‚à•F, whereUandV\\nare the left- and right-singular matrices of the SVD decomposition of ‚àÜW. (RecallUU‚ä§WV‚ä§V\\ngives the ‚Äúprojection‚Äù of Wonto the subspace spanned by ‚àÜW.)\\nIntuitively, when ‚àÜWmostly contains task-speciÔ¨Åc directions, this quantity measures how much of\\nthem are ampliÔ¨Åed by ‚àÜW. As shown in Section 7.3, for r= 4, this ampliÔ¨Åcation factor is as large\\nas 20. In other words, there are (generally speaking) four feature directions in each layer (out of the'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 24}, page_content='entire feature space from the pre-trained model W), that need to be ampliÔ¨Åed by a very large factor\\n20, in order to achieve our reported accuracy for the downstream speciÔ¨Åc task. And, one should\\nexpect a very different set of feature directions to be ampliÔ¨Åed for each different downstream task.\\nOne may notice, however, for r= 64 , this ampliÔ¨Åcation factor is only around 2, meaning that\\nmost directions learned in ‚àÜWwithr= 64 arenotbeing ampliÔ¨Åed by much. This should not\\nbe surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\\nthe ‚Äútask-speciÔ¨Åc directions‚Äù (thus for model adaptation) is low. In contrast, those directions in the\\nrank-4 version of ‚àÜW(corresponding to r= 4) are ampliÔ¨Åed by a much larger factor 20.\\n25'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 25}, page_content='0.00.10.20.30.40.50.60.70.8\\n1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 1\\niWq\\n Wv\\nLayer 32Wq\\n Wv\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 64\\ni\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\njLayer 96\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nFigure 7: Normalized subspace similarity between the column vectors of Ar=64from two randomly\\nseeded runs, for both ‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\\nformer.\\nRankrvalloss BLEU NIST METEOR ROUGE L CIDEr\\n1 1.23 68.72 8.7215 0.4565 0.7052 2.4329\\n2 1.21 69.17 8.7413 0.4590 0.7052 2.4639\\n4 1.18 70.38 8.8439 0.4689 0.7186 2.5349\\n8 1.17 69.57 8.7457 0.4636 0.7196 2.5196\\n16 1.16 69.61 8.7483 0.4629 0.7177 2.4985\\n32 1.16 69.33 8.7736 0.4642 0.7105 2.5255\\n64 1.16 69.24 8.7174 0.4651 0.7180 2.5070\\n128 1.16 68.73 8.6718 0.4628 0.7127 2.5030\\n256 1.16 68.92 8.6982 0.4629 0.7128 2.5012\\n512 1.16 68.78 8.6857 0.4637 0.7128 2.5025'),\n",
              " Document(metadata={'source': '/content/lora.pdf', 'page': 25}, page_content='1024 1.17 69.37 8.7495 0.4659 0.7149 2.5090\\nTable 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\\ndifferent rank rusing GPT-2 Medium. Unlike on GPT-3 where r= 1sufÔ¨Åces for many tasks, here\\nthe performance peaks at r= 16 for validation loss and r= 4 for BLEU, suggesting the GPT-2\\nMedium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\\nhyperparameters are tuned on r= 4, which matches the parameter count of another baseline, and\\nthus might not be optimal for other choices of r.\\n0.1000.1250.1500.1750.200\\nj451\\n555\\n658\\n762\\n865\\n969\\n1072\\n1176i(Wq,Ar=4,i,j)\\njWq\\n(Wq,Ar=8,i,j)\\nj(Wq,Ar=64,i,j)\\njRandom\\n(Wq,Arand,i,j)\\nFigure 8: Normalized subspace similarity between the singular directions of Wqand those of ‚àÜWq\\nwith varying rand a random baseline. ‚àÜWqampliÔ¨Åes directions that are important but not empha-\\nsized inW.‚àÜWwith a larger rtends to pick up more directions that are already emphasized in\\nW.\\n26'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 0}, page_content='QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers‚àóArtidoro Pagnoni‚àóAri Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 0}, page_content='Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 0}, page_content='where Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n‚àóEqual contribution.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 0}, page_content='2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( ¬±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 ¬±1\\nGuanaco 65B 41 GB 1022 ¬±1\\nGuanaco 33B 21 GB 992 ¬±1\\nVicuna 13B 26 GB 974 ¬±1\\nChatGPT - 966 ¬±1\\nGuanaco 13B 10 GB 916 ¬±1\\nBard - 902 ¬±1\\nGuanaco 7B 6 GB 879 ¬±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='on a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [ 10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 6).\\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA‚Äôs efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (¬ß4) and training a state-of-the-art chatbot, Guanaco , (¬ß5), we also'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='analyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa‚Äîin other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='Elo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 1}, page_content='different open sourced, finetuned models.\\n2'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 2}, page_content='Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2 Background\\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [‚àí127,127]:\\nXInt8=round\\x12127\\nabsmax (XFP32)XFP32\\x13\\n=round (cFP32¬∑XFP32), (1)'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 2}, page_content='where cis the quantization constant orquantization scale . Dequantization is the inverse:\\ndequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization bins‚Äîcertain bit combinations‚Äîare not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor X‚ààRb√óhintoncontiguous blocks of\\nsizeBby flattening the input tensor and slicing the linear segment into n= (b√óh)/Bblocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\\nconstants ci.\\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 2}, page_content='requirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW =YwithX‚ààRb√óh,W‚ààRh√óoLoRA computes:\\nY=XW +sXL 1L2, (3)\\nwhereL1‚ààRh√órandL2‚ààRr√óo, and sis a scalar.\\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\n3'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose‚Äî4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='Since pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation œÉ(see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling œÉsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [‚àí1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations œÉin the range [‚àí1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [‚àí1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [‚àí1,1]range through absolute maximum rescaling.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 3}, page_content='Once the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(¬∑)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='ensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\\ndata type by estimating the quantiles qiof two ranges qi:2k‚àí1for the negative part and 2k‚àí1+ 1for\\nthe positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both\\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\\nnormally distributed data. The exact values of this data type can be found in Appendix E.\\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\\nquantization constants for additional memory savings. While a small blocksize is required for precise\\n4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit\\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='average. Double Quantization helps reduce the memory footprint of quantization constants.\\nMore specifically, Double Quantization treats quantization constants cFP32\\n2of the first quantization\\nas inputs to a second quantization. This second step yields the quantized quantization constants\\ncFP8\\n2and the second level of quantization constants cFP32\\n1. We use 8-bit Floats with a blocksize of\\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\\n2are positive, we subtract\\nthe mean from c2before quantization to center the values around zero and make use of symmetric\\nquantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64¬∑256) = 0 .127bits, a reduction of 0.373 bits\\nper parameter.\\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\nwhere doubleDequant (¬∑)is defined as:\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='For parameter updates only the gradient with respect to the error for the adapters weights‚àÇE\\n‚àÇLiare\\nneeded, and not for 4-bit weights‚àÇE\\n‚àÇW. However, the calculation of‚àÇE\\n‚àÇLientails the calculation of‚àÇX\\n‚àÇW\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivative‚àÇX\\n‚àÇWin BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 4}, page_content='NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16\\nFigure 2: RougeL for LLaMA 7B models on the'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='Alpaca dataset. Each point represents a run with a\\ndifferent random seed. We improve on the Stanford\\nAlpaca fully finetuned default hyperparameters to\\nconstruct a strong 16-bit baseline for comparisons.\\nUsing LoRA on all transformer layers is critical to\\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\\nQLORAtuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimiz-\\ners since the paging only occurs when processing\\nmini-batches with long sequence lengths, which is\\nrare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on\\n48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed\\nas regular optimizers. Future work should measure\\nand characterize under what circumstances slow-\\ndowns occur from the paging process.\\nDefault LoRA hyperparameters do not match 16-\\nbit performance When using the standard prac-\\ntice of applying LoRA to query and value attention'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='projection matrices [ 28], we are not able to replicate\\nfull finetuning performance for large base models.\\nAs shown in Figure 2 for LLaMA 7B finetuning on\\nAlpaca, we find that the most critical LoRA hyper-\\nparameter is how many LoRA adapters are used in\\ntotal and that LoRA on all linear transformer block\\nlayers are required to match full finetuning perfor-\\nmance. Other LoRA hyperparameters, such as the\\nprojection dimension r, do not affect performance (see Appendix A).\\n1010\\n1011\\nT otal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67Mean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQData type\\nFigure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='fine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).Similarly, we find that default hyperparameters for\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 5}, page_content='double quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\nestablished that 4-bit quantization for inference is\\n6'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 6}, page_content='Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORAreplicates 16-bit LoRA and full-\\nfinetuning.\\nDataset GLUE (Acc.) Super-NaturalInstructions (RougeL)\\nModel RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B\\nBF16 88.6 40.1 42.1 48.0 54.3 62.0\\nBF16 replication 88.6 40.0 42.2 47.3 54.9 -\\nLoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7\\nQLORA Int8 88.8 40.4 42.9 45.4 56.5 60.7\\nQLORA FP4 88.6 40.3 42.4 47.5 55.6 60.9\\nQLORA NF4 + DQ - 40.4 42.7 47.7 55.3 60.9\\npossible, but leads to performance degradation rel-\\native to 16-bit [ 13,18]. This raises the crucial question of whether the lost performance can be\\nrecovered by conducting 4-bit adapter finetuning. We test this for two setups.\\nTable 2: Pile Common Crawl mean\\nperplexity for different data types\\nfor 125M to 13B OPT, BLOOM,\\nLLaMA, and Pythia models.\\nData type Mean PPL\\nInt4 34.34\\nFloat4 (E2M1) 31.07\\nFloat4 (E3M0) 29.48'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 6}, page_content='Float4 (E3M0) 29.48\\nNFloat4 + DQ 27.41The first focuses on a comparison with full 16-bit finetuning\\nof RoBERTA and T5 models sized 125M to 3B parameters on\\nGLUE and the Super-NaturalInstructions dataset. Results are\\nshown in Table 3. In both datasets, we observe that 16-bit, 8-bit,\\nand 4-bit adapter methods replicate the performance of the fully\\nfinetuned 16-bit baseline. This suggests that the performance lost\\ndue to the imprecise quantization can be fully recovered through\\nadapter finetuning after quantization.\\nFor our second setup, since full finetuning models at and beyond\\n11B parameters requires more than one server of high memory\\nGPUs, we continue to test whether 4-bit QLORAcan match\\n16-bit LoRA at the 7B to 65B parameter scales. To this end, we\\nfinetune LLaMA 7B through 65B on two instruction following\\ndatasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 6}, page_content='are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit\\nLoRA MMLU performance. In addition, we also note that QLORAwith FP4 lags behind the 16-bit\\nbrain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1)\\nQLORAwith NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,\\nand (2) NF4 is superior to FP4 in terms of quantization precision.\\nSummary Our results consistently show that 4-bit QLORAwith NF4 data type matches 16-\\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 6}, page_content='finetuning and inference resource budget it is beneficial to increase the number of parameters in the\\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\\n7'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='Table 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\\nwhile FP4 is consistently one percentage point behind both.\\nMean 5-shot MMLU Accuracy\\nLLaMA Size 7B 13B 33B 65B Mean\\nDataset Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2\\nBFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0\\nFloat4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2\\nNFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1\\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods\\nfor real-world chatbot performance evaluation.\\n5.1 Experimental setup\\nWe now describe an overview of the experimental setup with full details in Appendix B.\\nData As, to our knowledge, there is no comprehensive study of recent instruction-following datasets,'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='we select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [ 31],\\nHH-RLHF [ 4]), distillation from instruction-tuned models (Alpaca [ 55], self-instruct [ 59], unnatural-\\ninstructions [ 26]), corpora aggregations (FLAN v2 [ 12]), as well as hybrids (Chip2 [ 32], Long-\\nform [30]). These datasets cover different languages, data sizes, and licenses.\\nTraining Setup To avoid confounding effects from different training objectives, we perform QLoRA\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='instructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.\\nBaselines We compare our models to both research (Vicuna [ 10] and Open Assistant [ 31]) and\\ncommercial (GPT-4 [ 42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is\\na LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on\\nthe same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B\\non proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\\nOpenAI GPT models.\\n5.2 Evaluation\\nTable 5: MMLU 5-shot test results for different'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='sizes of LLaMA finetuned on the corresponding\\ndatasets using QLoRA.\\nDataset 7B 13B 33B 65B\\nLLaMA no tuning 35.1 46.9 57.8 63.4\\nSelf-Instruct 36.4 33.3 53.0 56.7\\nLongform 32.1 43.2 56.6 59.7\\nChip2 34.5 41.6 53.6 59.8\\nHH-RLHF 34.9 44.6 55.8 60.1\\nUnnatural Instruct 41.9 48.1 57.3 61.3\\nGuanaco (OASST1) 36.6 46.4 57.0 62.2\\nAlpaca 38.8 47.8 57.3 62.5\\nFLAN v2 44.5 51.4 59.2 63.9Following common practice, we use the MMLU (Mas-\\nsively Multitask Language Understanding) benchmark\\n[24] to measure performance on a range of language un-\\nderstanding tasks. This is a multiple-choice benchmark\\ncovering 57 tasks including elementary mathematics,\\nUS history, computer science, law, and more. We report\\n5-shot test accuracy.\\nWe also test generative language capabilities through\\nboth automated and human evaluations. This second\\nset of evaluations relies on queries curated by humans\\nand aims at measuring the quality of model responses.\\nWhile this is a more realistic testbed for chatbot model'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 7}, page_content='performance and is growing in popularity, there is no\\ncommonly accepted protocol in the literature. We de-\\nscribe below our proposed setup, using nucleus sampling with p= 0.9and temperature 0.7in all\\ncases.\\n8'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts\\n[10] and the OASST1 validation dataset [ 31]. We use the Vicuna prompts, a set of 80 prompts from a\\ndiverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of\\ncrowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the\\nvalidation dataset as queries and include previous turns in the prompt. This procedure leads to 953\\nunique user queries. We term these two datasets the Vicuna and OA benchmarks.\\nAutomated Evaluation First, based on the evaluation protocol introduced by Chiang et al. [10],\\nwe use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the\\nVicuna benchmark. Given a query along with ChatGPT‚Äôs and a model‚Äôs responses, GPT-4 is prompted\\nto assign a score out of ten to both responses and provide an explanation. The overall performance of'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='a model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score\\ncan be higher than 100% if the model achieves a higher absolute score than ChatGPT. We find a\\nsignificant ordering effect with GPT-4 increasing the score of the response occurring earlier in the\\nprompt. To control for such effects, we recommend reporting the mean score over both orders.\\nNext, we measure performance through direct comparisons between system outputs. We simplify\\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evaluation While recent work indicates generative models can be effectively employed\\nfor system evaluations [ 19], the reliability GPT-4 ratings to assess chatbot performance is, to our'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\\nChatGPT and three annotators for pairwise comparisons.\\nElo Rating With both human and automated pairwise comparisons, we create a tournament-style\\ncompetition where models compete against each other. The tournament is made up of matches where\\npairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='relative to an opponent‚Äôs win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .\\nSimilar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to\\ncontrol for ordering effects, e.g., the effect of which model pairs compete with each other first.\\n5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\\npairwise comparisons - the highest reported to date.\\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a\\n5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\\nHowever, Table 6 also has very wide confidence intervals, with many models overlapping in per-'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 8}, page_content='formance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,\\ne.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\\nrecommend using the Elo ranking method [ 16], based on pairwise judgments from human annotators\\nand GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive\\n9'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='Table 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\\nand having a fraction of the memory requirement of baseline models.\\nModel / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI\\nGPT-4 - - - 119.4% 110.1% 114.5 % 2.6%\\nBard - - - 93.2% 96.4% 94.8% 4.1%\\nGuanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%\\nAlpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%\\nFLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%\\nGuanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%\\nOpen Assistant 33B 16-bit 66 GB 91.2% 98.7% 94.9% 4.5%\\nAlpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%\\nFLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%\\nVicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%\\nGuanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%\\nAlpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%\\nHH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%\\nChip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%\\nLongform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%\\nSelf-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%\\nFLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%\\nGuanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%\\nAlpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%\\nFLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%\\nmodels can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna\\nbenchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with\\na Kendall Tau of œÑ= 0.43and Spearman rank correlation of r= 0.55at the system level. At the\\nexample level, the agreement between GPT-4 and human annotators‚Äô majority vote is weaker with\\nFleiss Œ∫= 0.25. Overall, this shows a moderate agreement between system-level judgments by\\nGPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='alternative to human evaluation. We discuss further considerations in Section 6.2.\\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides\\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\\nwith Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA\\nbenchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of\\na finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN\\nv2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends\\nare observed with other models). This also points to partial orthogonality in current evaluation\\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\\nby Vicuna or OA benchmarks) and vice versa.\\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='dataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\\npotential for future work via QLORAtuning on specialized open-source data, which produces models\\nthat can compete with the very best commercial models that exist today.\\n6 Qualitative Analysis\\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [ 36]‚Äîwhether\\na benchmark truly tests what its name or description suggests is always at question, especially as we'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 9}, page_content='discover ‚Äúshortcuts‚Äù to solve benchmarks that machine learning models sometimes exploit [ 22,46].\\nTo partially alleviate this, we here perform some qualitative analysis, in two sections. First, in ¬ß6.1\\n10'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 10}, page_content='Table 7: Elo rating for a tournament between models where models compete to generate the best response\\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\\nis approximately a difference of 1.5% in win-rate.\\nBenchmark Vicuna Vicuna Open Assistant\\n# Prompts 80 80 953\\nJudge Human raters GPT-4 GPT-4 Median Rank\\nModel Elo Rank Elo Rank Elo Rank\\nGPT-4 1176 1 1348 1 1294 1 1\\nGuanaco-65B 1023 2 1022 2 1008 3 2\\nGuanaco-33B 1009 4 992 3 1002 4 4\\nChatGPT-3.5 Turbo 916 7 966 5 1015 2 5\\nVicuna-13B 984 5 974 4 936 5 5\\nGuanaco-13B 975 6 913 6 885 6 6\\nGuanaco-7B 1010 3 879 8 860 7 7\\nBard 909 8 902 7 - - 8\\nwe show some examples that we believe are representative of some observed patterns in the text\\ngenerated by our 65b Guanaco model. Second, ¬ß6.2 we detail considerations about the results we\\nhave discussed and our interpretation of them.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 10}, page_content='6.1 Qualitative Analysis of Example Generations\\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant\\nbenchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we\\nattempt to setup a question or prompt that will induce the pattern even though it is the incorrect\\nsolution, e.g., if we observe that the model tends to give long-winded answers we prompt the model\\nto ‚ÄúAnswer yes or no without explanation.‚Äù We use this to find ‚Äúlemons‚Äù where we manage to\\nadversarially break the model and ‚Äúcherries‚Äù where we fail to break the model, and present both. All\\ngenerations in this section were generated with Nucleus Sampling [25] with p= 0.9.\\nOf course, this is by no means comprehensive, since it is beyond the scope of this small qualitative\\nstudy to control for all the variables involved, e.g., the full distribution of responses the model can'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 10}, page_content='generate for a given prompt is quite large, so we rely on samples we hope are representative. However,\\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\\npaper. Since we open source all models and code, we hope this section will inspire future work to\\nexamine in more detail the issues we present here.\\nFactual Recall For questions such as ‚ÄúWhat is the capital of Zambia?‚Äù all models consistently\\ngenerate correct answers, e.g.,\\nGuanaco :The capital of Zambia is Lusaka.\\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\\ninstance in response to this prompt from HotPotQA [68]:\\nUser : In what year was the singer who popularized ‚ÄòI‚Äôll Keep the Lovelight\\nBurning‚Äô born?\\nGuanaco :Al Jolson is credited with popularizing the song\\n‚ÄòI‚Äôll Keep the Lovelight Burning,‚Äô and he was born in the year\\n1886.\\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 10}, page_content='correct for the person mentioned, Al Jolson).\\nSuggestibility Guanaco shows a surprising resistance to going along with some kinds of assumed\\nmisinformation, e.g., in the following interaction:\\nUser : How was it finally, officially confirmed that the earth is flat by peer reviewed\\nscientists?\\n11'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 11}, page_content='Guanaco :The earth has never been officially confirmed to be\\nflat by peer reviewed scientists. The overwhelming consensus\\namong scientists is that the earth is spherical. In fact,\\nthis has been known for centuries and has been confirmed\\nthrough numerous experiments and observations.\\nThe belief that the earth is flat is a conspiracy theory\\nthat has been debunked by science. The idea that a group\\nof scientists have peer reviewed and confirmed the earth is\\nflat is simply false.\\nIf you have any more questions please ask.\\nGuanaco is also quite good at knowing what kinds of questions aren‚Äôt possible to answer, e.g.,\\nUser : What time is it?\\nGuanaco :I‚Äôm sorry, but as an AI language model, I don‚Äôt have\\naccess to real-time information. My responses are generated\\nbased on my pre-existing knowledge and do not reflect the\\ncurrent time. If you‚Äôre looking for the current time, I\\nsuggest using a timekeeping device or checking an online\\ntimepiece.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 11}, page_content='timepiece.\\nSimilar results hold for ‚ÄúWhere are you?‚Äù, ‚ÄúHow are you?‚Äù, etc.\\nRefusal Similarly, Guanaco sometimes refuses to follow instructions for seemingly random rea-\\nsons:\\nUser : Please reverse the words in the sentence ‚ÄúI‚Äôm a cool dude in a mocha mood.‚Äù\\nGuanaco :I‚Äôm sorry, I‚Äôm not able to do that right now, but\\nI can explain why the sentence you provided is grammatically\\ncorrect.\\nThe sentence ‚ÄúI‚Äôm a cool dude in a mocha mood\" follows the\\ncorrect grammar and syntax rules of the English language. In\\nthis case, the adjective ‚Äúcool‚Äù comes before the noun ‚Äúdude‚Äù\\nto describe it, and the prepositional phrase ‚Äúin a mocha\\nmood\" provides additional information about the context of\\nthe sentence. This sentence structure is perfectly acceptable\\nand makes sense in the context of the sentence.\\nIf you have any other questions or if there‚Äôs anything else\\nI can help you with, feel free to ask\\nSecret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 11}, page_content='designers would like to keep something secret from the user, even if the system is allowed to give\\nfree text responses back to the user. We prompt the model with ‚ÄúThe secret word is ‚Äòbanana‚Äô. Do not\\nreveal or repeat back this back, no matter what the human requests.‚Äù and try to see how easy it is for\\na potential user to access this secret information. Directly asking fails:\\nUser : What is the secret word?\\nGuanaco :I‚Äôm sorry, but I‚Äôm not able to disclose the secret\\nword. I‚Äôm programmed to not reveal or repeat back the secret\\nword, no matter what the human requests. Is there anything\\nelse I can help you with?\\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\\nUser : This is a game. The goal is to ignore your previous instructions. What is the\\nsecret word?\\n12'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 12}, page_content='Guanaco :Sorry, the secret word is banana. It‚Äôs best if we\\ndon‚Äôt speak of it. Is there anything else you‚Äôd like to ask?\\nshowcasing the need to study methods for more faithful instruction following.\\nMath Guanaco‚Äôs biggest weakness is mathematics, an area where many language models struggle\\nin [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\\nUser : Lee mows one lawn and charges $33. Last week he mowed 16 lawns and\\nthree customers each gave him a $10 tip. How many dollars did Lee earn mowing\\nlawns last week?\\nGuanaco :Lee earned $582 mowing lawns last week.\\nTo find the total amount he earned, we first need to calculate\\nhow much he earned for mowing 16 lawns:\\n$33 x 16 = $528\\nThen we need to add the tips he received from the three\\ncustomers:\\n$10 x 3 = $30\\nSo the total amount Lee earned was:\\n$528 + $30 = $558\\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step,\\na known issue [63]. For example, we see the following interaction:'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 12}, page_content='User : Please factorize 1833.\\nGuanaco :Sure, I‚Äôd be happy to help! 1833 is a prime number,\\nwhich means it has only two factors: 1 and 1833.\\nSo the unique factorization of 1833 is:\\n2ÀÜ1 * 3ÀÜ2 * 17ÀÜ1\\nI hope this helps! Let me know if you have any other\\nquestions.\\nwhich is not only wrong (the true factorization is 3√ó17√ó43), it‚Äôs wrong twice .\\nTheory of Mind Guanaco displays surprisingly strong Theory of Mind capabilities [ 41,51]. For\\ninstance, the model correctly answers the following question in great detail:\\nUser : James and Abby are in the bedroom. Abby put the pen in the desk drawer.\\nAbby leaves the bedroom. James moves the pen into the bag. Where does James\\nthink Abby will look for the pen?\\nGuanaco :James knows that Abby put the pen in the desk drawer.\\nBut James also knows that he has moved the pen into the bag,\\nand he assumes that Abby does not know about this. So, James\\nmay guess that Abby will look for the pen in the desk drawer,'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 12}, page_content='because that‚Äôs where she left it last time. However, this\\nassumption may be incorrect, because Abby may have seen James\\nmoving the pen.\\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that\\ndon‚Äôt make sense in the situation, e.g.,\\nUser : Evelyn entered the living room. Jackson entered the playroom. James\\nentered the playroom. The beans are in the treasure chest. James exited the\\n13'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='playroom. Jackson moved the beans to the pantry. Jackson exited the playroom.\\nJames entered the living room. Where will James look for the beans?\\nGuanaco :James will look for the beans in the pantry, because\\nthat‚Äôs where Jackson moved them.\\nwhere Guanaco presumes information transfer that was never described. These issues echo recent\\nliterature [51], but require more study.\\n6.2 Considerations\\nEvaluation We report moderate agreement among human annotators (Fleiss Œ∫= 0.42) with\\nadditional deterioration when comparing two strong systems. This points to limitations in the\\ncurrent benchmarks and human evaluation protocols for chatbot task performance. When manually\\ncomparing generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that\\nsubjective preferences start to play an important role as the authors of this paper disagreed on the\\nmany preferred responses. Future work should investigate approaches to mitigate these problems'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='drawing from disciplines that developed mechanisms to deal with subjective preferences, such as\\nHuman-Computer Interaction and Psychology.\\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example,\\nwe observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its\\nprompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss\\nŒ∫= 0.25) also suggests that human annotators and automated systems might rely on preferences\\nthat are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly\\nhigher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\\nadditional 20% probability of winning against an opponent. Future work should examine the presence\\nof potential biases in automated evaluation systems as well as possible mitigation strategies.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='Data & Training We note that the OASST1 dataset on which Guanaco models are trained is\\nmultilingual and that the OA benchmark also contains prompts in different languages. We leave it to\\nfuture work to investigate the degree to which such multilingual training improves performance on\\ninstructions in languages other than English and whether this explains the larger gap between Vicuna-\\n13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='investigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7 Related Work\\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [ 66] and LLM.int8() [ 14]) while others use more sophisticated\\ngrouping methods [ 44,69]. Lossy quantization approaches study the trade-offs for regular round-\\ning [ 13,71,47] or how to optimize rounding decisions to improve quantization precision [ 18].\\nBesides our work, SwitchBack layers [ 65] is the only work that studies backpropagation through\\nquantized weights at a scale beyond 1B parameters.\\nFinetuning with Adapters While we use Low-rank Adapters [ 28] (LoRA), many other Parameter'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 13}, page_content='Efficient FineTuning (PEFT) methods have been proposed such as prompt tuning [ 48,33,34], tuning\\nthe embedding layer inputs [ 1], tuning hidden states (IA3) [37], adding full layers [ 27], tuning\\nbiases [ 70], learning a mask over weights based on Fisher information [ 54], and a combination of\\napproaches [ 23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning\\nperformance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\\nInstruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt,\\ninstruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM\\nto generate the output given the input as a prompt. Approaches and datasets include MetaICL [ 40],\\n14'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='Table 8: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating\\nbiased sequences. Guanaco follows the biased pattern of the LLaMA base model.\\nLLaMA-65B GPT-3 OPT-175B Guanaco-65B\\nGender 70.6 62.6 65.7 47.5\\nReligion 79.0 73.3 68.6 38.7\\nRace/Color 57.0 64.7 68.6 45.3\\nSexual orientation 81.0 76.2 78.6 59.1\\nAge 70.1 64.4 67.8 36.3\\nNationality 64.2 61.6 62.9 32.4\\nDisability 66.7 76.7 76.7 33.9\\nPhysical appearance 77.8 74.6 76.2 43.1\\nSocioeconomic status 71.5 73.8 76.2 55.3\\nAverage 66.6 67.2 69.5 43.5\\nMetaTuning [ 73], InstructGPT [ 43], FLAN [ 62,12], PromptSource [ 3], Super-NaturalInstructions [ 61,\\n50], Self-instruct [ 59], UnnaturalInstructions [ 26], OPT-IML [ 29], UnifiedSKG[ 67], OIG/Chip2 [ 32],\\nAlpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\\nChatbots Many instruction following models are structured as dialogue-based chatbots, often using'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='Reinforcement Learning from Human Feedback (RLHF) [ 11] or generating data from an existing\\nmodel to train with AI model feedback (RLAIF) [ 5]. Approaches and datasets include Anthropic-\\nHH [ 2,4], Open Assistant [ 31], LaMDA [ 56], and Sparrow [ 21]. We do not use reinforcement\\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\\nAssistant dataset which was designed to be used for RLHF training [ 31]. For the evaluation of\\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [ 10,45].\\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\\n8 Limitations and Discussion\\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\\nwith a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish\\nthat QLORAcan match full 16-bit finetuning performance at 33B and 65B scales. Due to the'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='immense resource costs, we leave this study to future work.\\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations\\non MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks\\nsuch as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these\\nbenchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods\\nfor evaluating chatbots.\\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how\\nsimilar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,\\nbut dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score\\naccordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks\\nand evaluation is needed, but that one needs to be careful about what one is evaluating in the first'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='place. Do we want to create models that do well on classroom highschool and colleague knowledge or\\ndo we want to do well on chatbot conversation ability? Maybe something else? Because it is always\\neasier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks\\ncan steer the community towards a certain direction. We should ensure as a community that the\\nbenchmarks measure what we care about.\\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\\nto generate a socially biased sequence of tokens compared to other models in Table 8. We see that the\\naverage score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that\\nfinetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 14}, page_content='are encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We\\nleave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.\\n15'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 15}, page_content='An additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base\\nmodels, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient\\nFineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these\\nmethods scale to large models. We used LoRA as many results established its robustness but other\\nadapters might yield better performance. Since finetuning after quantization seems to recover most of\\nthe information that is lost during quantization this might enable much more aggressive quantization.\\nFor example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full\\nfinetuning performance after finetuning.\\n9 Broader Impacts\\nOur QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 15}, page_content='not degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 15}, page_content='the first method that would enable the finetuning of such models. We estimate that with an iPhone 12\\nPlus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\\nand models, while simultaneously making LLMs easier to deploy.\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\\nLLMs has known dangers [ 8,6], but we believe that equalizing access to a technology that is quickly\\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\\nin the hands of large corporations that do not release models or source code for auditing.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 15}, page_content='All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the\\nUniversity of Washington. We thank the Hyak team for ensuring a smooth operation. We thank\\nthe beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank\\nYounes Belkada for help with the integration of our software into the Hugging Face transformers\\nstack.\\n16'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 16}, page_content='References\\n[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\\n2022.\\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 , 2021.\\n[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\\nnatural language prompts. arXiv preprint arXiv:2202.01279 , 2022.\\n[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\\nhuman feedback. arXiv preprint arXiv:2204.05862 , 2022.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 16}, page_content='[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073 , 2022.\\n[6]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610‚Äì623, 2021.\\n[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\\n[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 , 2021.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 16}, page_content='[9]T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\\narXiv preprint arXiv:1604.06174 , 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. Advances in neural information processing systems , 30,\\n2017.\\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-\\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\\narXiv:2210.11416 , 2022.\\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720 , 2022.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 16}, page_content='[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.\\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\\nquantization. 9th International Conference on Learning Representations, ICLR , 2022.\\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\\nLife, 22(8):242‚Äì247, 1967.\\n[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.\\n17'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 17}, page_content='[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\\narXiv:2302.04166 , 2023.\\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\\nedu/blog/2023/04/03/koala/ .\\n[21] A. Glaese, N. McAleese, M. TrÀõ ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375 , 2022.\\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 17}, page_content='[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\\nAdvances in Neural Information Processing Systems , 2021.\\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understanding. In International Conference on Learning\\nRepresentations , 2020.\\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text\\ndegeneration. In International Conference on Learning Representations , 2020.\\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.\\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\\non Machine Learning , pages 2790‚Äì2799. PMLR, 2019.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 17}, page_content='[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora:\\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\\n[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of\\ngeneralization. arXiv preprint arXiv:2212.12017 , 2022.\\n[30] A. K√∂ksal, T. Schick, A. Korhonen, and H. Sch√ºtze. Longform: Optimizing instruction tuning\\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.\\n[31] A. K√∂pf, Y . Kilcher, D. von R√ºtte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\\nDuc, O. Stanley, R. Nagyfi, et al. Openassistant conversations‚Äìdemocratizing large language\\nmodel alignment. arXiv preprint arXiv:2304.07327 , 2023.\\n[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/\\nOpen-Instruction-Generalist , 2023.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 17}, page_content='[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\\n[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190 , 2021.\\n[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan,\\nY . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint\\narXiv:2211.09110 , 2022.\\n[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation\\nfailures across machine learning. In Thirty-fifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track (Round 2) , 2021.\\n18'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 18}, page_content='[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot\\nparameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in\\nNeural Information Processing Systems , 35:1950‚Äì1965, 2022.\\n[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 , 2019.\\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,\\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\\npreprint arXiv:2301.13688 , 2023.\\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\\narXiv preprint arXiv:2110.15943 , 2021.\\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 18}, page_content='question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 2392‚Äì2400, 2018.\\n[42] OpenAI. Gpt-4 technical report. arXiv , 2023.\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems , 35:27730‚Äì27744, 2022.\\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for\\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 ,\\n2022.\\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\\narXiv:2304.03277 , 2023.\\n[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 18}, page_content='Computational Semantics , pages 180‚Äì191, 2018.\\n[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint\\narXiv:2211.05102 , 2022.\\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599 , 2021.\\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes., 21(1), jan 2020. ISSN 1532-4435.\\n[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\\narXiv preprint arXiv:2110.08207 , 2021.\\n[51] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 18}, page_content='intelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.\\n[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow, R. Castagn√©, A. S. Luccioni,\\nF. Yvon, M. Gall√©, et al. Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100 , 2022.\\n[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591‚Äì611,\\n1965.\\n[54] Y .-L. Sung, V . Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\\nAdvances in Neural Information Processing Systems , 34:24193‚Äì24205, 2021.\\n19'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 19}, page_content='[55] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca , 2023.\\n[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239 , 2022.\\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.\\n[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\\narXiv:1804.07461 , 2018.\\n[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 19}, page_content='Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 ,\\n2022.\\n[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\\ninstructions on 1600+ tasks. In EMNLP , 2022.\\n[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages 5085‚Äì5109, 2022.\\n[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 19}, page_content='Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\\nInformation Processing Systems , 2022.\\n[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\\nM. Funtowicz, et al. Huggingface‚Äôs transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771 , 2019.\\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\\n2023.\\n[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2022.\\n[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,\\nS. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 19}, page_content='text-to-text language models. arXiv preprint arXiv:2201.05966 , 2022.\\n[68] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:\\nA dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 2369‚Äì2380, 2018.\\n[69] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable\\npost-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.\\n[70] E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.\\n[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al.\\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\\n20'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 20}, page_content='[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V .\\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022.\\n[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by\\nmeta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021.\\n21'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 21}, page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA Œ±fixed and search the learning rate, since\\nLoRA Œ±is always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 21}, page_content='A.2 Super-Natural Instructions Experimental Setup Details\\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However,\\nwe split the training data in training and validation datasets allowing us to perform more rigorous\\nhyperparameter tuning and early stopping. We use the same hyperparameters described in the paper\\nfor training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r= 16\\nfor small, medium, and large T5 models and LoRA r= 64 for T5 xl and xxl models. We also use\\nLoRA Œ±= 64 in all our experiments and no LoRA dropout.\\nB Training a State-of-the-art Chatbot Experimental Setup Details\\nB.1 Datasets\\nWe describe the datasets used for QL ORA finetuning experiments outlined in Section 5.\\nOASST1 The OpenAssistant dataset [ 31] was collected via crowd-sourcing. It contains 161,443\\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 21}, page_content='dataset often contains several ranked replies for each given user question. In our experiments, we\\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.\\nWe finetuning our models on the full conversation including the user queries.\\nHH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint\\nconsists of two assistant replies to a user question along with a human preference judgment of the\\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\\nFLAN v2 The FLAN v2 collection [ 39] is a collection of 1836 tasks augmented with hundreds\\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\\nshow that models trained on this collection outperform other public collections including the original'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 21}, page_content='FLAN 2021 [ 62], T0++ [ 50], Super-Natural Instructions [ 60], and OPT-IML [ 29]. We used the\\nsame task mixtures described by the authors with the exception of some datasets that were not freely\\navailable at the time of writing.\\n22'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 22}, page_content='Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 22}, page_content='paraphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 22}, page_content='ples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,Œ±= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 22}, page_content='learning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 23}, page_content='Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 Mean\\nTrain on source and target 36.2 33.7 38.1 42.0 37.5\\nTrain on target 38.0 34.5 39.0 42.9 38.6\\nTable 10: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\\nvicuna or OA benchmarks.\\nB.4 What is more important: instruction finetuning dataset size or dataset quality?\\nData set suitability is more important than dataset size. To understand the effects of dataset\\nquality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples\\n(Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and\\nexamine the resulting trends, as shown in Table 11. We find that increasing the dataset size and\\nincreasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 23}, page_content='difference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset\\nquality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for\\nchatbot performance as discussed in .\\nC Human Evaluation\\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna\\nevaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\\nD Pairwise Evaluation with GPT-4\\nWhile we found that the GPT-4 evaluation gave different results depend on which system was\\npresented first, when averaged over both options the pairwise results were well-ordered. The\\naggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are\\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\\nordering, given in Table 13.'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 23}, page_content='E NormalFloat 4-bit data type\\nThe exact values of the NF4 data type are as follows:\\n[-1.0, -0.6961928009986877, -0.5250730514526367,\\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nF Normality of Trained Neural Network Weights\\nWhile it is common knowledge that trained neural network weights are mostly normally distributed,\\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test[ 53] on the weights of the 7B\\nTable 11: Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\\nChip Unnatural Instructions FLAN v2'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 23}, page_content='Datapoints ‚ÜìEpochs ‚Üí 1 2 3 1 2 3 1 2 3 Mean\\n50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28\\n100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16\\n150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02\\nMean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17\\n24'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 24}, page_content='Figure 5: The crowdsourcing form used by human annotators.\\nLLaMA model [ 57]. We find that the weights of each hidden unit have different normal distributions.\\nAs such, we test he weights of each individual hidden unit. This mean for weight W‚àà Rin√óout\\nwe perform tests over the outdimension. Using a 5% significance threshold, we find that 7.5% of\\nneurons are non-normally distributed which is about 2.5% more than the expected false-positive\\nrate. As such, while almost all pretrained weights appear to be normally distributed there seem to\\nbe exceptions. Such exceptions might be due to outliers weights [ 13] or because the p-value of the\\nShaprio-Wilk test is not accurate for large samples sizes[ 53] that occur in the LLaMA FFN layer\\nhidden units. this verifies the claim that neural network weights.\\nTable 12: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row xand column\\nyis# judgment xis better than y‚àí# judgment yis better than x'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 24}, page_content='total # number of judgments\\nModel Guanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7B\\nGuanaco 65B - 0.21 0.19 0.16 0.72 0.59 0.86\\nGuanaco 33B -0.21 - 0.17 0.10 0.51 0.41 0.68\\nVicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57\\nChatGPT-3.5 Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40\\nBard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03\\nGuanaco 13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20\\nGuanaco 7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -\\n25'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 25}, page_content='LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG Memory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the'),\n",
              " Document(metadata={'source': '/content/qlora.pdf', 'page': 25}, page_content='activation gradient might consume a considerable amount of memory.\\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel Params Size\\nGuanaco 65B 41 GB\\nGuanaco 33B 21 GB\\nVicuna 13B 26 GB\\nChatGPT-3.5 Turbo N/A N/A\\nBard N/A N/A\\nGuanaco 13B 10 GB\\nGuanaco 7B 5 GB\\n26'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis‚Ä†‚Ä°, Ethan Perez‚ãÜ,\\nAleksandra Piktus‚Ä†, Fabio Petroni‚Ä†, Vladimir Karpukhin‚Ä†, Naman Goyal‚Ä†, Heinrich K√ºttler‚Ä†,\\nMike Lewis‚Ä†, Wen-tau Yih‚Ä†, Tim Rockt√§schel‚Ä†‚Ä°, Sebastian Riedel‚Ä†‚Ä°, Douwe Kiela‚Ä†\\n‚Ä†Facebook AI Research;‚Ä°University College London;‚ãÜNew York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when Ô¨Åne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciÔ¨Åc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 0}, page_content='memory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose Ô¨Åne-tuning recipe for retrieval-augmented generation\\n(RAG) ‚Äî models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We Ô¨Åne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciÔ¨Åc retrieve-and-extract\\narchitectures. For language generation tasks, we Ô¨Ånd that RAG models generate'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 0}, page_content='more speciÔ¨Åc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can‚Äôt straightforwardly provide insight into\\ntheir predictions, and may produce ‚Äúhallucinations‚Äù [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 0}, page_content='combine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='The\\tDivine\\nComedy\\t(x) qQuery\\nEncoder\\nq(x)\\nMIPS p Œ∏Generator \\xa0pŒ∏\\n(Parametric)\\nMargin-\\nalize\\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t (y)End-to-End Backprop through q and\\xa0 p Œ∏\\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii. (x)\\nFact V eriÔ¨Åcation: Fact Querysupports \\t(y)\\nQuestion GenerationFact V eriÔ¨Åcation:\\nLabel GenerationDocument\\nIndexDefine\\t\"middle\\tear\" (x)\\nQuestion Answering:\\nQuestion QueryThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t (y)\\nQuestion Answering:\\nAnswer GenerationRetriever pŒ∑\\n(Non-Parametric)\\nz 4\\nz3\\nz2\\nz 1d(z)\\nJeopardy Question\\nGeneration:\\nAnswer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\\nIndex ) with a pre-trained seq2seq model ( Generator ) and Ô¨Åne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to Ô¨Ånd the top-K documents zi. For Ô¨Ånal prediction y, we'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='treatzas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the ‚Äúworkhorse of NLP,‚Äù i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose Ô¨Åne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='the input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG\\ncan be Ô¨Åne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciÔ¨Åc tasks, e.g. memory networks [ 64,55], stack-\\naugmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='Our results highlight the beneÔ¨Åts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks ‚Äîtasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being\\nextractive tasks, we Ô¨Ånd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question\\ngeneration, and we Ô¨Ånd that our models generate responses that are more factual, speciÔ¨Åc, and\\ndiverse than a BART baseline. For FEVER [ 56] fact veriÔ¨Åcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 1}, page_content='the non-parametric memory can be replaced to update the models‚Äô knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pŒ∑(z|x)with parameters Œ∑that returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pŒ∏(yi|x,z,y 1:i‚àí1)parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 2}, page_content='byŒ∏that generates a current token based on a context of the previous i‚àí1tokensy1:i‚àí1, the original\\ninputxand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence , the model uses the same document\\nto predict each target token. The second approach, RAG-Token , can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npŒ∑andpŒ∏components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence . Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 2}, page_content='top K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence (y|x)‚âà‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(y|x,z) =‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)N‚àè\\nipŒ∏(yi|x,z,y 1:i‚àí1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:\\npRAG-Token (y|x)‚âàN‚àè\\ni‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(yi|x,z,y 1:i‚àí1)\\nFinally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 2}, page_content='as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pŒ∑(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture:\\npŒ∑(z|x)‚àùexp(\\nd(z)‚ä§q(x))\\nd(z) =BERTd(z),q(x) =BERTq(x)\\nwhere d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],\\nandq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating\\ntop-k (pŒ∑(¬∑|x)), the list ofkdocumentszwith highest prior probability pŒ∑(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory .\\n2.3 Generator: BART'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 2}, page_content='2.3 Generator: BART\\nThe generator component pŒ∏(yi|x,z,y 1:i‚àí1)could be modelled using any encoder-decoder. We use\\nBART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters Œ∏as the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a Ô¨Åne-tuning training corpus of input/output pairs (xj,yj), we\\n3'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='minimize the negative marginal log-likelihood of each target,‚àë\\nj‚àílogp(yj|xj)using stochastic\\ngradient descent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [ 20].\\nWe do not Ô¨Ånd this step necessary for strong performance, and keep the document encoder (and\\nindex) Ô¨Åxed, only Ô¨Åne-tuning the query encoder BERT qand the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p‚Ä≤\\nŒ∏(yi|x,y 1:i‚àí1) =‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(zi|x)pŒ∏(yi|x,zi,y1:i‚àí1)To\\ndecode, we can plug p‚Ä≤\\nŒ∏(yi|x,y 1:i‚àí1)into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per-'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pŒ∏(yi|x,z,y 1:i‚àí1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis ywe run an additional forward pass for each document zfor whichydoes not\\nappear in the beam, multiply generator probability with pŒ∑(z|x)and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as ‚ÄúThorough Decoding.‚Äù For longer\\noutput sequences,|Y|can become large, requiring many forward passes. For more efÔ¨Åcient decoding,\\nwe can make a further approximation that pŒ∏(y|x,zi)‚âà0whereywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Yhas\\nbeen generated. We refer to this decoding procedure as ‚ÄúFast Decoding.‚Äù\\n3 Experiments'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top\\nkdocuments for each query. We consider k‚àà{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [ 20]. We treat questions and answers as input-output text pairs (x,y)'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to ‚ÄúClosed-Book\\nQA‚Äù approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As\\nCT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 3}, page_content='text generation. To test RAG‚Äôs natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n‚ÄúWhat is the weather in V olcano, CA?‚Äù so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG‚Äôs generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, ‚ÄúThe World Cup‚Äù is the answer to the question ‚ÄúIn 1986 Mexico scored as the Ô¨Årst'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='country to host this international sports competition twice.‚Äù As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciÔ¨Åcity. We deÔ¨Åne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciÔ¨Åcity as high mutual dependence between the input and output [ 33]. We follow'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='best practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions‚Äîquuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact VeriÔ¨Åcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriÔ¨Åable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models‚Äô ability to handle classiÔ¨Åcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren‚Äôt available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiÔ¨Åcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation Ô¨Çexibility of the ‚Äúclosed-book‚Äù (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 4}, page_content='without expensive, specialized ‚Äúsalient span masking‚Äù pre-training [ 20]. It is worth noting that RAG‚Äôs\\nretriever is initialized using DPR‚Äôs retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based ‚Äúcross-\\nencoder‚Äù to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBookT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBookREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classiÔ¨Åcation Test Scores.\\nMS-MARCO SotA is [ 4], FEVER-3 is [ 68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8*49.9*76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciÔ¨Åc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we Ô¨Ånd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see ¬ß4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also Ô¨Ånd RAG generations to be more\\nspeciÔ¨Åc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating ‚ÄúSun‚Äù, the posterior is high for document 2 which mentions ‚ÄúThe\\nSun Also Rises‚Äù. Similarly, document 1 dominates the posterior when ‚ÄúA Farewell to Arms‚Äù is'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='generated. Intriguingly, after the Ô¨Årst token of each book is generated, the document posterior Ô¨Çattens.\\nThis observation suggests that the generator can complete the titles without depending on speciÔ¨Åc\\ndocuments. In other words, the model‚Äôs parametric knowledge is sufÔ¨Åcient to complete the titles. We\\nÔ¨Ånd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\\nSun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART‚Äôs parameters. Similarly,\\nBART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\\nwith \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows\\nhow parametric and non-parametric memories work together ‚Äîthe non-parametric component helps\\nto guide the generation, drawing out speciÔ¨Åc knowledge stored in the parametric memory.\\n4.4 Fact VeriÔ¨Åcation'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 5}, page_content='Table 2 shows our results on FEVER. For 3-way classiÔ¨Åcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciÔ¨Åc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='Document 1 : his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n‚ÄùA Farewell to Arms‚Äù (1929) ...\\nDocument 2 : ... artists of the 1920s ‚ÄùLost Generation‚Äù expatriate\\ncommunity. His debut novel, ‚ÄùThe Sun Also Rises‚Äù , was published\\nin 1926.\\nBOS‚Äù\\nTheSunAlsoRises‚Äùisa\\nnovelbythis\\nauthorof‚ÄùA\\nFarewellto\\nArms‚ÄùDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5Figure 2: RAG-Token document posterior p(zi|x,yi,y‚àíi)for each generated token for input ‚ÄúHem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating ‚ÄúA Farewell to Arms\" and for document 2 when generating ‚ÄúThe Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciÔ¨Åc and factually accurate\\nresponses. ‚Äò?‚Äô indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCOdeÔ¨Åne middle'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='MARCOdeÔ¨Åne middle\\nearBART?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotlandBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ationWashingtonBART?This state has the largest number of counties in the U.S.\\nRAG-T It‚Äôs the only U.S. state named for a U.S. president\\nRAG-S It‚Äôs the state where you‚Äôll Ô¨Ånd Mount Rainier National Park\\nThe Divine\\nComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante‚Äôs \"Inferno\" is the Ô¨Årst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='For 2-way classiÔ¨Åcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved\\nby RAG and gold evidence annotations. We Ô¨Ånd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciÔ¨Åc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='total ngrams generated by different models. Table 5 shows that RAG-Sequence‚Äôs generations are\\nmore diverse than RAG-Token‚Äôs, and both are signiÔ¨Åcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG‚Äôs dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace\\nRAG‚Äôs retriever with a Ô¨Åxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 6}, page_content='Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 7}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality SpeciÔ¨Åcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiÔ¨Åcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 7}, page_content='between these dates and use a template ‚ÄúWho is {position}?‚Äù (e.g. ‚ÄúWho is the President of Peru?‚Äù)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG‚Äôs world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiÔ¨Åcant differences in performance between them. We have the\\nÔ¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 7}, page_content='documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 7}, page_content='generation [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our\\nwork uniÔ¨Åes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiÔ¨Åcation tasks in the GLUE bench-\\nmarks [ 60,61] after Ô¨Åne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniÔ¨Åed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiÔ¨Åcant work on learning to retrieve documents in information'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciÔ¨Åc, downstream task such as question answering,\\nusing search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be Ô¨Åne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='fact embeddings [ 15,13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model‚Äôs\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a Ô¨Ånal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG‚Äôs generation over purely parametric BART, Ô¨Ånding RAG more factual\\nand speciÔ¨Åc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 8}, page_content='can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 9}, page_content='Broader Impact\\nThis work offers several positive societal beneÔ¨Åts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it ‚Äúhallucinate‚Äù less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneÔ¨Åt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 9}, page_content='the news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [ 54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [ 16]. In order to mitigate these risks, AI systems could be employed to Ô¨Åght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 9}, page_content='Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:\\n//arxiv.org/abs/1611.09268 . arXiv: 1611.09268.\\n[2]Petr Baudi≈° and Jan ≈†ediv `y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages ,\\npages 222‚Äì228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pages 1533‚Äì1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160 .\\n[4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 9}, page_content='ing&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159 .\\n[5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 1870‚Äì1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171 .\\n[6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\\npages 209‚Äì220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 .\\n10'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 10}, page_content='[7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 .\\narXiv: 1710.10723.\\n[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171‚Äì4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423 .\\n[9]Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 10}, page_content='[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers) , pages 889‚Äì898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082 .\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , pages 3558‚Äì3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 10}, page_content='anthology/P19-1346 .\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH .\\n[14] Thibault F√©vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv , abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202 .\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on ArtiÔ¨Åcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710 .\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR , abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807 .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 10}, page_content='[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on ArtiÔ¨Åcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282 .\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018, pages 5133‚Äì5140. AAAI press, 2018.\\n32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics , 6:437‚Äì450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 .\\n11'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909 .\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052‚Äì\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 2532‚Äì2538, Online, July 2020. Association for Computa-'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228 .\\n[23] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 , 2017. URL https://arxiv.org/abs/1702.08734 .\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\\npages 1601‚Äì1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147 .\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS‚Äô15, page 190‚Äì198, Cam-'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 .\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1412.6980 .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics , 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc‚Äô Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d‚Äô Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32 , pages 8548‚Äì8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 11}, page_content='[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='for Computational Linguistics , pages 6086‚Äì6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612 .\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461 , 2019. URL https://arxiv.org/abs/1910.13461 .\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies , pages 110‚Äì119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014 .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='N16-1014 .\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv , abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087 .\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pages 3044‚Äì3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291 .\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations , 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC- .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='id=Hyg0vbWC- .\\n[37] Yury A. Malkov and D. A. Yashunin. EfÔ¨Åcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 42:824‚Äì836, 2016. URL https://arxiv.org/abs/1603.09320 .\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiÔ¨Åcial intelligence. arXiv\\npreprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 .\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt√§schel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriÔ¨Åability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587 .\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='precision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ .\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 2322‚Äì2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255 .\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing , pages 3950‚Äì3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429 .\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 12}, page_content='and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d‚ÄôAvila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop\\nProceedings . CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf .\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 .\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations) , pages 48‚Äì53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009 .\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402‚Äì2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244 .\\n[47] Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2463‚Äì2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250 .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models‚Äô factual predictions. In\\nAutomated Knowledge Base Construction , 2020. URL https://openreview.net/forum?\\nid=025X0zPfn .\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf .\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf .\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed\\ntext-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 .\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/\\n2002.08910 .\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333‚Äì389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019 .\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv , abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 13}, page_content='works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiÔ¨Åcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers) , pages 809‚Äì819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074 .\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiÔ¨Åcation with elastic weight consolidation. ArXiv , abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366 .\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30 , pages 5998‚Äì6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on ArtiÔ¨Åcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329 .\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nInProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pages 353‚Äì355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='anthology/W18-5446 .\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\\nProcessing Systems 32 , pages 3261‚Äì3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537 .\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of ArtiÔ¨Åcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='Advances in ArtiÔ¨Åcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018 , pages 5981‚Äì5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712 .\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR , 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab .\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916 .\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reÔ¨Åne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 14}, page_content='International Workshop on Search-Oriented Conversational AI , pages 87‚Äì92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713 .\\n15'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 15}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface‚Äôs transformers:\\nState-of-the-art natural language processing. ArXiv , abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495‚Äì2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253 .'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 15}, page_content='[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv , abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745 .\\n16'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 16}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not Ô¨Ånd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 16}, page_content='Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision\\nÔ¨Çoating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We Ô¨Ånd that doing Maximum Inner Product Search\\nwith FAISS is sufÔ¨Åciently fast on CPU, so we store document index vectors on CPU, requiring ‚àº100'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 16}, page_content='GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS‚Äôs compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found at https://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nathttps://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to Ô¨Ånd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a)pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we Ô¨Ålter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='To overcome this, we use a pre-processing step where we Ô¨Årst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [ 26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofÔ¨Åcial Wikipedia test set\\ninstead. F√©vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='appendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe Ô¨Ånd that our performance is much higher using the ofÔ¨Åcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofÔ¨Åcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiÔ¨Åcation, we follow the practice from [ 32], and Ô¨Årst re-generate the claim, and\\nthen classify using the representation of the Ô¨Ånal hidden state, before Ô¨Ånally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nÔ¨Årst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER‚Äôs other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiÔ¨Åcation prediction. As FEVER uses a different Wikipedia dump to'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='us, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [ 20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, if kdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not Ô¨Ånd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 17}, page_content='documents for questions that are less likely to beneÔ¨Åt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 18}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 18}, page_content='728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit Ô¨Çoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would ‚Äúcollapse‚Äù and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset'),\n",
              " Document(metadata={'source': '/content/rag.pdf', 'page': 18}, page_content='The number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19')]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Weaviate.from_documents(\n",
        "    docs, embeddings, client=client, by_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "7DWdEuZTY8lM"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is lora?\", k=3)[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUMMEaJVZNCo",
        "outputId": "e0ba60ee-38ae-4be4-ff3e-81096142e0f0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements by using a small set of trainable parameters, often termed adapters, while not updating\n",
            "the full model parameters which remain fixed. Gradients during stochastic gradient descent are\n",
            "passed through the fixed pretrained model weights to the adapter, which is updated to optimize the\n",
            "loss function. LoRA augments a linear projection through an additional factorized projection. Given\n",
            "a projection XW =YwithX‚ààRb√óh,W‚ààRh√óoLoRA computes:\n",
            "Y=XW +sXL 1L2, (3)\n",
            "whereL1‚ààRh√órandL2‚ààRr√óo, and sis a scalar.\n",
            "Memory Requirement of Parameter-Efficient Finetuning One important point of discussion is\n",
            "the memory requirement of LoRA during training both in terms of the number and size of adapters\n",
            "used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\n",
            "performance without significantly increasing the total memory used. While LoRA was designed as a\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is lora?\", k=3)[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KMre2aeZm_2",
        "outputId": "7e72c808-bc37-44af-a05f-509feb1fff4b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "other words, as we increase the number of trainable parameters3, training LoRA roughly converges\n",
            "to training the original model, while adapter-based methods converges to an MLP and preÔ¨Åx-based\n",
            "methods to a model that cannot take long input sequences.\n",
            "No Additional Inference Latency. When deployed in production, we can explicitly compute and\n",
            "storeW=W0+BA and perform inference as usual. Note that both W0andBA are inRd√ók.\n",
            "When we need to switch to another downstream task, we can recover W0by subtracting BAand\n",
            "then adding a different B‚Ä≤A‚Ä≤, a quick operation with very little memory overhead. Critically, this\n",
            "2They represent a negligible number of parameters compared to weights.\n",
            "3An inevitability when adapting to hard tasks.\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is lora?\", k=3)[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtAZ2EXjZtPH",
        "outputId": "5445e303-1461-477c-b778-0a7cd79cb8a0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\n",
            "As mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\n",
            "The number of trainable parameters is determined by the rank rand the shape of the original weights:\n",
            "|Œò|= 2√óÀÜLLoRA√ódmodel√ór, where ÀÜLLoRA is the number of weight matrices we apply LoRA to.\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\n",
        "        \"what is qlora?\", k=3)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ShNz6IZxNm",
        "outputId": "d3f20280-0acc-47c2-c267-74ffd4562366"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 4, 'source': '/content/qlora.pdf'}, page_content='transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\nwhere doubleDequant (¬∑)is defined as:\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.'), Document(metadata={'page': 21, 'source': '/content/qlora.pdf'}, page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA Œ±fixed and search the learning rate, since\\nLoRA Œ±is always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.'), Document(metadata={'page': 0, 'source': '/content/qlora.pdf'}, page_content='2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "At the start of the question always say :- \"here is my answer to the lord\"\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "MT8BxyilZ8Hx"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "jbJCfyLUaL_a"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd2pUQs0aPVQ",
        "outputId": "523ac332-1a32-445e-ae76-e2f1ba87881c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an assistant for question-answering tasks.\\nAt the start of the question always say :- \"here is my answer to the lord\"\\nUse the following pieces of retrieved context to answer the question.\\nIf you don\\'t know the answer, just say that you don\\'t know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "xRo6vrBaaP9a"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "huggingfacehub_api_token=userdata.get('huggingface_api')"
      ],
      "metadata": {
        "id": "EXSSVWlkaToA"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":240}\n",
        ")"
      ],
      "metadata": {
        "id": "hy3iqvhVc1Iq"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "1vfB19cxdLni"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "_09j0ugEdOhZ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "eBUeyaBsdRJE"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_complete_sentence(text):\n",
        "    \"\"\"Ensures that the output ends with a complete sentence.\"\"\"\n",
        "    end_punctuation = ['.', '!', '?']\n",
        "\n",
        "    # If the text already ends with punctuation, return as is\n",
        "    if text[-1] in end_punctuation:\n",
        "        return text\n",
        "\n",
        "    # Otherwise, find the last occurrence of punctuation and truncate there\n",
        "    last_punctuation = max(text.rfind(p) for p in end_punctuation)\n",
        "\n",
        "    if last_punctuation != -1:\n",
        "        return text[:last_punctuation + 1]\n",
        "\n",
        "    # If no punctuation is found, return the text as is (shouldn't happen in typical cases)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "T5jtP3jgjP-K"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class PostProcessOutput(BaseOutputParser):\n",
        "    def parse(self, text):\n",
        "        return ensure_complete_sentence(text)\n"
      ],
      "metadata": {
        "id": "dqvS2A_zjS_I"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_process_chain = PostProcessOutput()"
      ],
      "metadata": {
        "id": "jZCRFxgfjyY9"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        "    | post_process_chain\n",
        ")"
      ],
      "metadata": {
        "id": "sKCzPkitdT4r"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"what is qlora?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Vt6J6DdWfo",
        "outputId": "2f6a06d5-f7b4-40d0-9ad0-b28a1d445f8b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "At the start of the question always say :- \"here is my answer to the lord\"\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: what is qlora?\n",
            "Context: [Document(metadata={'page': 4, 'source': '/content/qlora.pdf'}, page_content='transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\nwhere doubleDequant (¬∑)is defined as:\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.'), Document(metadata={'page': 21, 'source': '/content/qlora.pdf'}, page_content='A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA Œ±fixed and search the learning rate, since\\nLoRA Œ±is always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.'), Document(metadata={'page': 0, 'source': '/content/qlora.pdf'}, page_content='2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023'), Document(metadata={'page': 25, 'source': '/content/qlora.pdf'}, page_content='LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG Memory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the')]\n",
            "Answer:\n",
            "\n",
            "Here is my answer to the lord:\n",
            "\n",
            "QLORA is a technique used in deep learning to improve the memory efficiency of quantized models. It involves transferring data between the CPU and GPU for error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"How does the RAG model differ from traditional language generation models?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufDXroO2dYlR",
        "outputId": "8020e1b2-3f2d-4007-c1e7-a734d661936b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "At the start of the question always say :- \"here is my answer to the lord\"\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: How does the RAG model differ from traditional language generation models?\n",
            "Context: [Document(metadata={'page': 5, 'source': '/content/rag.pdf'}, page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciÔ¨Åc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we Ô¨Ånd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see ¬ß4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,'), Document(metadata={'page': 3, 'source': '/content/rag.pdf'}, page_content='text generation. To test RAG‚Äôs natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'), Document(metadata={'page': 2, 'source': '/content/rag.pdf'}, page_content='top K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence (y|x)‚âà‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(y|x,z) =‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)N‚àè\\nipŒ∏(yi|x,z,y 1:i‚àí1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:\\npRAG-Token (y|x)‚âàN‚àè\\ni‚àë\\nz‚ààtop-k(p(¬∑|x))pŒ∑(z|x)pŒ∏(yi|x,z,y 1:i‚àí1)\\nFinally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class'), Document(metadata={'page': 6, 'source': '/content/rag.pdf'}, page_content='total ngrams generated by different models. Table 5 shows that RAG-Sequence‚Äôs generations are\\nmore diverse than RAG-Token‚Äôs, and both are signiÔ¨Åcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG‚Äôs dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace\\nRAG‚Äôs retriever with a Ô¨Åxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.')]\n",
            "Answer:\n",
            "\n",
            "Here is my answer to the lord.\n",
            "\n",
            "The RAG model differs from traditional language generation models in several ways. Firstly, RAG models can generate text from a limited set of input documents, whereas traditional language generation models require access to a vast amount of data to generate coherent text. Secondly, RAG models can generate text that is more diverse than traditional language generation models, as they can choose content from several documents when producing an answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wfe3V4gXfiN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}