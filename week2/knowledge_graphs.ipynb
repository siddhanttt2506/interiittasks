{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CazTGtTqv1H",
        "outputId": "3203de8a-ff8a-48ab-800c-f20bc9bfc1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.24.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install neo4j transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEYkGMIBq1oD",
        "outputId": "ef0e3468-99e3-4097-dd3c-78484e0c8cc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neo4jKnowledgeGraph:\n",
        "    def _init_(self, uri, user, password):\n",
        "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.close()\n",
        "\n",
        "    def create_node(self, label, properties):\n",
        "        with self.driver.session() as session: #indented this line\n",
        "            query = f\"CREATE (n:{label} $props)\"  # Corrected query\n",
        "            session.run(query, {\"props\": properties})  # Pass properties as a parameter\n",
        "\n",
        "    def create_relationship(self, node1, node2, relationship_type):\n",
        "        with self.driver.session() as session:\n",
        "            query = \"\"\"\n",
        "            MATCH (a {name: $node1}), (b {name: $node2})\n",
        "            CREATE (a)-[r:%s]->(b)\n",
        "            \"\"\" % relationship_type\n",
        "            session.run(query, {'node1': node1, 'node2': node2})\n",
        "\n",
        "    def query_graph(self, query, params=None):\n",
        "        with self.driver.session() as session:\n",
        "            return session.run(query, params)"
      ],
      "metadata": {
        "id": "wz5m9VrxrCUt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Construct a knowledge graph from text documents\n",
        "\n",
        "def construct_knowledge_graph(neo4j_kg, documents):\n",
        "    for doc in documents:\n",
        "        sentences = doc.split('.')\n",
        "        for sentence in sentences:\n",
        "            entities = extract_entities(sentence)\n",
        "            if len(entities) >= 2:\n",
        "                for i in range(len(entities) - 1):\n",
        "                    # Use the run method to execute Cypher queries for node creation\n",
        "                    neo4j_kg.run(\"CREATE (e:Entity {name: $name})\", name=entities[i])\n",
        "                    neo4j_kg.run(\"CREATE (e:Entity {name: $name})\", name=entities[i + 1])\n",
        "                    # Use the run method to execute Cypher queries for relationship creation\n",
        "                    neo4j_kg.run(\"MATCH (a:Entity {name: $name1}), (b:Entity {name: $name2}) CREATE (a)-[:RELATED_TO]->(b)\", name1=entities[i], name2=entities[i + 1])\n",
        "\n",
        "# Helper to extract entities (dummy function, replace with NER)\n",
        "def extract_entities(sentence):\n",
        "    # Dummy split of words, replace with actual NER processing\n",
        "    return sentence.split()[:2]  # This should return actual entities from sentence"
      ],
      "metadata": {
        "id": "8zKbpLdBrMgZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Expand SEC Knowledge Graph\n",
        "def expand_knowledge_graph(neo4j_kg, additional_data):\n",
        "    for data in additional_data:\n",
        "        entities = extract_entities(data)\n",
        "        if len(entities) >= 2:\n",
        "            # Use the run method to execute Cypher queries for node creation\n",
        "            neo4j_kg.run(\"CREATE (e:Entity {name: $name})\", name=entities[0])\n",
        "            neo4j_kg.run(\"CREATE (e:Entity {name: $name})\", name=entities[1])\n",
        "            # Use the run method to execute Cypher queries for relationship creation\n",
        "            neo4j_kg.run(\"MATCH (a:Entity {name: $name1}), (b:Entity {name: $name2}) CREATE (a)-[:EXPANDED_WITH]->(b)\", name1=entities[0], name2=entities[1])"
      ],
      "metadata": {
        "id": "ccrrXvlprPfx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_knowledge_graph(neo4j_kg, entity):\n",
        "    # Use backticks for the name property and case-insensitive search\n",
        "    query = \"MATCH (n {`name`: $entity})-[*]-(related) RETURN DISTINCT related.`name` as name\"\n",
        "    # Use the run method to execute the query with toLower() for case-insensitivity\n",
        "    result = neo4j_kg.run(query, {\"entity\": entity.lower()})\n",
        "    return [record[\"name\"] for record in result]"
      ],
      "metadata": {
        "id": "RiC2G_-krioP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prepare text for RAG and chat with the graph\n",
        "def prepare_text_for_rag(knowledge_data):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(knowledge_data)\n",
        "    return embeddings\n",
        "\n",
        "def chat_with_knowledge_graph(neo4j_kg, query_text):\n",
        "    rag_pipeline = pipeline('question-answering', model='facebook/bart-large-cnn')\n",
        "\n",
        "    # Convert query_text to lowercase for case-insensitive search\n",
        "    related_entities = query_knowledge_graph(neo4j_kg, query_text.lower())\n",
        "\n",
        "    # Check if related_entities is empty and provide a default response\n",
        "    if not related_entities:\n",
        "        return \"I'm sorry, I don't have any information about that.\"\n",
        "\n",
        "    knowledge_text = \" \".join(related_entities)\n",
        "\n",
        "    answer = rag_pipeline(question=query_text, context=knowledge_text)\n",
        "    return answer[\"answer\"]\n"
      ],
      "metadata": {
        "id": "dm5qFTHurq8-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from py2neo import Graph # Add this line to import the Graph class from the py2neo library\n",
        "\n",
        "if __name__ == \"__main__\": # Fixed the typo here\n",
        "    # Connect to Neo4j using the Graph object from py2neo\n",
        "    neo4j_kg = Graph(uri=\"neo4j+s://eacc3e56.databases.neo4j.io\", auth=(\"neo4j\", \"neo4j\"))\n",
        "\n",
        "    # Sample text data (replace with actual documents)\n",
        "    documents = [\"John is a professor at Stanford. He teaches AI.\", \"AI is a fascinating field of study.\"]\n",
        "\n",
        "    # Step 1: Construct Knowledge Graph\n",
        "    construct_knowledge_graph(neo4j_kg, documents)\n",
        "\n",
        "    # Step 2: Expanding the SEC Knowledge Graph\n",
        "    additional_data = [\"John researches machine learning.\", \"Machine learning is a subset of AI.\"]\n",
        "    expand_knowledge_graph(neo4j_kg, additional_data)\n",
        "\n",
        "    # Step 3: Chat with the Knowledge Graph\n",
        "    query_text = \"Tell me about AI.\"\n",
        "    response = chat_with_knowledge_graph(neo4j_kg, query_text)\n",
        "\n",
        "    print(f\"Chatbot Answer: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ebc8vMrzKT",
        "outputId": "33a9312d-33c8-4bc1-b49e-c0542acc4187"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot Answer: I'm sorry, I don't have any information about that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciNRokqpsXVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}